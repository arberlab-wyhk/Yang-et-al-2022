{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32e8aaf5",
   "metadata": {},
   "source": [
    "# Figure 1 and S1 -- Distribution of cortical neurons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb70619",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a05b66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import scipy.stats as st\n",
    "import os\n",
    "from matplotlib import rc\n",
    "rc(\"pdf\", fonttype=42)\n",
    "\n",
    "root = os.path.abspath(\"\")\n",
    "datapath = root+r\"\\Fig 1 and S1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af349565",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_latrm_1 = pd.read_csv(datapath+r\"\\LatRM_1.csv\", index_col=0)\n",
    "data_latrm_2 = pd.read_csv(datapath+r\"\\LatRM_2.csv\", index_col=0)\n",
    "data_latrm_3 = pd.read_csv(datapath+r\"\\LatRM_3.csv\", index_col=0)\n",
    "data_csc_1 = pd.read_csv(datapath+r\"\\CSC_1.csv\", index_col=0)\n",
    "data_csc_2 = pd.read_csv(datapath+r\"\\CSC_2.csv\", index_col=0)\n",
    "data_csc_3 = pd.read_csv(datapath+r\"\\CSC_3.csv\", index_col=0)\n",
    "data_lsc_1 = pd.read_csv(datapath+r\"\\LSC_1.csv\", index_col=0)\n",
    "data_lsc_2 = pd.read_csv(datapath+r\"\\LSC_2.csv\", index_col=0)\n",
    "data_lsc_3 = pd.read_csv(datapath+r\"\\LSC_3.csv\", index_col=0)\n",
    "\n",
    "latrm = [data_latrm_1, data_latrm_2, data_latrm_3]\n",
    "csc = [data_csc_1, data_csc_2, data_csc_3]\n",
    "lsc = [data_lsc_1, data_lsc_2, data_lsc_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3952d",
   "metadata": {},
   "source": [
    "## neruonal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0fde41b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data_csc_1 ['x']/1000, data_csc_1['z'], marker='.')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data_latrm_1 ['x']/1000, data_latrm_1['z'], marker='.')\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(10,10))\n",
    "plt.scatter(data_lsc_1 ['x']/1000, data_lsc_1['z'], marker='.')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d438887d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour plots for top-down view\n",
    "xmin, xmax = 0, 4.5\n",
    "ymin, ymax = -2.5, 3\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "f_csc = np.reshape(np.zeros(100**2), xx.shape)\n",
    "for data in csc:\n",
    "    data_sub = data[(data.y>-1250)]\n",
    "    x = data_sub['x']/1000\n",
    "    y = data_sub['z']\n",
    "\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    f_csc += f\n",
    "f_csc /= f_csc.max()\n",
    "\n",
    "f_latrm = np.reshape(np.zeros(100**2), xx.shape)\n",
    "for data in latrm:\n",
    "    data_sub = data[(data.y>-1250)]\n",
    "    x = data_sub['x']/1000\n",
    "    y = data_sub['z']\n",
    "\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    f_latrm += f\n",
    "f_latrm /= f_latrm.max()\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "cset_csc = ax.contour(xx, yy, f_csc,levels = [0.1, 0.3, 0.5, 0.7, 0.9], colors='r')\n",
    "cset_latrm = ax.contour(xx, yy, f_latrm,levels = [0.1, 0.3, 0.5, 0.7, 0.9], colors='b')\n",
    "\n",
    "ax.set_xlabel('ML')\n",
    "ax.set_ylabel('AP')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329a78a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density curves along the antero-posterior axis\n",
    "\n",
    "def kernel_1d_y(data):\n",
    "    y_d = np.linspace(-2, 3, 1000)\n",
    "    data_sm = data[data.y>-1250]\n",
    "    y_sm = data_sm['z']\n",
    "    kernel_sm = st.gaussian_kde(y_sm)\n",
    "    p_sm= kernel_sm(y_d)\n",
    "    return p_sm\n",
    "    \n",
    "y_d = np.linspace(-2, 3, 1000)\n",
    "latrm_sm = kernel_1d_y(data_latrm_1)\n",
    "latrm_sm_2 = kernel_1d_y(data_latrm_2)\n",
    "latrm_sm_3 = kernel_1d_y(data_latrm_3)\n",
    "\n",
    "latrm_sm_all = np.array([latrm_sm, latrm_sm_2, latrm_sm_3])\n",
    "latrm_sm_all_mean = np.average(latrm_sm_all, axis=0)\n",
    "latrm_sm_all_sem = np.std(latrm_sm_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "csc_sm = kernel_1d_y(data_csc_1)\n",
    "csc_sm_2 = kernel_1d_y(data_csc_2)\n",
    "csc_sm_3 = kernel_1d_y(data_csc_3)\n",
    "\n",
    "csc_sm_all = np.array([csc_sm, csc_sm_2, csc_sm_3])\n",
    "csc_sm_all_mean = np.average(csc_sm_all, axis=0)\n",
    "csc_sm_all_sem = np.std(csc_sm_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(y_d, latrm_sm_all_mean, color='b')\n",
    "plt.fill_between(y_d, latrm_sm_all_mean+latrm_sm_all_sem, latrm_sm_all_mean-latrm_sm_all_sem, alpha = 0.2, color='b')\n",
    "\n",
    "plt.plot(y_d, csc_sm_all_mean, color='r')\n",
    "plt.fill_between(y_d, csc_sm_all_mean+csc_sm_all_sem, csc_sm_all_mean-csc_sm_all_sem, alpha = 0.2, color='r')\n",
    "\n",
    "plt.axvline(1.1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e00c8d48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# contour plots for top-down view in anterior cortex\n",
    "\n",
    "xmin, xmax = 0, 4.5\n",
    "ymin, ymax = -2.5, 3\n",
    "xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "\n",
    "f_csc = np.reshape(np.zeros(100**2), xx.shape)\n",
    "for data in csc:\n",
    "    data_sub = data[(data.y>-1250)&(data.z>1)]\n",
    "    x = data_sub['x']/1000\n",
    "    y = data_sub['z']\n",
    "\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    f_csc += f\n",
    "f_csc /= f_csc.max()\n",
    "\n",
    "f_latrm = np.reshape(np.zeros(100**2), xx.shape)\n",
    "for data in latrm:\n",
    "    data_sub = data[(data.y>-1250)&(data.z>1)]\n",
    "    x = data_sub['x']/1000\n",
    "    y = data_sub['z']\n",
    "\n",
    "    values = np.vstack([x, y])\n",
    "    kernel = st.gaussian_kde(values)\n",
    "    f = np.reshape(kernel(positions).T, xx.shape)\n",
    "    f_latrm += f\n",
    "f_latrm /= f_latrm.max()\n",
    "\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.gca()\n",
    "ax.set_xlim(xmin, xmax)\n",
    "ax.set_ylim(ymin, ymax)\n",
    "\n",
    "cset_csc = ax.contour(xx, yy, f_csc, 7, colors='r')\n",
    "cset_latrm = ax.contour(xx, yy, f_latrm, 7, colors='b')\n",
    "\n",
    "ax.set_xlabel('x')\n",
    "ax.set_ylabel('y')\n",
    "\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e11b506",
   "metadata": {},
   "outputs": [],
   "source": [
    "# density curves along the medio-lateral axis in anterior cortex\n",
    "\n",
    "x_d = np.linspace(0, 3, 1000)\n",
    "def kernel_1d(data):\n",
    "    data_antero = data[(data.y>-1250)&(data.z>1)]\n",
    "    x_antero = data_antero['x']/1000\n",
    "    kernel_antero = st.gaussian_kde(x_antero)\n",
    "    p_antero = kernel_antero(x_d)\n",
    "    return p_antero\n",
    "    \n",
    "latrm_antero = kernel_1d(data_latrm_1)\n",
    "latrm_antero_2 = kernel_1d(data_latrm_2)\n",
    "latrm_antero_3 = kernel_1d(data_latrm_3)\n",
    "\n",
    "latrm_antero_all = np.array([latrm_antero, latrm_antero_2, latrm_antero_3])\n",
    "latrm_antero_all_mean = np.average(latrm_antero_all, axis=0)\n",
    "latrm_antero_all_sem = np.std(latrm_antero_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "csc_antero = kernel_1d(data_csc_1)\n",
    "csc_antero_2 = kernel_1d(data_csc_2)\n",
    "csc_antero_3 = kernel_1d(data_csc_3)\n",
    "\n",
    "csc_antero_all = np.array([csc_antero, csc_antero_2, csc_antero_3])\n",
    "csc_antero_all_mean = np.average(csc_antero_all, axis=0)\n",
    "csc_antero_all_sem = np.std(csc_antero_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "plt.plot(x_d, latrm_antero_all_mean, color='b')\n",
    "plt.fill_between(x_d, latrm_antero_all_mean+latrm_antero_all_sem, latrm_antero_all_mean-latrm_antero_all_sem, alpha = 0.2, color='b')\n",
    "\n",
    "plt.plot(x_d, csc_antero_all_mean, color='r')\n",
    "plt.fill_between(x_d, csc_antero_all_mean+csc_antero_all_sem, csc_antero_all_mean-csc_antero_all_sem, alpha = 0.2, color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b281958b",
   "metadata": {},
   "source": [
    "## neuron number quantification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f37f8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def num_reg(antero_cut, reglist):\n",
    "    reglist_antero = [x[(x['z']>=antero_cut)&(x['y']>-1250)] for x in reglist]\n",
    "    reglist_postero = [x[(x['z']<antero_cut)&(x['y']>-1250)] for x in reglist]\n",
    "    reglist_antero_medio = [x[x['x']<1500] for x in reglist_antero]\n",
    "    reglist_antero_latero = [x[x['x']>=1500] for x in reglist_antero]\n",
    "    \n",
    "    num_reglist_antero = np.array([x.shape[0] for x in reglist_antero])\n",
    "    num_reglist_postero = np.array([x.shape[0] for x in reglist_postero])\n",
    "    num_reglist_antero_medio = np.array([x.shape[0] for x in reglist_antero_medio])\n",
    "    num_reglist_antero_latero = np.array([x.shape[0] for x in reglist_antero_latero])\n",
    "    \n",
    "    return num_reglist_antero, num_reglist_postero, num_reglist_antero_medio, num_reglist_antero_latero\n",
    "\n",
    "def per_reg(antero_cut, reglist):\n",
    "    num_reglist_antero, num_reglist_postero, num_reglist_antero_medio, num_reglist_antero_latero = num_reg(antero_cut, reglist)\n",
    "    per_antero_postero = (num_reglist_antero / (num_reglist_antero+num_reglist_postero)).mean()\n",
    "    sem_antero_postero = st.sem(num_reglist_antero / (num_reglist_antero+num_reglist_postero))\n",
    "    per_anero_medio_latero = (num_reglist_antero_medio / (num_reglist_antero_medio+num_reglist_antero_latero)).mean()\n",
    "    sem_anero_medio_latero = st.sem(num_reglist_antero_medio / (num_reglist_antero_medio+num_reglist_antero_latero))\n",
    "    \n",
    "    return per_antero_postero, sem_antero_postero, per_anero_medio_latero, sem_anero_medio_latero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b69af7cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of neurons in defined region\n",
    "\n",
    "num_latrm_antero, num_latrm_postero, num_latrm_antero_medio, num_latrm_antero_latero = num_reg(1, latrm)\n",
    "num_csc_antero, num_csc_postero, num_csc_antero_medio, num_csc_antero_latero = num_reg(1, csc)\n",
    "\n",
    "antero_latrm_csc = [num_latrm_antero.mean(), num_csc_antero.mean()]\n",
    "postero_latrm_csc = [num_latrm_postero.mean(), num_csc_postero.mean()]\n",
    "antero_latrm_csc_err =[st.sem(num_latrm_antero), st.sem(num_csc_antero)]\n",
    "postero_latrm_csc_err = [st.sem(num_latrm_postero), st.sem(num_csc_postero)]\n",
    "\n",
    "x_values = range(len(antero_latrm_csc))\n",
    "x_values = ['LatRM projecting', 'CSC projection']\n",
    "\n",
    "plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.bar(x_values, antero_latrm_csc, yerr=antero_latrm_csc_err, capsize=10, color = ['darkcyan', 'darkmagenta'], alpha = 0.7)\n",
    "plt.bar(x_values, postero_latrm_csc, bottom=antero_latrm_csc, yerr=postero_latrm_csc_err, capsize=10, color = ['paleturquoise', 'pink'], alpha = 0.7)\n",
    "plt.scatter([x_values[0]]*3, num_latrm_antero, color = 'darkcyan', edgecolors= 'k')\n",
    "plt.scatter([x_values[0]]*3, num_latrm_postero+num_latrm_antero.mean(), color = 'paleturquoise', edgecolors= 'k')\n",
    "plt.scatter([x_values[1]]*3, num_csc_antero, color = 'darkmagenta', edgecolors= 'k')\n",
    "plt.scatter([x_values[1]]*3, num_csc_postero+num_csc_antero.mean(), color = 'pink', edgecolors= 'k')\n",
    "plt.title('Number of neurons along RC axis', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5677415",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie plot to show the proportion of neurons in related regions\n",
    "\n",
    "plt.pie([antero_latrm_csc[0],postero_latrm_csc[0]], colors = ['darkcyan', 'paleturquoise'])\n",
    "plt.title('Proportion of LatRM projection neurons: Anterior vs Posterior', fontsize = 14)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.pie([antero_latrm_csc[1],postero_latrm_csc[1]], colors = ['darkmagenta', 'pink'])\n",
    "plt.title('Proportion of cervical SC projection neurons: Anterior vs Posterior', fontsize = 14)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06a3e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count the number of neurons in defined region\n",
    "\n",
    "medio_latrm_csc = [num_latrm_antero_medio.mean(), num_csc_antero_medio.mean()]\n",
    "latero_latrm_csc = [num_latrm_antero_latero.mean(), num_csc_antero_latero.mean()]\n",
    "medio_latrm_csc_err =[st.sem(num_latrm_antero_medio), st.sem(num_csc_antero_medio)]\n",
    "latero_latrm_csc_err = [st.sem(num_latrm_antero_latero), st.sem(num_csc_antero_latero)]\n",
    "\n",
    "x_values = range(len(medio_latrm_csc))\n",
    "x_values = ['LatRM projecting', 'cervical SC projection']\n",
    "ax=plt.figure(figsize = (10,10))\n",
    "\n",
    "plt.bar(x_values, medio_latrm_csc, yerr=medio_latrm_csc_err, capsize=10, color = ['darkcyan', 'darkmagenta'], alpha = 0.7)\n",
    "plt.bar(x_values, latero_latrm_csc, bottom=medio_latrm_csc, yerr=latero_latrm_csc_err, capsize=10, color = ['paleturquoise', 'pink'], alpha = 0.7)\n",
    "plt.scatter([x_values[0]]*3, num_latrm_antero_medio, color = 'darkcyan', edgecolors= 'k')\n",
    "plt.scatter([x_values[0]]*3, num_latrm_antero_latero+num_latrm_antero_medio.mean(), color = 'paleturquoise', edgecolors= 'k')\n",
    "plt.scatter([x_values[1]]*3, num_csc_antero_medio, color = 'darkmagenta', edgecolors= 'k')\n",
    "plt.scatter([x_values[1]]*3, num_csc_antero_latero+num_csc_antero_medio.mean(), color = 'pink', edgecolors= 'k')\n",
    "plt.title('Number of neurons in AC along ML axis', fontsize = 14)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1d92c54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pie plot to show the proportion of neurons in related regions\n",
    "\n",
    "plt.pie([medio_latrm_csc[0],latero_latrm_csc[0]], colors = ['darkcyan', 'paleturquoise'])\n",
    "plt.title('Proportion of LatRM projection neurons: Medial vs Lateral', fontsize = 14)\n",
    "plt.axis('equal')\n",
    "plt.show()\n",
    "\n",
    "plt.pie([medio_latrm_csc[1],latero_latrm_csc[1]], colors = ['darkmagenta', 'pink'])\n",
    "plt.title('Proportion of cervical SC projection neurons: Medial vs Lateral', fontsize = 14)\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d49146ca",
   "metadata": {},
   "source": [
    "# Figure 2, 3 and S2 -- Distribution of cortical synapses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76849da9",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "306ac27b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.neighbors import KernelDensity\n",
    "import scipy.stats as st\n",
    "import glob\n",
    "import os\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from statsmodels.stats.multicomp import (pairwise_tukeyhsd, MultiComparison)\n",
    "from statsmodels.stats.libqsturng import psturng\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import itertools\n",
    "import scipy.cluster.hierarchy\n",
    "import scipy.spatial.distance as ssd\n",
    "from scipy.stats import f_oneway\n",
    "import statsmodels.stats.multicomp as mc\n",
    "import statsmodels.api as sa\n",
    "import scikit_posthocs as sp\n",
    "from statsmodels.formula.api import ols\n",
    "from bioinfokit.analys import stat\n",
    "\n",
    "from matplotlib import rc\n",
    "rc(\"pdf\", fonttype=42)\n",
    "\n",
    "root_path = root+r'\\Fig 2, 3 and S2'\n",
    "datafile_path_ros = root_path + r'\\rostral'\n",
    "datafile_path_int = root_path + r'\\intermediate'\n",
    "datafile_path_cau = root_path + r'\\caudal'\n",
    "\n",
    "class Injection_Sites():\n",
    "    def __init__(self, datapath, filename):\n",
    "        self.datapath = datapath\n",
    "        self.filename = filename\n",
    "        self.datapath_file = glob.glob(datapath +'\\\\'+ '*' + filename)\n",
    "        \n",
    "    def read_data(self):  \n",
    "        if \"xls\" in self.filename[-4:]:\n",
    "            data = pd.read_excel(self.datapath_file[0], index_col=0)\n",
    "            return data\n",
    "        if self.filename[-3:] == 'csv':\n",
    "            data = pd.read_csv(self.datapath_file[0], index_col=0)\n",
    "            return data\n",
    "    \n",
    "    def get_coordinate(self, good_sites=True):\n",
    "        data_good = self.read_data()\n",
    "            \n",
    "        data_good.loc[:, \"y\"] = (data_good.get('y_max')+data_good.get('y_min'))/2\n",
    "        data_good.loc[:, \"x\"] = (data_good.get('x_max')+data_good.get('x_min'))/2\n",
    "            \n",
    "        return data_good\n",
    "    \n",
    "    def cross_plot(self, good_sites=True, top_down=True, side_view=False, name=False, skip=None, exp=None):\n",
    "        inj_site_coordinate = self.get_coordinate(good_sites)\n",
    "        \n",
    "        if skip is not None:\n",
    "            for item in skip:\n",
    "                inj_site_coordinate = inj_site_coordinate.loc[inj_site_coordinate.brain != item]\n",
    "                \n",
    "        fig = plt.figure(figsize=(20,20))\n",
    "        ax = fig.gca()\n",
    "        ax.plot([inj_site_coordinate[\"x_min\"], inj_site_coordinate[\"x_max\"]], [inj_site_coordinate[\"y\"], inj_site_coordinate[\"y\"]], color='red')\n",
    "        ax.plot([inj_site_coordinate[\"x\"], inj_site_coordinate[\"x\"]], [inj_site_coordinate[\"y_min\"], inj_site_coordinate[\"y_max\"]], color='green')\n",
    "        ax.scatter(inj_site_coordinate[\"x\"], inj_site_coordinate[\"y\"], c='blue')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        plt.axis('equal')\n",
    "\n",
    "        if name is True:\n",
    "            for point in inj_site_coordinate['brain']:\n",
    "                ax.text(x=inj_site_coordinate.loc[inj_site_coordinate['brain']==point][\"x\"], \n",
    "                        y=inj_site_coordinate.loc[inj_site_coordinate['brain']==point][\"y\"], s=point, size=10)\n",
    "        \n",
    "        if exp == True:\n",
    "            plt.savefig(f'crossplot_{self.filename[:-4]}.pdf')\n",
    "            \n",
    "    def rainbow_cross(self, good_sites=True, top_down=True, side_view=False, skip=None, name=False, exp=None, colormap=\"cool\", data=None, exp_cbar=None, cbar=False, plot=True, min_max=None):\n",
    "        inj_site_coordinate = self.get_coordinate(good_sites)\n",
    "        \n",
    "        if skip is not None:\n",
    "            for item in skip:\n",
    "                inj_site_coordinate = inj_site_coordinate.loc[inj_site_coordinate.brain != item]\n",
    "                \n",
    "        if data is None:\n",
    "            raise Exception(\"Please specify a coloumn as data to plot\")\n",
    "        \n",
    "        inj_site_coordinate.reset_index(drop=True, inplace=True)\n",
    "        if min_max:\n",
    "            minima = min_max[0]\n",
    "            maxima = min_max[1]\n",
    "        else:\n",
    "            minima = min(inj_site_coordinate[data])\n",
    "            maxima = max(inj_site_coordinate[data])\n",
    "        norm = matplotlib.colors.Normalize(vmin=minima, vmax=maxima)\n",
    "        mapper = matplotlib.cm.ScalarMappable(norm=norm, cmap=colormap)\n",
    "        \n",
    "        if cbar:\n",
    "            cfig, cax = plt.subplots(figsize=(10, 1))\n",
    "            matplotlib.colorbar.Colorbar(cax, mappable=mapper, orientation='horizontal')\n",
    "            if exp_cbar is True:\n",
    "                plt.savefig((f'cmap_{data}_{self.filename[:-4]}.pdf'))\n",
    "\n",
    "        if plot:\n",
    "            lst=[]\n",
    "            for i in inj_site_coordinate[data]:\n",
    "                lst.append(mapper.to_rgba(i))\n",
    "            inj_site_coordinate[\"color\"]=lst\n",
    "\n",
    "            fig = plt.figure(figsize=(20,20))\n",
    "            ax = fig.gca()\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            plt.axis('equal')\n",
    "\n",
    "            for j in range(len(inj_site_coordinate[data])):\n",
    "                color = np.array(inj_site_coordinate[\"color\"][j])\n",
    "                ax.scatter(inj_site_coordinate[\"x\"][j], inj_site_coordinate[\"y\"][j], color=color)\n",
    "                ax.plot([inj_site_coordinate[\"x_min\"][j], inj_site_coordinate[\"x_max\"][j]], [inj_site_coordinate[\"y\"][j], inj_site_coordinate[\"y\"][j]], color=color)\n",
    "                ax.plot([inj_site_coordinate[\"x\"][j], inj_site_coordinate[\"x\"][j]], [inj_site_coordinate[\"y_min\"][j], inj_site_coordinate[\"y_max\"][j]], color=color)\n",
    "        \n",
    "            if name is True:\n",
    "                for point in inj_site_coordinate['brain']:\n",
    "                    ax.text(x=inj_site_coordinate.loc[inj_site_coordinate['brain']==point][\"x\"], y=inj_site_coordinate.loc[inj_site_coordinate['brain']==point][\"y\"], s=point, size=10)\n",
    "        \n",
    "        if exp == True:\n",
    "            plt.savefig(f'rainbow_{data}_{self.filename[:-4]}.pdf')\n",
    "            \n",
    "class Syn_Position(): #position data exported from imaris\n",
    "    \n",
    "    def __init__(self, data_path=None, filename=None, datadf=None):\n",
    "        self.data_path = data_path\n",
    "        self.filename = filename\n",
    "        self.datadf = datadf\n",
    "    \n",
    "    def read_nor_data(self):\n",
    "        data_filepath = glob.glob(self.data_path +'\\\\'+ '*' + self.filename)\n",
    "            \n",
    "        if '.xls' in self.filename:\n",
    "            data = pd.read_excel(data_filepath[0], index_col=0)\n",
    "        elif '.csv' in self.filename:\n",
    "            data = pd.read_csv(data_filepath[0], index_col=0)\n",
    "        else:\n",
    "            return 'Only read xls or csv file'\n",
    "        return data\n",
    "            \n",
    "    def split_data(self, one_side=True, left=None, right=None, nor=None, nordf=None):\n",
    "        if nor is True:\n",
    "            f_data = self.read_nor_data()\n",
    "        elif nordf is True:\n",
    "            f_data = self.datadf\n",
    "\n",
    "        region = 'contra'\n",
    "        return f_data, region\n",
    "    \n",
    "    def scatter_plot(self, xmin=-4.5, xmax=1, ymin=-2, ymax=2, size=2, exp=None, one_side=True, label=None, filled=None, delim=None, left=None, right=None, nor=True, nordf=None, section=None):\n",
    "        data_sub, region = self.split_data(one_side=one_side, left=left, right=right, nor=nor, nordf=nordf)\n",
    "            \n",
    "        if nordf is True:\n",
    "            x = data_sub['x']\n",
    "            y = data_sub['y']\n",
    "        else:\n",
    "            x = data_sub['x']/1000\n",
    "            y = data_sub['y']/1000\n",
    "        \n",
    "        fig = plt.figure(figsize=(10,10))\n",
    "        ax = fig.gca()\n",
    "        ax.set_xlim(xmin, xmax)\n",
    "        ax.set_ylim(ymin, ymax)\n",
    "        ax.scatter(x, y, s=size, c='black')\n",
    "        ax.set_xlabel('X')\n",
    "        ax.set_ylabel('Y')\n",
    "        plt.axis('equal')\n",
    "        \n",
    "        # Save plot\n",
    "        if exp == True:\n",
    "            if self.filename:\n",
    "                plt.savefig(f'spot_{region}_{self.filename[:-4]}.pdf')\n",
    "            else:\n",
    "                plt.savefig(f'spot_{section}.pdf')\n",
    "    \n",
    "    def contour_plot(self, xmin=-2.5, xmax=0.5, ymin=-1, ymax=2, line_num=10, exp=None, one_side=True, label=None, filled=None, delim=None, left=None, right=None, nor=True, nordf=None, section=None, est_exp=None, plot=True):\n",
    "        data_sub, region = self.split_data(one_side=one_side, left=left, right=right, nor=nor, nordf=nordf)\n",
    "\n",
    "        xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "        positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "        f_syn = np.reshape(np.zeros(100**2), xx.shape)\n",
    "            \n",
    "        if nordf is True:\n",
    "            x = data_sub['x']\n",
    "            y = data_sub['y']\n",
    "        else:\n",
    "            x = data_sub['x']/1000\n",
    "            y = data_sub['y']/1000\n",
    "\n",
    "        values = np.vstack([x, y])\n",
    "        kernel = st.gaussian_kde(values)\n",
    "        f = np.reshape(kernel(positions).T, xx.shape)\n",
    "        f_syn += f\n",
    "        if type(line_num)==list:\n",
    "            f_syn /= f_syn.max()\n",
    "        \n",
    "        if plot:\n",
    "            fig = plt.figure(figsize=(10,10))\n",
    "            ax = fig.gca()\n",
    "            ax.set_xlim(xmin, xmax)\n",
    "            ax.set_ylim(ymin, ymax)\n",
    "            # Contour plot\n",
    "            cset = ax.contour(xx, yy, f_syn, line_num, colors='b')\n",
    "\n",
    "            # Contourf plot\n",
    "            if filled == True:\n",
    "                cfset = ax.contourf(xx, yy, f_syn, cmap='Blues')\n",
    "            ## Or kernel density estimate plot instead of the contourf plot\n",
    "            #ax.imshow(np.rot90(f), cmap='Blues', extent=[xmin, xmax, ymin, ymax])\n",
    "\n",
    "            # Label plot\n",
    "            if label == True:\n",
    "                ax.clabel(cset, inline=1, fontsize=10)\n",
    "\n",
    "            ax.set_xlabel('X')\n",
    "            ax.set_ylabel('Y')\n",
    "            plt.axis('equal')\n",
    "        \n",
    "        # Save plot\n",
    "        if exp == True:\n",
    "            if self.filename:\n",
    "                plt.savefig(f'contour_{region}_{self.filename[:-4]}.pdf')\n",
    "            else:\n",
    "                plt.savefig(f'contour_{section}.pdf')\n",
    "        \n",
    "        if est_exp is True:    \n",
    "            return f_syn\n",
    "    \n",
    "    def kde1d_x_plot(self, xmin=-2.5, xmax=0.5, bandwidth=0.1, exp=None, one_side=True, delim=None, left=None, right=None, nor=True, nordf=None, est_exp=None, plot=True):\n",
    "        data_sub, region = self.split_data(one_side=one_side, left=left, right=right, nor=nor, nordf=nordf)\n",
    "            \n",
    "        x = np.array(data_sub['x']/1000)\n",
    "        \n",
    "        x_d = np.linspace(xmin, xmax, 100)\n",
    "        kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "        kde.fit(np.reshape(x, (len(x),1)))\n",
    "        logprob = kde.score_samples(x_d[:, None])\n",
    "        if plot:\n",
    "            plt.plot(x_d, np.exp(logprob))\n",
    "            plt.ylim(-0.02, 2.3)\n",
    "        \n",
    "        # Save plot\n",
    "        if exp == True:\n",
    "            plt.savefig(f'kde1d_x_{region}_{self.filename[:-4]}.pdf')\n",
    "            \n",
    "        if est_exp is True:\n",
    "            return pd.DataFrame({'x':x_d, 'est':np.exp(logprob)})\n",
    "\n",
    "    def kde1d_y_plot(self, ymin=-0.5, ymax=2.3, bandwidth=0.1, exp=None, one_side=True, delim=None, left=None, right=None, nor=True, nordf=None, est_exp=None, plot=True):\n",
    "        data_sub, region = self.split_data(one_side=one_side, left=left, right=right, nor=nor, nordf=nordf)\n",
    "            \n",
    "        y = np.array(data_sub['y']/1000)\n",
    "        \n",
    "        y_d = np.linspace(ymin, ymax, 100)\n",
    "        kde = KernelDensity(bandwidth=bandwidth, kernel='gaussian')\n",
    "        kde.fit(np.reshape(y, (len(y),1)))\n",
    "        logprob = kde.score_samples(y_d[:, None])\n",
    "        if plot:\n",
    "            plt.plot(y_d, np.exp(logprob))\n",
    "            plt.ylim(-0.02, 2.3)\n",
    "        \n",
    "        # Save plot\n",
    "        if exp == True:\n",
    "            plt.savefig(f'kde1d_y_{region}_{self.filename[:-4]}.pdf') \n",
    "            \n",
    "        if est_exp is True:\n",
    "            return pd.DataFrame({'y':y_d, 'est':np.exp(logprob)})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33d35bda",
   "metadata": {},
   "source": [
    "## plot the injection sites and the synapse number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6de94685",
   "metadata": {},
   "outputs": [],
   "source": [
    "inj_site = Injection_Sites(root_path, \"injection_site.csv\")\n",
    "inj_site_table = inj_site.read_data()\n",
    "inj_sum = inj_site_table.groupby('region')\n",
    "colors = ['darkmagenta', 'darkturquoise', 'lightpink', 'yellowgreen', 'dimgrey']\n",
    "ctx_lst = ['mac',\"lac\",\"cfa\",\"ins\",\"pos\"]\n",
    "ctx_table = [inj_sum.get_group(ctx) for ctx in ctx_lst]\n",
    "bs_lst = ['rostral', 'intermediate', 'caudal']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5854f5ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the injection sites color coded with synapse number and lateral index\n",
    "for bs in bs_lst:\n",
    "    inj_site.rainbow_cross(data=f\"num_{bs}\", colormap='plasma', min_max=[100, 6000])\n",
    "\n",
    "inj_site.rainbow_cross(data=\"lat_ind\", colormap='plasma')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f814bb87",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for bs in bs_lst:\n",
    "    plt.title(f'Number of synapses in {bs} medulla', fontsize = 20)\n",
    "    plt.xlabel('Cortical injection site', fontsize = 18)\n",
    "    plt.bar(ctx_lst, [ctx[f'num_{bs}'].mean() for ctx in ctx_table], yerr=[ctx.num_rostral.sem() for ctx in ctx_table], capsize=10, error_kw={'markeredgewidth':1}, alpha = 0.4, color = colors)\n",
    "    for idx, ctx in enumerate(ctx_lst):\n",
    "        plt.scatter([ctx]*ctx_table[idx].shape[0], ctx_table[idx][f'num_{bs}'].values, color = colors[idx])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c18f84e9",
   "metadata": {},
   "source": [
    "## plot the distribution of synapses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50e06ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def kde1d_group(inj_site_data, datafile_path, axis, xmin, xmax, ymin, ymax, group=None, plot=True, exp=None, est_exp=None, peak_exp=None):\n",
    "    est_group = pd.DataFrame()\n",
    "    peak = []\n",
    "    if group:\n",
    "        group_data = inj_site_data[inj_site_data.region==group].reset_index(drop=True)\n",
    "    else:\n",
    "        group_data = inj_site_data\n",
    "        \n",
    "    for i in group_data.brain:\n",
    "        item = Syn_Position(datafile_path, i+'.csv')\n",
    "        \n",
    "        if axis== 'x' or axis== 'X':\n",
    "            est = item.kde1d_x_plot(xmin, xmax, est_exp=True, plot=plot)\n",
    "            est_group[\"x\"]=est.x\n",
    "            est_group[i]=est.est\n",
    "            est_group_max = est.iloc[est.idxmax()['est']]['x']\n",
    "            \n",
    "        elif axis=='y' or axis=='Y':\n",
    "            est = item.kde1d_y_plot(ymin, ymax, est_exp=True, plot=plot)\n",
    "            est_group[\"y\"]=est.y\n",
    "            est_group[i]=est.est\n",
    "            est_group_max = est.iloc[est.idxmax()['est']]['y']\n",
    "            \n",
    "        else:\n",
    "            print(\"Please specify axis\")\n",
    "        \n",
    "        peak.append(est_group_max)\n",
    "            \n",
    "    if plot:\n",
    "        plt.legend(est_group.columns[1:])\n",
    "    if exp==True:\n",
    "        if axis=='x':\n",
    "            plt.savefig(f'syn_kde1d_x_{group}.pdf')\n",
    "        elif axis=='y':\n",
    "            plt.savefig(f'syn_kde1d_y_{group}.pdf')\n",
    "            \n",
    "    if est_exp==True:\n",
    "        return est_group\n",
    "    \n",
    "    if peak_exp == True:\n",
    "        return peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d82dc0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot synaptic distribution from individual cortical site\n",
    "\n",
    "syn_mac_2 = Syn_Position(datafile_path_ros, 'mac_2.csv') \n",
    "syn_mac_2.scatter_plot()\n",
    "syn_mac_2.contour_plot()\n",
    "\n",
    "#distribution along ML axis\n",
    "syn_mac_2.kde1d_x_plot()\n",
    "\n",
    "#distribtuion along DV axis\n",
    "syn_mac_2.kde1d_y_plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "768a6cdb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot synaptic distribution from a group of cortical sites in ML axis\n",
    "kde1d_group(inj_site_table, datafile_path_ros, axis='x', xmin=-2.5, xmax=0.5, ymin=-0.5, ymax=1.4, group='mac')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95d7bff",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot synaptic distribution from a group of cortical sites in DV axis\n",
    "kde1d_group(inj_site_table, datafile_path_ros, axis='y', xmin=-2.5, xmax=0.5, ymin=-0.5, ymax=2.3, group='lac')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c8aba70",
   "metadata": {},
   "source": [
    "## cosine similarity and clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfecf23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fancy_dendrogram(*args, **kwargs):\n",
    "    max_d = kwargs.pop('max_d', None)\n",
    "    if max_d and 'color_threshold' not in kwargs:\n",
    "        kwargs['color_threshold'] = max_d\n",
    "    annotate_above = kwargs.pop('annotate_above', 0)\n",
    "\n",
    "    ddata = dendrogram(*args, **kwargs)\n",
    "\n",
    "    if not kwargs.get('no_plot', False):\n",
    "        plt.title('Hierarchical Clustering Dendrogram (truncated)')\n",
    "        plt.xlabel('sample index or (cluster size)')\n",
    "        plt.ylabel('distance')\n",
    "        for i, d, c in zip(ddata['icoord'], ddata['dcoord'], ddata['color_list']):\n",
    "            x = 0.5 * sum(i[1:3])\n",
    "            y = d[1]\n",
    "            if y > annotate_above:\n",
    "                plt.plot(x, y, 'o', c=c)\n",
    "                plt.annotate(\"%.3g\" % y, (x, y), xytext=(0, -5),\n",
    "                             textcoords='offset points',\n",
    "                             va='top', ha='center')\n",
    "        if max_d:\n",
    "            plt.axhline(y=max_d, c='k')\n",
    "    return ddata\n",
    "\n",
    "def kde2d(inj_site_data, datafile_path, xmin, xmax, ymin, ymax):\n",
    "    est2d = kde2d_matrix(inj_site_data, datafile_path, xmin, xmax, ymin, ymax)\n",
    "    est2d_out = {}\n",
    "    for key in est2d.keys():\n",
    "        est2d_out[key]=np.reshape(est2d[key], (1,-1))\n",
    "    return est2d_out\n",
    "\n",
    "def kde2d_to_1d(data):\n",
    "    #data should be a dictionary of matrix\n",
    "    out_1d = {}\n",
    "    for key in data.keys():\n",
    "        out_1d[key]=np.reshape(data[key], (1,-1))\n",
    "    return out_1d\n",
    "\n",
    "def kde2d_matrix(inj_site_data, datafile_path, xmin, xmax, ymin, ymax):\n",
    "    est2d = {}\n",
    "    z = 0\n",
    "\n",
    "    for i in inj_site_data.brain:\n",
    "        reg = inj_site_data['region'].iloc[z]\n",
    "        item = Syn_Position(datafile_path, i+'.csv')\n",
    "        label = f'{reg}{z}'\n",
    "        est2d[label]=item.contour_plot(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, est_exp=True, plot=False)\n",
    "        z+=1\n",
    "    return est2d\n",
    "\n",
    "def cosine_similarity_matrix(data, exp=None, cos_exp=None, section=None):\n",
    "    corrmatrix2d = pd.DataFrame(index=data.keys())\n",
    "    for i, j in itertools.product(data.keys(), data.keys()):\n",
    "        x = data[i]\n",
    "        y = data[j]\n",
    "        corrmatrix2d.loc[i,j] = cosine_similarity(x, y)[0][0]\n",
    "\n",
    "    sns.clustermap(corrmatrix2d, cmap = 'plasma')\n",
    "    if exp==True:\n",
    "        plt.savefig(f'cluster_kde2d_cossim_{section}.pdf')\n",
    "    if cos_exp==True:\n",
    "        return corrmatrix2d\n",
    "    \n",
    "def cosine_distance_matrix(data, exp=None, cos_exp=None, section=None):\n",
    "    corrmatrix2d_dis = pd.DataFrame(index=data.keys())\n",
    "    for i, j in itertools.product(data.keys(), data.keys()):\n",
    "        x = data[i]\n",
    "        y = data[j]\n",
    "        if i == j:\n",
    "            corrmatrix2d_dis.loc[i,j] = 0\n",
    "        else:\n",
    "            corrmatrix2d_dis.loc[i,j] = 1-cosine_similarity(x, y)[0][0]\n",
    "\n",
    "    sns.clustermap(corrmatrix2d_dis, cmap = 'plasma')\n",
    "    if exp==True:\n",
    "        plt.savefig(f'cluster_kde2d_cosdis_{section}.pdf')\n",
    "    if cos_exp==True:\n",
    "        return corrmatrix2d_dis\n",
    "    \n",
    "def cosine_distance_dendrogram(data, exp=None, section=None):\n",
    "    corrmatrix2d_dis = cosine_distance_matrix(data, cos_exp=True)\n",
    "    # convert the redundant n*n square matrix form into a condensed nC2 array\n",
    "    distArray = ssd.squareform(corrmatrix2d_dis)\n",
    "    HAC= scipy.cluster.hierarchy.linkage(distArray, method = 'ward')\n",
    "    plt.figure(figsize = (20,7))\n",
    "    a = fancy_dendrogram(\n",
    "        HAC,\n",
    "        labels = list(corrmatrix2d_dis.keys()), \n",
    "        leaf_rotation=90.,  # rotates the x axis labels\n",
    "        leaf_font_size=20.,  # font size for the x axis labels\n",
    "        annotate_above = 0.25\n",
    "    )\n",
    "    if exp==True:\n",
    "        plt.savefig(f'cluster_2d_dis_{section}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590d8980",
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate and vectorize the synpatic distribtuion for all injection sites in all levels\n",
    "est2d_ros = kde2d(inj_site.read_data(), datafile_path_ros, xmin=-2.5, xmax=0.5, ymin=-0.5, ymax=1.4)\n",
    "est2d_int = kde2d(inj_site.read_data(), datafile_path_int, xmin=-2.1, xmax=0.5, ymin=-0.5, ymax=2.1)\n",
    "est2d_cau = kde2d(inj_site.read_data(), datafile_path_cau, xmin=-3, xmax=0.5, ymin=-0.5, ymax=3)\n",
    "\n",
    "# generate the synaptic distribution in 3D based on 2D distribution at multiple levels\n",
    "est3d = {}\n",
    "for item in est2d_ros.keys():\n",
    "    est_ros = est2d_ros[item]\n",
    "    est_int = est2d_int[item]\n",
    "    est_cau = est2d_cau[item]\n",
    "    est3d[item] = np.concatenate((est2d_ros[item], est2d_int[item], est2d_cau[item]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0360a11d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clustering analysis\n",
    "corrmatrix2d_distance=cosine_distance_dendrogram(est2d_ros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5f24112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# similarity matrix in 3D\n",
    "cosine_similarity_matrix(est3d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99becf87",
   "metadata": {},
   "source": [
    "## dorsal-ventral synaptic peak"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8834080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the peak in DV axis\n",
    "inj_site_table['y_peak_rostral'] = kde1d_group(inj_site_table, datafile_path_ros, axis='y', xmin=-2.5, xmax=0.5, ymin=-0.5, ymax=2.3, peak_exp=1, plot=0)\n",
    "inj_site_table['y_peak_intermediate'] = kde1d_group(inj_site_table, datafile_path_int, axis='y', xmin=-2.5, xmax=0.5, ymin=-0.5, ymax=2.3, peak_exp=1, plot=0)\n",
    "inj_site_table['y_peak_caudal'] = kde1d_group(inj_site_table, datafile_path_cau, axis='y', xmin=-2.5, xmax=0.5, ymin=-0.5, ymax=2.3, peak_exp=1, plot=0)\n",
    "inj_sum = inj_site_table.groupby('region')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e051d7b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the DV peak for all groups\n",
    "\n",
    "ctx_lst = ['ins',\"lac\",\"mac\",\"cfa\"]\n",
    "ctx_table = [inj_sum.get_group(ctx) for ctx in ctx_lst]\n",
    "colors = ['yellowgreen', 'darkturquoise', 'darkmagenta', 'lightpink']\n",
    "plt.figure(figsize = (15,15), facecolor = 'w')\n",
    "plt.title('Location of peak of distribution of synapses on DV axis', fontsize = 20)\n",
    "\n",
    "plt.bar([f'{ctx}_\\n{bs}' for ctx in ctx_lst for bs in bs_lst],\n",
    "        [ctx[f'y_peak_{bs}'].mean() for ctx in ctx_table for bs in bs_lst],\n",
    "        yerr=[ctx[f'y_peak_{bs}'].sem() for ctx in ctx_table for bs in bs_lst],\n",
    "        color = np.repeat(colors,3),\n",
    "        capsize=10, error_kw={'markeredgewidth':1}, alpha = 0.4)\n",
    "\n",
    "for bs in bs_lst:\n",
    "    for idx, ctx in enumerate(ctx_lst):\n",
    "        plt.scatter([f'{ctx}_\\n{bs}']*len(ctx_table[idx]), ctx_table[idx][f'y_peak_{bs}'].values, color = colors[idx])\n",
    "        \n",
    "plt.grid(b=None)\n",
    "plt.xticks(fontsize = 10)\n",
    "plt.yticks(fontsize = 16)\n",
    "plt.xlabel('Cortical injection site', fontsize = 18)\n",
    "plt.ylabel('Dorso-ventral location (mm)',  fontsize = 18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "954abdcd",
   "metadata": {},
   "source": [
    "## statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "882e1183",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for number of synapses in different medulla levels\n",
    "\n",
    "for bs in bs_lst:\n",
    "    f_value = f_oneway(inj_sum.get_group('mac')[f'num_{bs}'], inj_sum.get_group('lac')[f'num_{bs}'], \n",
    "         inj_sum.get_group('cfa')[f'num_{bs}'], inj_sum.get_group('ins')[f'num_{bs}'], \n",
    "         inj_sum.get_group('pos')[f'num_{bs}'])\n",
    "    print(f_value)\n",
    "    comp = mc.MultiComparison(pd.to_numeric(inj_site_table[f'num_{bs}']), inj_site_table['region'])\n",
    "    post_hoc_res = comp.tukeyhsd()\n",
    "    print(post_hoc_res.summary())\n",
    "    model = ols(f'num_{bs} ~ C(region)', data=inj_site_table).fit()\n",
    "    w, pvalue = st.shapiro(model.resid)\n",
    "    print(w, pvalue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "553d0a01",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# DV peaks across three levels\n",
    "inj_site_test = inj_site_table[inj_site_table['region']!='pos']\n",
    "y_peak_lst = list(inj_site_test.y_peak_rostral) + list(inj_site_test.y_peak_intermediate) + list(inj_site_test.y_peak_caudal)\n",
    "region_lst = list(inj_site_test.region)*3\n",
    "bs_lst = np.repeat(bs_lst, len(inj_site_test))\n",
    "y_peak_test = pd.DataFrame({\n",
    "    'region': region_lst,\n",
    "    'bs': bs_lst,\n",
    "    'y_peak': y_peak_lst})\n",
    "\n",
    "model = ols('y_peak ~ C(bs) + C(region) + C(bs):C(region)', data=y_peak_test).fit()\n",
    "anova_table = sa.stats.anova_lm(model, typ=3)\n",
    "print(anova_table)\n",
    "\n",
    "res = stat()\n",
    "res.tukey_hsd(df=y_peak_test, res_var='y_peak', xfac_var='region', anova_model='y_peak ~ C(bs) + C(region) + C(bs):C(region)')\n",
    "print(res.tukey_summary)\n",
    "\n",
    "w, pvalue = st.shapiro(model.resid)\n",
    "print(w, pvalue)\n",
    "sa.qqplot(res.anova_std_residuals, line='45')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc2cdd5a",
   "metadata": {},
   "source": [
    "# Figure 4 and S3 -- MouseLight analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "439e58e7",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6cdcf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "from scipy import stats as st\n",
    "from matplotlib import cm\n",
    "import matplotlib\n",
    "#import neurom.io.swc as swc\n",
    "\n",
    "datapath = root+r'\\Fig 4 and S3'\n",
    "\n",
    "bregma = [5400, 0, 5700]\n",
    "\n",
    "striatum = '477'\n",
    "medulla = '354'\n",
    "stn = '470'\n",
    "suc = '294'\n",
    "pg = '931'\n",
    "thalamus = '549'\n",
    "rn = '214'\n",
    "snr = '381'\n",
    "py = '190'\n",
    "regions = [medulla, thalamus, suc, striatum, pg, stn, rn, snr, py]\n",
    "ctxs = ['mac', 'lac', 'ins']\n",
    "region_names = ['Medulla', 'Thalamus', 'SuC', 'Striatum', 'Pontine_gray', 'STN', 'RN', 'SNr', 'pyramidal_tract']\n",
    "\n",
    "def allen_to_paxinos(coords_allen):\n",
    "    #coords need to be [ap, dv, ml]\n",
    "    bregma = [5400, 0, 5700]\n",
    "    \n",
    "    if type(coords_allen) == list:\n",
    "        coords_paxinos = np.zeros(len(coords_allen))\n",
    "        coords_paxinos[0] = (bregma[0] - coords_allen[0])/1000\n",
    "        coords_paxinos[1] = (coords_allen[1] - bregma[1])/1000\n",
    "        coords_paxinos[2] = (coords_allen[2] - bregma[2])/1000\n",
    "    \n",
    "    elif type(coords_allen) == np.ndarray:\n",
    "        coords_paxinos = np.zeros(coords_allen.shape)\n",
    "        coords_paxinos[:, 0] = (bregma[0] - coords_allen[:, 0])/1000\n",
    "        coords_paxinos[:, 1] = (coords_allen[:, 1] - bregma[1])/1000\n",
    "        coords_paxinos[:, 2] = (coords_allen[:, 2] - bregma[2])/1000\n",
    "    \n",
    "    return coords_paxinos\n",
    "\n",
    "def read_mouselight(filename):\n",
    "    with open(filename) as f:\n",
    "        neurons = json.load(f)\n",
    "        \n",
    "    return neurons\n",
    "\n",
    "def get_allen_info(filename, neuron):\n",
    "    neurons = read_mouselight(filename)\n",
    "    allen_info = neurons['neurons'][neuron]['allenInformation']\n",
    "    \n",
    "    return allen_info\n",
    "\n",
    "def get_medulla_id(filename, neuron, medulla_allen_id='354'):\n",
    "    in_medulla_allen_ids = []\n",
    "    allen_info = get_allen_info(filename, neuron)\n",
    "    for id in range(len(allen_info)):\n",
    "        id_list = list(filter(None, allen_info[id]['structureIdPath'].split('/')))\n",
    "        if medulla_allen_id in id_list:\n",
    "            in_medulla_allen_ids.append(allen_info[id]['allenId'])\n",
    "    print ('These are the regions in the medulla for given neuron')\n",
    "    for id in range(len(allen_info)):\n",
    "        if allen_info[id]['allenId'] in in_medulla_allen_ids:\n",
    "            print(allen_info[id]['safeName'])\n",
    "        \n",
    "    return in_medulla_allen_ids\n",
    "\n",
    "def get_terminals(filename, neuron):\n",
    "    neurons = read_mouselight(filename)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    \n",
    "    parent_id = []\n",
    "    all_samples = np.arange(0, len(axon_sample_list), 1) + 1\n",
    "    for sample in range(len(axon_sample_list)):\n",
    "        parent_id.append(axon_sample_list[sample]['parentNumber'])\n",
    "    unique_parent_samples = np.unique(parent_id)\n",
    "    axon_terminals = list(set(all_samples) - set(unique_parent_samples))\n",
    "    print(f'Found {len(axon_terminals)} axon terminals')\n",
    "    \n",
    "    axon_terminal_sample_ind = np.array(axon_terminals) - 1 \n",
    "    #since sample numbers begin with 1, subtract 1 to get the sample index\n",
    "    \n",
    "    return axon_terminal_sample_ind\n",
    "\n",
    "def get_collaterals(filename, neuron):\n",
    "    neurons = read_mouselight(filename)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']     \n",
    "    parent_id = []\n",
    "    all_samples = np.arange(0, len(axon_sample_list), 1) + 1\n",
    "    for sample in range(len(axon_sample_list)):\n",
    "        parent_id.append(axon_sample_list[sample]['parentNumber'])\n",
    "    unique_parent_samples, children_counts = np.unique(parent_id, return_counts=True)\n",
    "    branches = unique_parent_samples[children_counts > 1]\n",
    "    print(f'Found {len(branches)} branches in whole brain')\n",
    "    branches_sample_ind = np.array(branches) - 1\n",
    "    return branches_sample_ind\n",
    "    \n",
    "def get_axons(filename, neuron):\n",
    "    neurons = read_mouselight(filename)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    \n",
    "    parent_id = []\n",
    "    all_samples = np.arange(0, len(axon_sample_list), 1) + 1\n",
    "    \n",
    "    axon_sample_ind = all_samples - 1 \n",
    "    #since sample numbers begin with 1, subtract 1 to get the sample index\n",
    "    \n",
    "    return axon_sample_ind\n",
    "\n",
    "def get_neuron_id(filename, neuron):\n",
    "    neurons = read_mouselight(filename)\n",
    "    neuron_id = neurons['neurons'][neuron]['idString']\n",
    "    \n",
    "    return neuron_id\n",
    "\n",
    "def get_soma(filename, neuron):\n",
    "    neurons = read_mouselight(filename)\n",
    "    soma = neurons['neurons'][neuron]['soma']\n",
    "    soma_coord = [soma['z'], soma['y'], soma['x']] #AP, DV, ML\n",
    "    \n",
    "    return soma_coord  \n",
    "    \n",
    "def get_ctx_region(filename, neuron):\n",
    "    soma_coord = get_soma(filename, neuron)\n",
    "\n",
    "    #coords need to be a numpy array [ap, dv, ml]\n",
    "    bregma = [5400, 0, 5700]\n",
    "    ap = (bregma[0] - soma_coord[0])/1000\n",
    "    dv = (soma_coord[1] - bregma[1])/1000\n",
    "    ml = abs((soma_coord[2] - bregma[2])/1000)\n",
    "\n",
    "    if (ap>0.865) & (ml<1.5):\n",
    "        ctx = 'mac'\n",
    "    elif (ap<0.865) & (ap>0) & (ml<3):\n",
    "        ctx = 'cfa'\n",
    "    elif (ml>1.5) & (ap>0.1) & (ml<3):\n",
    "        ctx = 'lac'\n",
    "    elif (ap<0):\n",
    "        ctx = 'pos'\n",
    "    else:\n",
    "        ctx = 'other'\n",
    "\n",
    "    return ctx\n",
    "\n",
    "def get_terminals_in_medulla(filename, neuron, savedir, contra=1, exp=None, bregma = [5400, 0, 5700]):\n",
    "    neurons = read_mouselight(filename)\n",
    "    in_medulla_allen_ids = get_medulla_id(filename, neuron)\n",
    "    axon_terminal_sample_ind = get_terminals(filename, neuron)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    soma_coord = get_soma(filename, neuron)\n",
    "    soma_ml = soma_coord[2] - bregma[2]\n",
    "    neuron_id = get_neuron_id(filename, neuron)\n",
    "    ctx = get_ctx_region(filename, neuron)\n",
    "    \n",
    "    terminal_coords = []\n",
    "    for terminal in axon_terminal_sample_ind:\n",
    "        terminal_allen_id = axon_sample_list[terminal]['allenId']\n",
    "        if terminal_allen_id in in_medulla_allen_ids:\n",
    "            coord = [[axon_sample_list[terminal]['z'], axon_sample_list[terminal]['y'], axon_sample_list[terminal]['x']]]\n",
    "            if contra:\n",
    "                term_ml = coord[0][2] - bregma[2]\n",
    "                if term_ml * soma_ml > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    terminal_coords.extend(coord)\n",
    "            else:\n",
    "                terminal_coords.extend(coord)\n",
    "        \n",
    "    print(f'There are {len(terminal_coords)} terminals in the medulla')\n",
    "\n",
    "    if terminal_coords:\n",
    "        if exp:\n",
    "            if savedir:\n",
    "                savepath = os.path.join(savedir, f'{ctx}_terminal_coords_{neuron_id}.npy')\n",
    "                np.save(os.path.join(savedir, f'{ctx}_terminal_coords_{neuron_id}.npy'), np.array(terminal_coords).astype(int))\n",
    "\n",
    "def get_terminals_in_region(filename, neuron, savedir, contra=1, ipsi = 0, exp=None, bregma = [5400, 0, 5700], region = '354'):\n",
    "    neurons = read_mouselight(filename)\n",
    "    in_medulla_allen_ids = get_medulla_id(filename, neuron, medulla_allen_id = region)\n",
    "    axon_terminal_sample_ind = get_terminals(filename, neuron)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    soma_coord = get_soma(filename, neuron)\n",
    "    soma_ml = soma_coord[2] - bregma[2]\n",
    "    neuron_id = get_neuron_id(filename, neuron)\n",
    "    ctx = get_ctx_region(filename, neuron)\n",
    "    \n",
    "    terminal_coords = []\n",
    "    for terminal in axon_terminal_sample_ind:\n",
    "        terminal_allen_id = axon_sample_list[terminal]['allenId']\n",
    "        if terminal_allen_id in in_medulla_allen_ids:\n",
    "            coord = [[axon_sample_list[terminal]['z'], axon_sample_list[terminal]['y'], axon_sample_list[terminal]['x']]]\n",
    "            if contra:\n",
    "                term_ml = coord[0][2] - bregma[2]\n",
    "                if term_ml * soma_ml > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    terminal_coords.extend(coord)\n",
    "            elif ipsi:\n",
    "                term_ml = coord[0][2] - bregma[2]\n",
    "                if term_ml * soma_ml < 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    terminal_coords.extend(coord)\n",
    "            else:\n",
    "                terminal_coords.extend(coord)\n",
    "        \n",
    "    print(f'There are {len(terminal_coords)} terminals')\n",
    "\n",
    "    if terminal_coords:\n",
    "        if exp:\n",
    "            if savedir:\n",
    "                savepath = os.path.join(savedir, f'{ctx}_{region}_terminal_coords_{neuron_id}.npy')\n",
    "                np.save(os.path.join(savedir, f'{ctx}_{region}_terminal_coords_{neuron_id}.npy'), np.array(terminal_coords).astype(int))        \n",
    "                \n",
    "def get_collaterals_in_region(filename, neuron, savedir, contra=1, ipsi = 0, exp=None, bregma = [5400, 0, 5700], region = '354'):\n",
    "    neurons = read_mouselight(filename)\n",
    "    in_medulla_allen_ids = get_medulla_id(filename, neuron, medulla_allen_id = region)\n",
    "    axon_terminal_sample_ind = get_collaterals(filename, neuron)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    soma_coord = get_soma(filename, neuron)\n",
    "    soma_ml = soma_coord[2] - bregma[2]\n",
    "    neuron_id = get_neuron_id(filename, neuron)\n",
    "    ctx = get_ctx_region(filename, neuron)\n",
    "    print(f'processing {len(axon_terminal_sample_ind)} branches')\n",
    "    terminal_coords = []\n",
    "    for terminal in axon_terminal_sample_ind:\n",
    "        terminal_allen_id = axon_sample_list[terminal]['allenId']\n",
    "        if terminal_allen_id in in_medulla_allen_ids:\n",
    "            coord = [[axon_sample_list[terminal]['z'], axon_sample_list[terminal]['y'], axon_sample_list[terminal]['x']]]\n",
    "            if contra:\n",
    "                term_ml = coord[0][2] - bregma[2]\n",
    "                if term_ml * soma_ml > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    terminal_coords.extend(coord)\n",
    "            elif ipsi:\n",
    "                term_ml = coord[0][2] - bregma[2]\n",
    "                if term_ml * soma_ml < 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    terminal_coords.extend(coord)\n",
    "            else:\n",
    "                terminal_coords.extend(coord)\n",
    "        \n",
    "    print(f'There are {len(terminal_coords)} collaterals in region')\n",
    "\n",
    "    if terminal_coords:\n",
    "        if exp:\n",
    "            if savedir:\n",
    "                savepath = os.path.join(savedir, f'{ctx}_{region}_collaterals_coords_{neuron_id}.npy')\n",
    "                np.save(os.path.join(savedir, f'{ctx}_{region}_collaterals_coords_{neuron_id}.npy'), np.array(terminal_coords).astype(int))\n",
    "\n",
    "def get_collaterals_in_pyr(filename, neuron, savedir, contra=1, ipsi = 0, exp=None, bregma = [5400, 0, 5700], region = '354'):\n",
    "    region = '190'\n",
    "    neurons = read_mouselight(filename)\n",
    "    in_medulla_allen_ids = get_medulla_id(filename, neuron, medulla_allen_id = region)\n",
    "    axon_terminal_sample_ind = get_collaterals(filename, neuron)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    soma_coord = get_soma(filename, neuron)\n",
    "    soma_ml = soma_coord[2] - bregma[2]\n",
    "    neuron_id = get_neuron_id(filename, neuron)\n",
    "    ctx = get_ctx_region(filename, neuron)\n",
    "    print(f'Proessing {len(axon_terminal_sample_ind)} branches')\n",
    "    terminal_coords = []\n",
    "    for terminal in axon_terminal_sample_ind:\n",
    "        terminal_allen_id = axon_sample_list[terminal]['allenId']\n",
    "        coord = [[axon_sample_list[terminal]['z'], axon_sample_list[terminal]['y'], axon_sample_list[terminal]['x']]]\n",
    "        pax_coord  = allen_to_paxinos(coord[0])\n",
    "        term_ml = coord[0][2] - bregma[2]\n",
    "        \n",
    "        if np.abs(term_ml) < 500 and pax_coord[0]< -3.5:\n",
    "            terminal_coords.extend(coord)\n",
    "        \n",
    "    print(f'There are {len(terminal_coords)} putative collaterals in pyramidal tract')\n",
    "\n",
    "    if terminal_coords:\n",
    "        if exp:\n",
    "            if savedir:\n",
    "                savepath = os.path.join(savedir, f'{ctx}_{region}_collaterals_coords_{neuron_id}.npy')\n",
    "                np.save(os.path.join(savedir, f'{ctx}_{region}_collaterals_coords_{neuron_id}.npy'), np.array(terminal_coords).astype(int))\n",
    "                       \n",
    "        \n",
    "def get_axons_in_medulla(filename, neuron, savedir, contra=1, exp=None, bregma = [5400, 0, 5700]):\n",
    "    neurons = read_mouselight(filename)\n",
    "    in_medulla_allen_ids = get_medulla_id(filename, neuron)\n",
    "    axon_terminal_sample_ind = get_axons(filename, neuron)\n",
    "    axon_sample_list = neurons['neurons'][neuron]['axon']\n",
    "    soma_coord = get_soma(filename, neuron)\n",
    "    soma_ml = soma_coord[2] - bregma[2]\n",
    "    neuron_id = get_neuron_id(filename, neuron)\n",
    "    ctx = get_ctx_region(filename, neuron)\n",
    "    \n",
    "    terminal_coords = []\n",
    "    for terminal in axon_terminal_sample_ind:\n",
    "        terminal_allen_id = axon_sample_list[terminal]['allenId']\n",
    "        if terminal_allen_id in in_medulla_allen_ids:\n",
    "            coord = [[axon_sample_list[terminal]['z'], axon_sample_list[terminal]['y'], axon_sample_list[terminal]['x']]]\n",
    "            if contra:\n",
    "                term_ml = coord[0][2] - bregma[2]\n",
    "                if term_ml * soma_ml > 0:\n",
    "                    continue\n",
    "                else:\n",
    "                    terminal_coords.extend(coord)\n",
    "            else:\n",
    "                terminal_coords.extend(coord)\n",
    "        \n",
    "    print(f'There are {len(terminal_coords)} axon samples in the medulla')\n",
    "\n",
    "    if terminal_coords:\n",
    "        if exp:\n",
    "            if savedir:\n",
    "                savepath = os.path.join(savedir, f'{ctx}_axon_coords_{neuron_id}.npy')\n",
    "                np.save(os.path.join(savedir, f'{ctx}_axon_coords_{neuron_id}.npy'), np.array(terminal_coords).astype(int))\n",
    "        \n",
    "def run_all_neurons(filename, savedir, exp=None, terminal=None, axon=None, contra = 1):\n",
    "    neurons = read_mouselight(filename)\n",
    "    for nn in range(len(neurons['neurons'])):\n",
    "        neuron_id = neurons['neurons'][nn]['idString']\n",
    "        print(f'{neuron_id}')\n",
    "        if terminal:\n",
    "            get_terminals_in_medulla(filename, nn, savedir, exp=exp)\n",
    "        if axon: \n",
    "            get_axons_in_medulla(filename, nn, savedir, exp=exp)\n",
    "        \n",
    "def run_all_neurons_region(filename, savedir, exp=None, terminal=None, axon=None, collateral = None, region = '354', contra = 1, ipsi = 0):\n",
    "    neurons = read_mouselight(filename)\n",
    "    for nn in range(len(neurons['neurons'])):\n",
    "        neuron_id = neurons['neurons'][nn]['idString']\n",
    "        print(f'{neuron_id}')\n",
    "        if terminal:\n",
    "            get_terminals_in_region(filename, nn, savedir, exp=exp,contra = contra, ipsi = ipsi, region = region)\n",
    "        if axon: \n",
    "            get_axons_in_medulla(filename, nn, savedir, exp=exp)\n",
    "        if collateral:\n",
    "            get_collaterals_in_pyr(filename, nn, savedir, exp=exp,contra = contra, ipsi = ipsi, region = region)\n",
    "            \n",
    "def all_neuron_idx(filename):\n",
    "    neurons = read_mouselight(filename)\n",
    "    neuron_idx = pd.DataFrame()\n",
    "    id_lst = []\n",
    "    for nn in range(len(neurons['neurons'])):\n",
    "        neuron_id = neurons['neurons'][nn]['idString']\n",
    "        id_lst.append(neuron_id)\n",
    "    neuron_idx['index'] = range(len(neurons['neurons']))\n",
    "    neuron_idx['id'] = id_lst\n",
    "    \n",
    "    return neuron_idx\n",
    "\n",
    "def all_soma(filename, datapath, region = '354', datatype='mouselight'):\n",
    "    if datatype == \"mouselight\":\n",
    "        neuron_idx = all_neuron_idx(filename)\n",
    "\n",
    "    soma = []\n",
    "    ctx_lst = []\n",
    "    axon_lst = []\n",
    "    term_lst = []\n",
    "    axon_num_lst = []\n",
    "    term_num_lst = []\n",
    "    coll_lst = []\n",
    "    coll_num_lst = []\n",
    "    for ii in neuron_idx.index:\n",
    "        soma_coord = allen_to_paxinos(get_soma(filename, ii))\n",
    "        soma_coord[2] = abs(soma_coord[2])\n",
    "        soma.append(soma_coord)\n",
    "        \n",
    "        ctx = get_ctx_region(filename, ii)\n",
    "        ctx_lst.append(ctx)\n",
    "        \n",
    "        nid = neuron_idx['id'].iloc[ii]\n",
    "        axon_file = f'{ctx}_{region}_axon_coords_{nid}.npy'\n",
    "        term_file = f'{ctx}_{region}_terminal_coords_{nid}.npy'\n",
    "        collateral_file = f'{ctx}_{region}_collaterals_coords_{nid}.npy'\n",
    "        axon_lst.append(axon_file)\n",
    "        term_lst.append(term_file)\n",
    "        coll_lst.append(collateral_file)\n",
    "        \n",
    "        try:\n",
    "            axon_coords_allen = np.load(os.path.join(datapath, axon_file))\n",
    "        except FileNotFoundError:\n",
    "            axon_num = 0\n",
    "        else:\n",
    "            axon_num = len(axon_coords_allen)\n",
    "        axon_num_lst.append(axon_num)\n",
    "        \n",
    "        try:\n",
    "            term_coords_allen = np.load(os.path.join(datapath, term_file))\n",
    "        except FileNotFoundError:\n",
    "            term_num = 0\n",
    "        else:\n",
    "            term_num = len(term_coords_allen)\n",
    "        term_num_lst.append(term_num)\n",
    "        \n",
    "        try:\n",
    "            collateral_coords_allen = np.load(os.path.join(datapath, collateral_file))\n",
    "        except FileNotFoundError:\n",
    "            coll_num = 0\n",
    "        else:\n",
    "            coll_num = len(collateral_coords_allen)\n",
    "        coll_num_lst.append(coll_num)\n",
    "\n",
    "    soma = np.array(soma)\n",
    "    \n",
    "    neuron_idx['x'] = soma[:,2]\n",
    "    neuron_idx['y'] = soma[:,0]\n",
    "    neuron_idx['z'] = soma[:,1]\n",
    "    neuron_idx['ctx'] = ctx_lst\n",
    "    neuron_idx['axon_file'] = axon_lst\n",
    "    neuron_idx['term_file'] = term_lst\n",
    "    neuron_idx['axon_num'] = axon_num_lst\n",
    "    neuron_idx[f'term_num_{region}'] = term_num_lst\n",
    "    neuron_idx[f'coll_num_{region}'] = coll_num_lst\n",
    "    neuron_idx['coll_file'] = coll_lst\n",
    "    return neuron_idx\n",
    "\n",
    "ms_table = pd.read_csv(datapath+r'\\term_num_Medulla.csv', index_col=0)\n",
    "lst=[]\n",
    "for i in ms_table['ctx']:\n",
    "    if i == 'mac':\n",
    "        lst.append('darkmagenta')\n",
    "    elif i == 'lac':\n",
    "        lst.append('darkcyan')\n",
    "    elif i == 'ins':\n",
    "        lst.append('yellowgreen')\n",
    "    else:\n",
    "        lst.append('grey')\n",
    "ms_table[\"color\"]=lst"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0837548",
   "metadata": {},
   "source": [
    "## soma location and the distribution of axon terminals in DV, ML and AP axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df3c7b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot the soma location\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca()\n",
    "\n",
    "for ii, jj in enumerate(ms_table.index):\n",
    "    nid = ms_table['id'].iloc[ii]\n",
    "    text = f'{nid}'\n",
    "    ax.text(x=np.absolute(ms_table['x']).iloc[ii]+0.03, y = ms_table['y'].iloc[ii], s=text, size=10)\n",
    "    \n",
    "plt.axvline(1.5, ls='--')\n",
    "plt.axhline(1, ls='--')\n",
    "plt.scatter(x=np.absolute(ms_table['x']), y = ms_table['y'], c = ms_table[\"color\"], s=500)\n",
    "\n",
    "plt.title('soma location of mouselight neurons')\n",
    "plt.xlabel('ML (mm)')\n",
    "plt.ylabel('AP (mm)')\n",
    "plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2a5578",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coronal view of mouselight axon terminals in medulla\n",
    "\n",
    "ap = [-7.8, -5]\n",
    "dv = [5, 7.2]\n",
    "ml = [-2.5, 0]\n",
    "\n",
    "med_axon_term_mac_mouselight = allen_to_paxinos(np.load(datapath+'\\medulla_axon_term_mac_354_mouselight.npy'))\n",
    "med_axon_term_lac_mouselight = allen_to_paxinos(np.load(datapath+'\\medulla_axon_term_lac_354_mouselight.npy'))\n",
    "\n",
    "med_axon_term_mac_mouselight = np.asarray(med_axon_term_mac_mouselight)\n",
    "med_axon_term_lac_mouselight = np.asarray(med_axon_term_lac_mouselight)\n",
    "\n",
    "x_mac_mouselight = med_axon_term_mac_mouselight[:, 2]\n",
    "y_mac_mouselight = med_axon_term_mac_mouselight[:, 1]\n",
    "x_lac_mouselight = med_axon_term_lac_mouselight[:, 2]\n",
    "y_lac_mouselight = med_axon_term_lac_mouselight[:, 1]\n",
    "\n",
    "xx, yy = np.mgrid[ml[0]: ml[1]:100j, dv[0]:dv[1]:100j]\n",
    "positions = np.vstack([xx.ravel(), yy.ravel()])\n",
    "f_syn_mac_mouselight = np.reshape(np.zeros(100**2), xx.shape)\n",
    "f_syn_lac_mouselight = np.reshape(np.zeros(100**2), xx.shape)\n",
    "\n",
    "values_mac_mouselight = np.vstack([x_mac_mouselight, y_mac_mouselight])\n",
    "kernel_mac_mouselight = st.gaussian_kde(values_mac_mouselight)\n",
    "f_mac_mouselight = np.reshape(kernel_mac_mouselight(positions).T, xx.shape)\n",
    "f_syn_mac_mouselight += f_mac_mouselight\n",
    "\n",
    "values_lac_mouselight = np.vstack([x_lac_mouselight, y_lac_mouselight])\n",
    "kernel_lac_mouselight = st.gaussian_kde(values_lac_mouselight)\n",
    "f_lac_mouselight = np.reshape(kernel_lac_mouselight(positions).T, xx.shape)\n",
    "f_syn_lac_mouselight += f_lac_mouselight\n",
    "\n",
    "norm_mac_mouselight_kde = f_syn_mac_mouselight / f_syn_mac_mouselight.max()\n",
    "norm_lac_mouselight_kde = f_syn_lac_mouselight / f_syn_lac_mouselight.max()\n",
    "\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "ax = fig.gca()\n",
    "levels = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "cset_lac_mouselight = ax.contour(xx, dv[1]-yy, norm_lac_mouselight_kde, levels= [0.3, 0.45, 0.6, 0.75, 0.9], colors='darkcyan')\n",
    "cset_mac_mouselight = ax.contour(xx, dv[1]-yy, norm_mac_mouselight_kde, levels= [0.3, 0.45, 0.6, 0.75, 0.9], colors='darkmagenta')\n",
    "\n",
    "plt.xlabel('ML (mm)')\n",
    "plt.ylabel('DV (mm)')\n",
    "plt.title('Distribution of axon terminals from mouselight')\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e12306",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Distribution of axon terminals in ML\n",
    "bw = 0.1\n",
    "x_d = np.linspace(ml[0], ml[1], 100)\n",
    "kde = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "kde.fit(np.reshape(x_mac_mouselight, (len(x_mac_mouselight),1)))\n",
    "logprob = kde.score_samples(x_d[:, None])\n",
    "plt.plot(x_d, np.exp(logprob), c='darkmagenta')\n",
    "\n",
    "kde = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "kde.fit(np.reshape(x_lac_mouselight, (len(x_lac_mouselight),1)))\n",
    "logprob = kde.score_samples(x_d[:, None])\n",
    "plt.plot(x_d, np.exp(logprob), c='darkcyan')\n",
    "\n",
    "plt.xlabel('ML (mm)')\n",
    "plt.ylabel(\"KDE\")\n",
    "plt.title('Distribution of axon terminals in ML mouselight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02139a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of axon terminals in DV\n",
    "y_d = np.linspace(dv[0], dv[1], 100)\n",
    "kde = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "kde.fit(np.reshape(y_mac_mouselight, (len(y_mac_mouselight),1)))\n",
    "logprob = kde.score_samples(y_d[:, None])\n",
    "plt.plot(dv[1]-y_d, np.exp(logprob), c='darkmagenta')\n",
    "\n",
    "kde = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "kde.fit(np.reshape(y_lac_mouselight, (len(y_lac_mouselight),1)))\n",
    "logprob = kde.score_samples(y_d[:, None])\n",
    "plt.plot(dv[1]-y_d, np.exp(logprob), c='darkcyan')\n",
    "\n",
    "plt.xlabel('DV (mm)')\n",
    "plt.ylabel(\"KDE\")\n",
    "plt.title('Distribution of axon terminals in DV mouselight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ce6b5d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Distribution of axon terminals in AP\n",
    "bw = 0.1\n",
    "bins = np.linspace(ap[0], ap[1], 100)\n",
    "line_all = pd.DataFrame()\n",
    "\n",
    "for idx, ii in enumerate(ms_table.index):\n",
    "    reg = medulla\n",
    "    ctx = ms_table['ctx'].loc[ii]\n",
    "    neuron_id = ms_table['id'].loc[ii]\n",
    "    f_config = ms_table['term_file'].loc[ii].split('_')\n",
    "    f_config[1] = reg\n",
    "    fpath = os.path.join(datapath, '_'.join(f_config))\n",
    "    \n",
    "    if os.path.exists(fpath):\n",
    "        axon = allen_to_paxinos(np.load(fpath))\n",
    " \n",
    "    plt.figure(figsize=(10,10))\n",
    "    plt.title(f'Distribution of axon terminals in the medulla for {ctx} {neuron_id}')\n",
    "    kde = KernelDensity(bandwidth=bw, kernel='gaussian')\n",
    "    x = axon[:,0]\n",
    "    kde.fit(np.reshape(x, (len(x),1)))\n",
    "    logprob = kde.score_samples(bins[:, None])\n",
    "    plt.plot(bins, np.exp(logprob), color=ms_table['color'].loc[ii], alpha=1)\n",
    "    line_all[idx] = np.exp(logprob)\n",
    "\n",
    "    plt.axhline(-0, ls='--', lw=1, c='k', alpha=0.5)\n",
    "\n",
    "    plt.axvline(-5, ls='--', lw=1, c='k', alpha=0.5)\n",
    "    plt.axvline(-6, ls='--', lw=1, c='k', alpha=0.5)\n",
    "    plt.axvline(-6.7, ls='--', lw=1, c='k', alpha=0.5)\n",
    "    plt.text(-5.6, -0.04, '7N level')\n",
    "    plt.text(-6.5, -0.04, 'Int level')\n",
    "    plt.text(-7.4, -0.04, '12N level')\n",
    "    plt.xlabel('AP (mm)')\n",
    "    plt.xlim(ap[0], ap[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6833c833",
   "metadata": {},
   "source": [
    "# Figure 5 -- LOF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cb972a6",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6329c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os, requests\n",
    "import pandas as pd\n",
    "import math\n",
    "import sys\n",
    "import glob\n",
    "from pathlib import Path\n",
    "from scipy import ndimage\n",
    "from matplotlib import rc, patches\n",
    "rc(\"pdf\", fonttype=42)\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.stats import mannwhitneyu\n",
    "import scikit_posthocs as sp\n",
    "import itertools\n",
    "import math\n",
    "import pingouin as pg\n",
    "\n",
    "class bcolors:\n",
    "    HEADER = '\\033[95m'\n",
    "    OKBLUE = '\\033[94m'\n",
    "    OKGREEN = '\\033[92m'\n",
    "    WARNING = '\\033[93m'\n",
    "    FAIL = '\\033[91m'\n",
    "    ENDC = '\\033[0m'\n",
    "    BOLD = '\\033[1m'\n",
    "    UNDERLINE = '\\033[4m'\n",
    "    \n",
    "def rotate(origin, point, angle):\n",
    "    \"\"\"\n",
    "    Rotate a point counterclockwise by a given angle around a given origin.\n",
    "\n",
    "    The angle should be given in radians.\n",
    "    \"\"\"\n",
    "    ox, oy = origin\n",
    "    px, py = point\n",
    "\n",
    "    qx = ox + math.cos(angle) * (px - ox) - math.sin(angle) * (py - oy)\n",
    "    qy = oy + math.sin(angle) * (px - ox) + math.cos(angle) * (py - oy)\n",
    "    return qx, qy\n",
    "\n",
    "def line_intersection(df1, df2, df3, df4):\n",
    "    int_point = pd.DataFrame()\n",
    "    x = np.zeros(df1.shape[0])\n",
    "    y = np.zeros(df1.shape[0])\n",
    "    for ii in range(df1.shape[0]):\n",
    "        line1 = ((df1.iloc[ii,0],df1.iloc[ii,1]),(df2.iloc[ii,0],df2.iloc[ii,1]))\n",
    "        line2 = ((df3.iloc[ii,0],df3.iloc[ii,1]),(df4.iloc[ii,0],df4.iloc[ii,1]))\n",
    "        xdiff = (line1[0][0] - line1[1][0], line2[0][0] - line2[1][0])\n",
    "        ydiff = (line1[0][1] - line1[1][1], line2[0][1] - line2[1][1])\n",
    "\n",
    "        def det(a, b):\n",
    "            return a[0] * b[1] - a[1] * b[0]\n",
    "\n",
    "        div = det(xdiff, ydiff)\n",
    "        if div == 0:\n",
    "            x[ii] = (line1[0][0]  + line2[1][0] + line1[1][0] + line2[0][0])/4\n",
    "            y[ii] = (line1[0][1]  + line2[1][1] + line1[1][1] + line2[0][1])/4\n",
    "        else:\n",
    "            d = (det(*line1), det(*line2))\n",
    "            x[ii] = det(d, xdiff) / div\n",
    "            y[ii]= det(d, ydiff) / div\n",
    "    int_point['int_point'] = x\n",
    "    int_point['int_point.1'] = y\n",
    "    return int_point\n",
    "\n",
    "def clean_lkhood(trackdata, thresh = 0.6, exclude_thresh = 20, trial_len = [150]):\n",
    "    lkhood = trackdata.iloc[:,2]\n",
    "    mask = lkhood < thresh\n",
    "    nan_trackdata = trackdata.iloc[:,0:2]\n",
    "    nan_trackdata[mask] = np.NaN\n",
    "    inc_trials = []\n",
    "    n_trials = len(trial_len)\n",
    "  \n",
    "    sum_len = 0\n",
    "    count = 0  \n",
    "    clean_trackdata = pd.DataFrame()\n",
    "    bp = trackdata.columns[0]\n",
    "    for ii in range(n_trials):\n",
    "        #count number of nans in the trial \n",
    "        trial_df = nan_trackdata.iloc[sum_len:sum_len+ trial_len[ii], 0:2]\n",
    "        n_nans = trial_df.isnull().sum(axis = 0)[0]\n",
    "        interpolated_trial = trial_df.interpolate(method='spline', order=2, limit_direction='both')\n",
    "        if n_nans <= exclude_thresh:\n",
    "            interpolated_trial[f'to_include_{bp}'] = True\n",
    "            inc_trials.append(ii)\n",
    "        else:\n",
    "            interpolated_trial[f'to_include_{bp}'] = False\n",
    "            count = count +1\n",
    "        sum_len += trial_len[ii]\n",
    "    \n",
    "        clean_trackdata = pd.concat([clean_trackdata, interpolated_trial])\n",
    "\n",
    "    print (f'{bcolors.FAIL}{count}{bcolors.ENDC} trial(s) to be excluded due to unreliable tracking for {bcolors.WARNING}{bp}{bcolors.ENDC}. Consider changing threshold of this is too high')\n",
    "    return clean_trackdata, inc_trials\n",
    "\n",
    "def extract_body_parts(dlc_df, body_parts, thresh = 0.6, exclude_thresh = 20, trial_len = [150]):\n",
    "    trial_set_list = []\n",
    "    dlc_clean = pd.DataFrame()\n",
    "\n",
    "    for part in body_parts:\n",
    "        bp, inc_trials = clean_lkhood(dlc_df.loc[:, [part, f'{part}.1', f'{part}.2']], thresh, exclude_thresh, trial_len)\n",
    "        dlc_clean = pd.concat([dlc_clean,bp], axis=1)\n",
    "        trial_set_list.append(set(inc_trials))\n",
    "    include_cols = [col for col in dlc_clean.columns if 'to_include' in col]\n",
    "\n",
    "    mask = np.zeros(dlc_clean.shape[0])\n",
    "    for idx in range(dlc_clean.shape[0]):\n",
    "        if dlc_clean.loc[:,include_cols].iloc[idx,:].all():\n",
    "            mask[idx] = True\n",
    "    dlc_clean['mask'] = mask\n",
    "    dlc_clean_masked = dlc_clean[dlc_clean['mask'] == True]\n",
    "    dlc_clean_masked = dlc_clean_masked.drop(columns = include_cols) \n",
    "    dlc_clean_masked = dlc_clean_masked.drop(columns = 'mask') \n",
    "    inc_trials_final = list(set.intersection(*trial_set_list))\n",
    "\n",
    "    print(f\"{bcolors.FAIL}{int(len(trial_len) -len(inc_trials_final))}{bcolors.ENDC} trial(s) excluded overall /n {bcolors.OKGREEN}{len(inc_trials_final)}{bcolors.ENDC} kept\")\n",
    "\n",
    "    return dlc_clean_masked, inc_trials_final\n",
    "\n",
    "def distance_comp(df1, df2):\n",
    "\n",
    "    dx = df1.iloc[:, 0].subtract(df2.iloc[:,0])\n",
    "    dx = np.asarray(dx.abs())\n",
    "    \n",
    "    dy = df2.iloc[:, 1].subtract(df2.iloc[:,1])\n",
    "    dy = np.asarray(dy.abs())\n",
    "\n",
    "    dist= np.hypot(dx,dy)\n",
    "\n",
    "    return dist\n",
    "\n",
    "def get_summary_df(mouse):\n",
    "    CTXs = ['LAC', 'MAC', 'PBS']\n",
    "    frame_rate = 100 #Hz\n",
    "    pixel_size = 0.0147\n",
    "\n",
    "    annot = {}\n",
    "    reach = {}\n",
    "    eat = {}\n",
    "    eat_fl = {}\n",
    "    eat_mouth = {}\n",
    "    retract = {}\n",
    "    for ctx in CTXs:\n",
    "        dir = f'{datapath}/{ctx}/n{mouse}/' ## folder where all data is \n",
    "        annot[ctx] = pd.read_csv(Path(dir, 'Annot.csv'))\n",
    "        annot[ctx].fillna(0, inplace = True)\n",
    "        behavior_groups = annot[ctx].groupby('Behavior')\n",
    "        if any(annot[ctx].Behavior == 'Reach'):\n",
    "            reach[ctx] = behavior_groups.get_group('Reach')\n",
    "            retract[ctx] = behavior_groups.get_group('Retract')\n",
    "        else:\n",
    "            reach[ctx] = pd.DataFrame()\n",
    "            retract[ctx] = pd.DataFrame()\n",
    "\n",
    "        eat[ctx]  = behavior_groups.get_group('Eat')\n",
    "        eat_fl[ctx]  = eat[ctx][eat[ctx].Handle_FL.isin([1])]\n",
    "        eat_mouth[ctx] = eat[ctx][eat[ctx].Handle_FL.isin([0])]\n",
    "        if ctx == 'MAC':\n",
    "            engage = behavior_groups.get_group('Engage')\n",
    "\n",
    "    summary_df = pd.DataFrame(columns = CTXs)\n",
    "    for ctx in CTXs:\n",
    "        prop_FL_bouts = np.sum(eat[ctx].Handle_FL)/eat[ctx].shape[0]\n",
    "        regrips = np.sum(eat[ctx].regrip)/(np.sum(eat[ctx].iloc[:,6])/frame_rate)\n",
    "        #regrips = np.sum(eat[ctx].regrip)/(eat[ctx].shape[0])\n",
    "        if np.sum(eat[ctx].regrip) == 0:\n",
    "            regrips = 0\n",
    "        prop_mouth_eat = 1-prop_FL_bouts\n",
    "        prop_drop_handle = np.sum(eat[ctx].Handle_drop)/np.sum(eat[ctx].Handle_FL)\n",
    "        n_reaches = reach[ctx].shape[0]\n",
    "        n_reaches_per_min = n_reaches/(annot[ctx].Stop.max()/frame_rate/60)\n",
    "        \n",
    "        summary_df.loc['prop_mouth_eat',ctx] = prop_mouth_eat\n",
    "        summary_df.loc['prop_drop_handle',ctx] = prop_drop_handle\n",
    "        summary_df.loc['n_reaches_per_min',ctx] = n_reaches_per_min\n",
    "        summary_df.loc['regrip',ctx] = regrips\n",
    "        summary_df_t = summary_df.T\n",
    "        summary_df_t.insert(0, 'ctx', summary_df_t.index)\n",
    "        summary_df_t['prop_mouth_eat'] = summary_df_t['prop_mouth_eat'].astype(float)\n",
    "        summary_df_t['prop_drop_handle'] = summary_df_t['prop_drop_handle'].astype(float)\n",
    "        summary_df_t['n_reaches_per_min'] = summary_df_t['n_reaches_per_min'].astype(float)\n",
    "        summary_df_t['regrip'] = summary_df_t['regrip'].astype(float)\n",
    "    \n",
    "    return summary_df_t\n",
    "    \n",
    "\n",
    "datapath = root+r'\\Fig 5'\n",
    "pixel_size = 0.0147"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26d93aa6",
   "metadata": {},
   "source": [
    "## plot trajectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "935e6d97",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot reaching trajectories in PBS and LAC silencing conditions\n",
    "\n",
    "CTXs = ['PBS','LAC']\n",
    "pop = 'Fig 5'\n",
    "mouse = 1 ## mouse number\n",
    "exp_dir = datapath\n",
    "bh = 'Reaching'\n",
    "body_parts = ['L_Hand']\n",
    "pixel_length = pixel_size  ####################################################\n",
    "mean_bin = 0.1 # in normalised time,i.e, 0.1 = reach divided in 10 bins\n",
    "bins = np.arange(0,1+mean_bin,mean_bin)\n",
    "av_trajectory = {} \n",
    "\n",
    "slit_df =  pd.read_excel(os.path.join(exp_dir , f'coord_0{mouse}.xlsx'))\n",
    "slit_df = slit_df[slit_df.iloc[:,0] == 'slit']\n",
    "\n",
    "reach_speed = pd.DataFrame(columns = ['Average', 'SEM'])\n",
    "av_reach_speeds = {}\n",
    "annot = {}\n",
    "reach = {}\n",
    "eat = {}\n",
    "eat_fl = {}\n",
    "eat_mouth = {}\n",
    "retract = {}\n",
    "max_extension_df = pd.DataFrame(columns = ['max'])\n",
    "for ctx in CTXs:\n",
    "    dir = f'{datapath}/{ctx}/n{mouse}/' ## folder where all data is \n",
    "    slit = slit_df.loc[slit_df.loc[:,'ctx'] == ctx,['x','y']]\n",
    "    slit.iloc[0,0] = 0\n",
    "    slit.iloc[0,1] = 0\n",
    "  \n",
    "    dlc_tracking = pd.read_csv(os.path.join(dir, f'{bh}_{ctx}_muscimolDLC_resnet50_POF_bottomDec13shuffle1_550000.csv'), delimiter=',', na_values='nan', skiprows=(0,2))\n",
    "  \n",
    "    annot[ctx] = pd.read_csv(Path(dir, 'Annot.csv'))\n",
    "    annot[ctx].fillna(0, inplace = True)\n",
    "    behavior_groups = annot[ctx].groupby('Behavior')\n",
    "    if any(annot[ctx].Behavior == 'Reach'):\n",
    "        reach[ctx] = behavior_groups.get_group('Reach')\n",
    "        retract[ctx] = behavior_groups.get_group('Retract')\n",
    "    else:\n",
    "        reach[ctx] = pd.DataFrame()\n",
    "        retract[ctx] = pd.DataFrame()\n",
    "  \n",
    "    eat[ctx]  = behavior_groups.get_group('Eat')\n",
    "    eat_fl[ctx]  = eat[ctx][eat[ctx].Handle_FL.isin([1])]\n",
    "    eat_mouth[ctx] = eat[ctx][eat[ctx].Handle_FL.isin([0])]\n",
    "    if ctx == 'MAC':\n",
    "        engage = behavior_groups.get_group('Engage')\n",
    "  \n",
    "    reach[ctx] = reach[ctx].reset_index()\n",
    "    retract[ctx] = retract[ctx].reset_index()\n",
    "    reach_pellet = reach[ctx][reach[ctx]['Pellet Touch'] == 1]\n",
    "    retract_pellet = retract[ctx][reach[ctx]['Pellet Touch'] == 1]\n",
    "    ts_start = reach_pellet.Start.values\n",
    "    ts_stop = retract_pellet.Stop.values\n",
    "    reach_end = reach_pellet.Stop.values\n",
    "    trial_len = ts_stop-ts_start\n",
    "    trial_len += 10\n",
    "    reach_len = reach_end - ts_start\n",
    "    exc_reaches = np.where(reach_len < 11)[0]\n",
    "    coords, inc_trials= extract_body_parts(dlc_tracking, body_parts, thresh = 0.4, exclude_thresh=5, trial_len = trial_len)\n",
    "    inc_trials = np.asarray(inc_trials)\n",
    "    print(f'number of nans in total {np.sum(np.isnan(coords.values))}')\n",
    "  \n",
    "    speed = coords.diff()\n",
    "    speed_val = pd.DataFrame()\n",
    "    for bp in body_parts:\n",
    "        speed_val[bp] = np.hypot(speed[bp], speed[f'{bp}.1'])\n",
    "  \n",
    "    coords_dict = {}\n",
    "    for part in body_parts:\n",
    "        colie = [col for col in coords.columns if part in col]\n",
    "        coords_dict[part] = coords[colie]\n",
    "  \n",
    "    trial_dict = {}\n",
    "    trials_speed = {}\n",
    "    sum_len = 0\n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        if trial_idx in exc_reaches:\n",
    "            sum_len += trial_len[trial_idx]\n",
    "            continue\n",
    "        else:\n",
    "            for part in body_parts:\n",
    "                trial_dict[f'{part}_{trial_idx}'] = coords_dict[part].iloc[sum_len: sum_len + trial_len[trial_idx]]\n",
    "                trials_speed[f'{part}_{trial_idx}'] = ndimage.median_filter(speed_val[bp][sum_len: sum_len + trial_len[trial_idx]], size=4)  * pixel_length * 100\n",
    "\n",
    "        sum_len += trial_len[trial_idx]\n",
    "    \n",
    "    for exc in exc_reaches:\n",
    "        inc_trials = inc_trials[inc_trials != exc]\n",
    "    print(f'{len(exc_reaches)} number of trials below minimum duration excluded')\n",
    "  \n",
    "    part = body_parts[0]\n",
    "    sum_len = 0\n",
    "    max_extension = []\n",
    "    plt.figure(figsize = (15,15), facecolor='w', edgecolor='k', frameon='FalseSC')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    binned_trajectory_x = np.zeros((len(inc_trials), len(bins)-1))\n",
    "    binned_trajectory_y = np.zeros((len(inc_trials), len(bins)-1))\n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        \n",
    "        key = f'{part}_{trial_idx}'\n",
    "        trial_df = trial_dict[key]\n",
    "    \n",
    "    \n",
    "        reach_trial = trial_df.iloc[10: 10+ reach_len[trial_idx]]\n",
    "        reach_trial_smooth_x = (savgol_filter(reach_trial.iloc[:, 0], 5,2) - slit.iloc[0,0])*pixel_length\n",
    "        reach_trial_smooth_y = (savgol_filter(reach_trial.iloc[:, 1], 5,2) - slit.iloc[0,1])*pixel_length\n",
    "        max_extension.append(np.max(reach_trial_smooth_x))\n",
    "        time = np.linspace(0,1,len(reach_trial))\n",
    "        \n",
    "        for ii in range(len(bins)-1):\n",
    "            time_pts = (time >= bins[ii]) & (time <= bins[ii+1])\n",
    "            binned_trajectory_x[idx, ii] = np.mean(reach_trial_smooth_x[time_pts])\n",
    "            binned_trajectory_y[idx, ii] = np.mean(reach_trial_smooth_y[time_pts])\n",
    "        plt.plot(reach_trial_smooth_x,reach_trial_smooth_y, color = 'w', alpha = 0.1)\n",
    "        plt.scatter(reach_trial_smooth_x,reach_trial_smooth_y, c= time, cmap=\"plasma\", alpha = 0.2, s= 25)\n",
    "    \n",
    "        plt.title(f'{pop}_{ctx}_{bh}')\n",
    "    plt.axis('equal')\n",
    "    \n",
    "    sum_len = 0\n",
    "    av_reach_speed = []\n",
    "    ax = plt.gca()\n",
    "\n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        key = f'{part}_{trial_idx}'\n",
    "        trial_df = trials_speed[key]\n",
    "        trial_coord = trial_dict[key]\n",
    "        reach_trial_coord = trial_coord.iloc[5: 10+ reach_len[trial_idx]]\n",
    "        \n",
    "        reach_trial = trial_df[5 : 10+ reach_len[trial_idx]]\n",
    "        av_reach_speed.append(np.mean(reach_trial[5:]))\n",
    "        reach_trial_smooth_x = savgol_filter(reach_trial_coord.iloc[:, 0], 5,2)\n",
    "        time = np.linspace(-0.1,1,len(reach_trial))\n",
    "\n",
    "    av_trajectory[f'{ctx}_x_mean'] = np.mean(binned_trajectory_x, axis = 0)\n",
    "    av_trajectory[f'{ctx}_y_mean'] = np.mean(binned_trajectory_y, axis = 0)\n",
    "    av_trajectory[f'{ctx}_y_sem'] = np.std(binned_trajectory_y, axis = 0)/np.sqrt(len(inc_trials))\n",
    "    av_trajectory[f'{ctx}_x_sem'] = np.std(binned_trajectory_x, axis = 0)/np.sqrt(len(inc_trials))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    time = np.linspace(0,1,len(av_trajectory[f'{ctx}_x_mean']))\n",
    "    plt.plot(av_trajectory[f'{ctx}_x_mean'],av_trajectory[f'{ctx}_y_mean'], color = 'w', alpha = 0.6)\n",
    "    plt.scatter(av_trajectory[f'{ctx}_x_mean'],av_trajectory[f'{ctx}_y_mean'], c= time, cmap=\"plasma\", s = 150)\n",
    "    plt.errorbar(av_trajectory[f'{ctx}_x_mean'], av_trajectory[f'{ctx}_y_mean'], xerr= av_trajectory[f'{ctx}_x_sem'], yerr = av_trajectory[f'{ctx}_y_sem'], fmt = 'none', ecolor = 'red', alpha = 0.4, elinewidth = 2, capsize = 3)\n",
    "    plt.axis('equal')\n",
    "\n",
    "    reach_speed.loc[ctx,'Average'] = np.mean(av_reach_speed)\n",
    "    reach_speed.loc[ctx,'SEM'] = np.std(av_reach_speed)/np.sqrt(len(av_reach_speed))\n",
    "    av_reach_speeds[ctx] = av_reach_speed\n",
    "    max_extension_df.loc[ctx, 'max'] = np.max(max_extension)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b361fc03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# plot reaching trajectories in MAC silencing condition\n",
    "\n",
    "CTXs = ['MAC']\n",
    "exp_dir = datapath\n",
    "mouse = 1 ## mouse number\n",
    "bh = 'Reaching'\n",
    "body_parts = ['L_Hand']\n",
    "pixel_length = pixel_size  ####################################################\n",
    "mean_bin = 0.1 # in normalised time,i.e, 0.1 = reach divided in 10 bins\n",
    "bins = np.arange(0,1+mean_bin,mean_bin)\n",
    "av_trajectory = {} \n",
    "\n",
    "slit_df =  pd.read_excel(os.path.join(exp_dir , f'coord_0{mouse}.xlsx'))\n",
    "slit_df = slit_df[slit_df.iloc[:,0] == 'slit']\n",
    "\n",
    "reach_speed = pd.DataFrame(columns = ['Average', 'SEM'])\n",
    "av_reach_speeds = {}\n",
    "annot = {}\n",
    "reach = {}\n",
    "eat = {}\n",
    "eat_fl = {}\n",
    "eat_mouth = {}\n",
    "retract = {}\n",
    "max_extension_df = pd.DataFrame(columns = ['max'])\n",
    "for ctx in CTXs:\n",
    "    dir = f'{datapath}/{ctx}/n{mouse}/' ## folder where all data is \n",
    "    slit = slit_df.loc[slit_df.loc[:,'ctx'] == ctx,['x','y']]\n",
    "    dlc_tracking = pd.read_csv(os.path.join(dir, f'attempt_reach_{ctx}_muscimolDLC_resnet50_POF_bottomDec13shuffle1_550000.csv'), delimiter=',', na_values='nan', skiprows=(0,2))\n",
    "  \n",
    "    annot[ctx] = pd.read_csv(Path(dir, 'Annot.csv'))\n",
    "    annot[ctx].fillna(0, inplace = True)\n",
    "    behavior_groups = annot[ctx].groupby('Behavior')\n",
    "    if any(annot[ctx].Behavior == 'Reach'):\n",
    "        reach[ctx] = behavior_groups.get_group('Reach')\n",
    "        retract[ctx] = behavior_groups.get_group('Retract')\n",
    "    else:\n",
    "        reach[ctx] = pd.DataFrame()\n",
    "        retract[ctx] = pd.DataFrame()\n",
    "  \n",
    "    eat[ctx]  = behavior_groups.get_group('Eat')\n",
    "    eat_fl[ctx]  = eat[ctx][eat[ctx].Handle_FL.isin([1])]\n",
    "    eat_mouth[ctx] = eat[ctx][eat[ctx].Handle_FL.isin([0])]\n",
    "    if ctx == 'MAC':\n",
    "        engage = behavior_groups.get_group('Engage')\n",
    "    reach = engage.reset_index()\n",
    "    ts_start = reach.Start.values\n",
    "    ts_stop = reach.Stop.values\n",
    "    trial_len = ts_stop-ts_start\n",
    "    reach_len = trial_len\n",
    "  \n",
    "    exc_reaches = np.where(reach_len < 11)[0]\n",
    "    coords, inc_trials= extract_body_parts(dlc_tracking, body_parts, thresh = 0.2, exclude_thresh=5, trial_len = trial_len)\n",
    "    inc_trials = np.asarray(inc_trials)\n",
    "    print(f'number of nans in total {np.sum(np.isnan(coords.values))}')\n",
    "  \n",
    "    speed = coords.diff()\n",
    "    speed_val = pd.DataFrame()\n",
    "    for bp in body_parts:\n",
    "        speed_val[bp] = np.hypot(speed[bp], speed[f'{bp}.1'])\n",
    "\n",
    "    coords_dict = {}\n",
    "    for part in body_parts:\n",
    "        colie = [col for col in coords.columns if part in col]\n",
    "        coords_dict[part] = coords[colie]\n",
    "  \n",
    "    trial_dict = {}\n",
    "    trials_speed = {}\n",
    "    sum_len = 0\n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        if trial_idx in exc_reaches:\n",
    "            sum_len += trial_len[trial_idx]\n",
    "            continue\n",
    "        else:\n",
    "            for part in body_parts:\n",
    "                trial_dict[f'{part}_{trial_idx}'] = coords_dict[part].iloc[sum_len: sum_len + trial_len[trial_idx]]\n",
    "                trials_speed[f'{part}_{trial_idx}'] = ndimage.median_filter(speed_val[bp][sum_len: sum_len + trial_len[trial_idx]], size=4)  * pixel_length * 100\n",
    "\n",
    "        sum_len += trial_len[trial_idx]\n",
    "    \n",
    "    for exc in exc_reaches:\n",
    "        inc_trials = inc_trials[inc_trials != exc]\n",
    "    print(f'{len(exc_reaches)} number of trials below minimum duration excluded')\n",
    "  \n",
    "    part = body_parts[0]\n",
    "    sum_len = 0\n",
    "    max_extension = []\n",
    "    plt.figure(figsize = (15,15), facecolor='w', edgecolor='k', frameon='FalseSC')\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    binned_trajectory_x = np.zeros((len(inc_trials), len(bins)-1))\n",
    "    binned_trajectory_y = np.zeros((len(inc_trials), len(bins)-1))\n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        \n",
    "        key = f'{part}_{trial_idx}'\n",
    "        trial_df = trial_dict[key]\n",
    "    \n",
    "    \n",
    "        reach_trial = trial_df.iloc[0: np.min([25, reach_len[trial_idx]])]\n",
    "        reach_trial_smooth_x = (savgol_filter(reach_trial.iloc[:, 0], 5,2) - slit.iloc[0,0])*pixel_length\n",
    "        reach_trial_smooth_y = (savgol_filter(reach_trial.iloc[:, 1], 5,2) - slit.iloc[0,1])*pixel_length\n",
    "        max_extension.append(np.max(reach_trial_smooth_x))\n",
    "        time = np.linspace(0,1,len(reach_trial))\n",
    "        \n",
    "        for ii in range(len(bins)-1):\n",
    "            time_pts = (time >= bins[ii]) & (time <= bins[ii+1])\n",
    "            binned_trajectory_x[idx, ii] = np.mean(reach_trial_smooth_x[time_pts])\n",
    "            binned_trajectory_y[idx, ii] = np.mean(reach_trial_smooth_y[time_pts])\n",
    "\n",
    "        plt.plot(reach_trial_smooth_x,reach_trial_smooth_y, color = 'w', alpha = 0.1)\n",
    "        plt.scatter(reach_trial_smooth_x,reach_trial_smooth_y, c= time, cmap=\"plasma\", alpha = 0.2, s= 25)\n",
    "    \n",
    "        plt.title(f'{pop}_{ctx}_{bh}')\n",
    "    plt.axis('equal')\n",
    "\n",
    "    sum_len = 0\n",
    "    av_reach_speed = []\n",
    "    ax = plt.gca()\n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        key = f'{part}_{trial_idx}'\n",
    "        trial_df = trials_speed[key]\n",
    "        trial_coord = trial_dict[key]\n",
    "        reach_trial_coord = trial_coord.iloc[5: 10+ reach_len[trial_idx]]\n",
    "        \n",
    "        reach_trial = trial_df[5 : 10+ reach_len[trial_idx]]\n",
    "        av_reach_speed.append(np.mean(reach_trial[5:]))\n",
    "        reach_trial_smooth_x = savgol_filter(reach_trial_coord.iloc[:, 0], 5,2)\n",
    "        time = np.linspace(-0.1,1,len(reach_trial))\n",
    "\n",
    "    av_trajectory[f'{ctx}_x_mean'] = np.mean(binned_trajectory_x, axis = 0)\n",
    "    av_trajectory[f'{ctx}_y_mean'] = np.mean(binned_trajectory_y, axis = 0)\n",
    "    av_trajectory[f'{ctx}_y_sem'] = np.std(binned_trajectory_y, axis = 0)/np.sqrt(len(inc_trials))\n",
    "    av_trajectory[f'{ctx}_x_sem'] = np.std(binned_trajectory_x, axis = 0)/np.sqrt(len(inc_trials))\n",
    "    ax = plt.gca()\n",
    "    ax.set_facecolor('black')\n",
    "    time = np.linspace(0,1,len(av_trajectory[f'{ctx}_x_mean']))\n",
    "    plt.plot(av_trajectory[f'{ctx}_x_mean'],av_trajectory[f'{ctx}_y_mean'], color = 'w', alpha = 0.6)\n",
    "    plt.scatter(av_trajectory[f'{ctx}_x_mean'],av_trajectory[f'{ctx}_y_mean'], c= time, cmap=\"plasma\", s = 150)\n",
    "    plt.errorbar(av_trajectory[f'{ctx}_x_mean'], av_trajectory[f'{ctx}_y_mean'], xerr= av_trajectory[f'{ctx}_x_sem'], yerr = av_trajectory[f'{ctx}_y_sem'], fmt = 'none', ecolor = 'red', alpha = 0.4, elinewidth = 2, capsize = 3)\n",
    "    plt.axis('equal')\n",
    "    plt.xlim((-1.5,1))\n",
    "    reach_speed.loc[ctx,'Average'] = np.mean(av_reach_speed)\n",
    "    reach_speed.loc[ctx,'SEM'] = np.std(av_reach_speed)/np.sqrt(len(av_reach_speed))\n",
    "    av_reach_speeds[ctx] = av_reach_speed\n",
    "    max_extension_df.loc[ctx, 'max'] = np.max(max_extension)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af135a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot handling trajectories in PBS, MAC silencing and LAC silencing conditions\n",
    "\n",
    "CTXs = ['PBS', 'MAC', 'LAC']\n",
    "pop = 'Muscimol' #ReaChR Gtacr\n",
    "mouse = 3 ## mouse number\n",
    "bh = 'Handling'\n",
    "body_parts = ['L_Hand', 'R_Hand', 'Snout', 'Tailbase']\n",
    "pixel_length = pixel_size   ####################################################\n",
    "summary = pd.DataFrame(columns = ['Average', 'SEM'])\n",
    "d_summary = pd.DataFrame(columns = ['d_max', 'd_max_err', 'd_av', 'd_av_err'])\n",
    "av_handle_speeds = {}\n",
    "d_max_trials = {}\n",
    "av_traj_std = {}\n",
    "annot = {}\n",
    "reach = {}\n",
    "eat = {}\n",
    "eat_fl = {}\n",
    "eat_mouth = {}\n",
    "retract = {}\n",
    "std = {}\n",
    "max_extension_df = pd.DataFrame(columns = ['max'])\n",
    "for ctx in CTXs:\n",
    "    dir = f'{datapath}/{ctx}/n{mouse}/' ## folder where all data is\n",
    "\n",
    "    dlc_tracking = pd.read_csv(os.path.join(dir, f'{bh}_{ctx}_m{pop[1:]}DLC_resnet50_POF_bottomDec13shuffle1_550000.csv'), delimiter=',', na_values='nan', skiprows=(0,2))\n",
    "\n",
    "    annot[ctx] = pd.read_csv(Path(dir, 'Annot.csv'))\n",
    "    annot[ctx].fillna(0, inplace = True)\n",
    "    behavior_groups = annot[ctx].groupby('Behavior')\n",
    "\n",
    "\n",
    "    eat[ctx]  = behavior_groups.get_group('Eat')\n",
    "    eat_fl[ctx]  = eat[ctx][eat[ctx].Handle_FL.isin([1])]\n",
    "    eat_mouth[ctx] = eat[ctx][eat[ctx].Handle_FL.isin([0])]\n",
    "\n",
    "    ts_start = eat_fl[ctx].Start.values\n",
    "    ts_stop = eat_fl[ctx].Stop.values\n",
    "    handle_len = ts_stop-ts_start\n",
    "    trial_len = ts_stop-ts_start\n",
    "    trial_len += 200\n",
    "\n",
    "\n",
    "    coords, inc_trials= extract_body_parts(dlc_tracking, body_parts, thresh = 0.4, exclude_thresh=5, trial_len = trial_len)\n",
    "    print(f'number of nans in total {np.sum(np.isnan(coords.values))}')\n",
    "\n",
    "    coords_dict = {}\n",
    "    for part in body_parts:\n",
    "        colie = [col for col in coords.columns if part in col]\n",
    "        coords_dict[part] = coords[colie]\n",
    "\n",
    "    trial_dict = {}\n",
    "    trials_speed = {}\n",
    "    trial_FL_dist = {}\n",
    "    sum_len = 0\n",
    "    \n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        for part in body_parts:\n",
    "            trial_dict[f'{part}_{idx}'] = coords_dict[part].iloc[sum_len: sum_len + trial_len[trial_idx]]\n",
    "            speed = trial_dict[f'{part}_{idx}'].diff()\n",
    "            speed = speed.fillna(0)\n",
    "    \n",
    "            speed_val = np.hypot(speed[part], speed[f'{part}.1'])\n",
    "            \n",
    "            trials_speed[f'{part}_{idx}'] = savgol_filter(speed_val, 5,3)  * pixel_length * 100\n",
    "  \n",
    "        trial_FL_dist[idx] = distance_comp(trial_dict[f'R_Hand_{idx}'], trial_dict[f'L_Hand_{idx}'])  \n",
    "        sum_len += trial_len[trial_idx]\n",
    "\n",
    "    sum_len = 0\n",
    "    max_extension = []\n",
    "    plt.figure(figsize = (15,15), facecolor='w', edgecolor='k', frameon='FalseSC')\n",
    "    ax = plt.gca()\n",
    "    markerlist = ['o', 'o', '+', 'p']\n",
    "    ax.set_facecolor('black')\n",
    "    body_parts_2plt = ['L_Hand', 'R_Hand']\n",
    "    std_L = []\n",
    "    std_R = []\n",
    "    \n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        endpoint = (trial_dict[f'Tailbase_{idx}'].iloc[handle_len[trial_idx],0], trial_dict[f'Tailbase_{idx}'].iloc[handle_len[trial_idx],1])\n",
    "        FL_mid_x = (trial_dict[f'L_Hand_{idx}'].iloc[5,0] + trial_dict[f'R_Hand_{idx}'].iloc[5,0] )/2\n",
    "        FL_mid_y = (trial_dict[f'L_Hand_{idx}'].iloc[5,1] + trial_dict[f'R_Hand_{idx}'].iloc[5,1] )/2\n",
    "        origin = (FL_mid_x, FL_mid_y)\n",
    "        vector= endpoint[0]-origin[0] , endpoint[1]-origin[1]\n",
    "        angle = math.atan2(vector[0],vector[1])\n",
    "  \n",
    "        key_L = f'L_Hand_{idx}'\n",
    "        key_R = f'R_Hand_{idx}'\n",
    "        trial_df_L = trial_dict[key_L]\n",
    "        handle_coord_L = trial_df_L[5 : np.min([50, handle_len[trial_idx]-5])]\n",
    "        handle_trial_smooth_x_L = savgol_filter(handle_coord_L.iloc[:, 0], 3,2)\n",
    "        handle_trial_smooth_y_L = savgol_filter(handle_coord_L.iloc[:, 1], 3,2)\n",
    "        trial_df_R = trial_dict[key_R]\n",
    "        handle_coord_R = trial_df_R[5 : np.min([50, handle_len[trial_idx]-5])]\n",
    "        handle_trial_smooth_x_R = savgol_filter(handle_coord_R.iloc[:, 0], 3,2)\n",
    "        handle_trial_smooth_y_R = savgol_filter(handle_coord_R.iloc[:, 1], 3,2)\n",
    "  \n",
    "        for ii in range(len(handle_trial_smooth_x_L)):\n",
    "            point = (handle_trial_smooth_x_L[ii], handle_trial_smooth_y_L[ii])\n",
    "            handle_trial_smooth_x_L[ii], handle_trial_smooth_y_L[ii]  = rotate(origin, point, angle)\n",
    "            point = (handle_trial_smooth_x_R[ii], handle_trial_smooth_y_R[ii])\n",
    "            handle_trial_smooth_x_R[ii], handle_trial_smooth_y_R[ii]  = rotate(origin, point, angle)\n",
    "            mid_x = (handle_trial_smooth_x_L[ii] + handle_trial_smooth_x_R[ii])/2\n",
    "            mid_y = (handle_trial_smooth_y_L[ii] + handle_trial_smooth_y_R[ii])/2\n",
    "            handle_trial_smooth_y_R[ii] = handle_trial_smooth_y_R[ii] - mid_y\n",
    "            handle_trial_smooth_y_L[ii] = handle_trial_smooth_y_L[ii] - mid_y\n",
    "            handle_trial_smooth_x_R[ii] = handle_trial_smooth_x_R[ii] - mid_x\n",
    "            handle_trial_smooth_x_L[ii] = handle_trial_smooth_x_L[ii] - mid_x\n",
    "  \n",
    "  \n",
    "        time = np.linspace(0,1,len(handle_coord_L))\n",
    "  \n",
    "        std_L.append(np.mean([np.std(handle_trial_smooth_x_L),np.std(handle_trial_smooth_y_L)]))\n",
    "        std_R.append(np.mean([np.std(handle_trial_smooth_x_R),np.std(handle_trial_smooth_y_R)]))\n",
    "        \n",
    "        plt.scatter(handle_trial_smooth_x_R*pixel_length,handle_trial_smooth_y_R*pixel_length, c= time, cmap=\"plasma\", marker = markerlist[0])\n",
    "        plt.scatter(handle_trial_smooth_x_L*pixel_length,handle_trial_smooth_y_L*pixel_length, c= time, cmap=\"plasma\", marker = markerlist[1])\n",
    "        plt.plot(handle_trial_smooth_x_L*pixel_length,handle_trial_smooth_y_L*pixel_length, color = 'gray', alpha = 0.3)\n",
    "        plt.plot(handle_trial_smooth_x_R*pixel_length,handle_trial_smooth_y_R*pixel_length, color = 'gray', alpha = 0.3)\n",
    "        plt.title(f'{pop}_{ctx}_{bh}')\n",
    "\n",
    "    std[f'{ctx}_std_L'] = std_L\n",
    "    std[f'{ctx}_std_R'] = std_R\n",
    "\n",
    "    for part_idx, part in enumerate(body_parts):\n",
    "        std_trial = [] \n",
    "        av_trial_speed = []\n",
    "        d_max  = []\n",
    "        d_mean = []\n",
    "        for idx, trial_idx in enumerate(inc_trials):\n",
    "            key = f'{part}_{idx}'\n",
    "            trial_df = trials_speed[key]\n",
    "            trial_coord = trial_dict[key]\n",
    "            handle_coord = trial_coord[0 : handle_len[trial_idx]]\n",
    "            handle_trial = trial_df[0 : handle_len[trial_idx]]\n",
    "    \n",
    "            av_trial_speed.append(np.mean(handle_trial[5:35]))\n",
    "    \n",
    "            time = np.linspace(0,1,len(handle_trial))\n",
    "  \n",
    "        summary.loc[f'{ctx}_{part}','Average'] = np.mean(av_trial_speed)\n",
    "        summary.loc[f'{ctx}_{part}','SEM'] = np.std(av_trial_speed)/np.sqrt(len(av_trial_speed))\n",
    "        av_handle_speeds[f'{ctx}_{part}'] = av_trial_speed\n",
    "    \n",
    "    for idx, trial_idx in enumerate(inc_trials):\n",
    "        d_trial = trial_FL_dist[idx]\n",
    "        d_handle = d_trial[0 : handle_len[trial_idx]]\n",
    "        d_max.append(np.max(d_handle))\n",
    "        d_mean.append(np.mean(d_handle))\n",
    "  \n",
    "        time = np.linspace(0,1,len(d_handle[5:]))\n",
    "  \n",
    "        d_summary.loc[f'{ctx}','d_max'] = np.mean(d_max)\n",
    "        d_summary.loc[f'{ctx}','d_max_err'] = np.std(d_max)/np.sqrt(len(av_trial_speed))\n",
    "        d_summary.loc[f'{ctx}','d_av'] = np.mean(d_mean)\n",
    "        d_summary.loc[f'{ctx}','d_av_err'] = np.std(d_mean)/np.sqrt(len(av_trial_speed))\n",
    "  \n",
    "        d_max_trials[f'{ctx}_{part}'] = d_max"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcd7a94",
   "metadata": {},
   "source": [
    "## quantification and statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c5db1fb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# read the tables including all the parameters for each animal\n",
    "CTXs = ['PBS', 'LAC', 'MAC']\n",
    "full_summary = pd.DataFrame()\n",
    "for ii in range(1,10):\n",
    "    animal_table = get_summary_df(ii)\n",
    "    \n",
    "    max_table = pd.read_csv(datapath+f'\\max_extension_n{ii}.csv', index_col=0)\n",
    "    max_table.iloc[:,0] = (max_table.iloc[:,0]-894)*pixel_size\n",
    "    max_table.iloc[list(max_table.iloc[:,0] <0), 0] = 0\n",
    "    animal_table = animal_table.join(max_table)\n",
    "    \n",
    "    sd_table = pd.read_csv(datapath+f'\\sd_FL_handle_n{ii}.csv', index_col=0)\n",
    "    sd_table_t = sd_table.T.rename(columns={'s.d':'sd'})\n",
    "    \n",
    "    sd_table_t_norm = sd_table_t.div(sd_table_t.max(axis = 0), axis = 1)\n",
    "    animal_table = animal_table.join(sd_table_t_norm)\n",
    "    \n",
    "    animal_table['animal'] = ii\n",
    "    full_summary = pd.concat([full_summary, animal_table]).reset_index(drop=True)\n",
    "group_sum = full_summary.groupby('ctx')\n",
    "\n",
    "reach_speed_table = pd.DataFrame()\n",
    "for ii in range(1,10):\n",
    "    reach_speed = pd.read_csv(datapath+f'\\\\reach_speed_n{ii}.csv', index_col=0)\n",
    "    reach_speed = pd.read_csv(datapath+f'\\\\reach_speed_n{ii}.csv', index_col=0)\n",
    "    reach_speed['animal'] = ii\n",
    "    reach_speed.insert(0, 'ctx', reach_speed.index)\n",
    "    reach_speed_table = pd.concat([reach_speed_table, reach_speed]).reset_index(drop=True)\n",
    "reach_speed_group_sum = reach_speed_table.groupby('ctx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4b98bf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the features\n",
    "fig, ax = plt.subplots(7, figsize = (10,40))\n",
    "colors = ['darkorange', 'darkcyan', 'darkmagenta']\n",
    "\n",
    "for idx,ctx in enumerate(CTXs):\n",
    "    table = group_sum.get_group(ctx)\n",
    "    n = len(table)\n",
    "    mean = table.iloc[:,1:7].mean()\n",
    "    sem = table.iloc[:,1:7].std()/np.sqrt(n)\n",
    "    for ii in range(6):\n",
    "        points = table.loc[:,mean.index[ii]]\n",
    "        points_no_nan = points.dropna()\n",
    "        points = points.fillna(0)\n",
    "        ax[ii].bar(ctx,mean.iloc[ii], alpha = 0.5, yerr = sem.iloc[ii], color = colors[idx])\n",
    "        ax[ii].set_title(mean.index[ii].replace('_', ' '))\n",
    "        ax[ii].scatter([ctx]*n, points, c = colors[idx])\n",
    "\n",
    "for idx,ctx in enumerate(['PBS', 'LAC']):\n",
    "    reach_speed_table_sub = reach_speed_group_sum.get_group(ctx)\n",
    "    reach_speed_table_sub.iloc[:,1] = reach_speed_table_sub.iloc[:,1]*pixel_size\n",
    "    n = len(reach_speed_table_sub)\n",
    "    reach_speed_mean = reach_speed_table_sub.iloc[:,1].mean()\n",
    "    reach_speed_sem = reach_speed_table_sub.iloc[:,1].std()/np.sqrt(n)\n",
    "    reach_speed_points = reach_speed_table_sub['Average']\n",
    "    \n",
    "    ax[6].bar(ctx, reach_speed_mean, yerr=reach_speed_sem, color=colors[idx], alpha=0.5)\n",
    "    ax[6].set_title(\"reach speed\".replace('_', ' '))\n",
    "    ax[6].scatter([ctx]*n, reach_speed_points, c = colors[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57d1ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# statistics\n",
    "print('n_reaches_per_min')\n",
    "print(pg.rm_anova(dv='n_reaches_per_min', within='ctx', subject='animal', data=full_summary, detailed=True))\n",
    "print(pg.pairwise_tests(dv='n_reaches_per_min', within='ctx', subject='animal', padjust='fdr_bh', data=full_summary))\n",
    "print('max')\n",
    "print(pg.rm_anova(dv='max', within='ctx', subject='animal', data=full_summary, detailed=True))\n",
    "print(pg.pairwise_tests(dv='max', within='ctx', subject='animal', padjust='fdr_bh', data=full_summary))\n",
    "print('reach_speed')\n",
    "print(pg.ttest(reach_speed_group_sum.get_group('LAC')['Average'], reach_speed_group_sum.get_group('PBS')['Average'], paired=True))\n",
    "print('prop_mouth_eat')\n",
    "print(pg.rm_anova(dv='prop_mouth_eat', within='ctx', subject='animal', data=full_summary, detailed=True))\n",
    "print(pg.pairwise_tests(dv='prop_mouth_eat', within='ctx', subject='animal', padjust='fdr_bh', data=full_summary))\n",
    "print('prop_drop_handle')\n",
    "print(pg.rm_anova(dv='prop_drop_handle', within='ctx', subject='animal', data=full_summary, detailed=True))\n",
    "print(pg.pairwise_tests(dv='prop_drop_handle', within='ctx', subject='animal', padjust='fdr_bh', data=full_summary))\n",
    "print('regrip')\n",
    "print(pg.rm_anova(dv='regrip', within='ctx', subject='animal', data=full_summary, detailed=True))\n",
    "print(pg.pairwise_tests(dv='regrip', within='ctx', subject='animal', padjust='fdr_bh', data=full_summary))\n",
    "print('sd')\n",
    "print(pg.rm_anova(dv='sd', within='ctx', subject='animal', data=full_summary, detailed=True))\n",
    "print(pg.pairwise_tests(dv='sd', within='ctx', subject='animal', padjust='fdr_bh', alternative='less', data=full_summary))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04b8e97",
   "metadata": {},
   "source": [
    "# Figure 6 and S5 -- electrophysiological recording"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3288c0f7",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eccf7c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import os\n",
    "import math\n",
    "from scipy import ndimage, signal, stats\n",
    "from scipy.stats import mannwhitneyu\n",
    "from scipy.stats import wilcoxon\n",
    "import random\n",
    "from pathlib import Path\n",
    "import glob\n",
    "from scipy.signal import savgol_filter\n",
    "from matplotlib.colors import LinearSegmentedColormap\n",
    "from matplotlib import rc\n",
    "rc(\"pdf\", fonttype=42)\n",
    "rc('figure', max_open_warning = 0)\n",
    "\n",
    "class Spike_Read():\n",
    "    def __init__(self, datapath, datatype):\n",
    "        self.datapath = Path(datapath)\n",
    "        self.datatype = datatype\n",
    "    \n",
    "    def read_spike(self, channel):\n",
    "        #return 0 a very long dataframe of cluster_id and timepoint\n",
    "        #1 a dataframe of cluster_id and KSLabel (good or mua)\n",
    "        #2 a list of channels to consider in NPX, defined by channel argument\n",
    "        data_folder = self.datapath\n",
    "        spike_clusters = np.load(data_folder / 'spike_clusters.npy')\n",
    "        spike_clusters = np.hstack(spike_clusters)\n",
    "        spikes_info = pd.read_csv(data_folder / 'cluster_info.tsv', sep='\\t')\n",
    "        \n",
    "        if 'NPX' in self.datatype:\n",
    "            spike_times = np.load(data_folder / 'spike_times_sec_adj.npy')\n",
    "            channel_filepath = os.path.join(data_folder, f'*{channel}_channels.csv')\n",
    "            channel_file = glob.glob(channel_filepath)[0]\n",
    "            ch = pd.read_csv(channel_file, usecols = [0])\n",
    "            ch_id = [int(x.split('_')[1]) for x in ch.values.squeeze()]\n",
    "\n",
    "        else:\n",
    "            spike_times = np.load(data_folder / 'spike_times.npy')\n",
    "            spike_times_clusters = pd.DataFrame(spike_times, spike_clusters)\n",
    "            ch_id = None\n",
    "            \n",
    "        spike_times_clusters = pd.DataFrame(spike_times, spike_clusters)\n",
    "\n",
    "        return spike_times_clusters, spikes_info, ch_id\n",
    "\n",
    "    def extract_spike(self, include_mua, exp, wodis, channel):\n",
    "        #extract sorted spikes from kilosort\n",
    "        #a dictionary data structure will be returned\n",
    "        #keys indicate good or mua\n",
    "        #valuses are a list of spike times of the given unit\n",
    "        #if not need to tell good or mua, set wodis to True and use unit for all\n",
    "        spike_times_clusters, spikes_info, ch_id = self.read_spike(channel)\n",
    "        \n",
    "        if ch_id:\n",
    "            spike_info = spikes_info[spikes_info.ch.isin(ch_id)]\n",
    "        else:\n",
    "            spike_info = spikes_info\n",
    "            \n",
    "        spikes_id = spike_info.loc[spike_info['group'] == 'good'].fillna({'KSLabel':'mua'})\n",
    "        good_and_mua = {'good':[], 'mua':[]}\n",
    "        spike_dict = {}\n",
    "        if wodis is True:\n",
    "            prekey = 'unit'\n",
    "        else:\n",
    "            prekey = 'good'\n",
    "\n",
    "        if 'id' in spikes_id.columns:\n",
    "            for idx in set(spikes_id.id):\n",
    "                label = spikes_id[spikes_id.id==idx].KSLabel.values[0]\n",
    "                good_and_mua[label].append(idx)\n",
    "        elif 'cluster_id' in spikes_id.columns:\n",
    "            for idx in set(spikes_id.cluster_id):\n",
    "                label = spikes_id[spikes_id.cluster_id==idx].KSLabel.values[0]\n",
    "                good_and_mua[label].append(idx)\n",
    "            \n",
    "        for unit in good_and_mua['good']:\n",
    "            spikes = spike_times_clusters.loc[unit]\n",
    "            key = f'{prekey}_{unit}'\n",
    "            spike_dict[key] = spikes\n",
    "\n",
    "        if include_mua:\n",
    "            if wodis is not True:\n",
    "                prekey = 'mua'\n",
    "            for unit in good_and_mua['mua']:\n",
    "                spikes = spike_times_clusters.loc[unit]\n",
    "                key = f'{prekey}_{unit}'\n",
    "                spike_dict[key] = spikes\n",
    "                \n",
    "        if exp is True:\n",
    "            if spikes is not None:\n",
    "                spikes.to_csv(self.datapath/f'{key}.csv')\n",
    "        \n",
    "        return spike_dict, spike_times_clusters, spikes_info, ch_id\n",
    "\n",
    "def read_bh_data(datatype, spike_data_path, stim_single=None, stim_train=None, laser_files=None, \n",
    "                 bh_time_dataframe=None, bh_data_plexon=None, bh_data_csv=None, bh_data_intan=None, bh_data_NPX=None):\n",
    "\n",
    "    if ('plexon' in datatype) and (bh_data_plexon is not None):\n",
    "        bh_path = spike_data_path+'/'+ bh_data_plexon\n",
    "        bh_time_table = pd.read_csv(bh_path, names=['Behavior', '1','2','3','Start','Stop','4','5'])\n",
    "\n",
    "    elif datatype == 'laser_intan':\n",
    "        if stim_single or stim_train:\n",
    "            laser_intan = Laser_Intan(spike_data_path)\n",
    "            bh_time_table = laser_intan.get_stim_table(stim_single, stim_train)\n",
    "        else:\n",
    "            print(\"The datatype is laser_intan, please specify stimulation patterns\")\n",
    "            \n",
    "    elif datatype == 'laser_NPX':\n",
    "        if stim_single or stim_train:\n",
    "            laser_NPX = Laser_NPX(spike_data_path)\n",
    "            bh_time_table = laser_NPX.get_stim_table(laser_files=laser_files, single=stim_single, train=stim_train)\n",
    "        else:\n",
    "            print(\"The datatype is laser_NPX, please specify stimulation patterns and laser_files\")\n",
    "\n",
    "    elif ('dataframe' in datatype) and (bh_time_dataframe is not None):\n",
    "            bh_time_table = bh_time_dataframe\n",
    "\n",
    "    elif datatype == 'csv':\n",
    "        if bh_data_csv:\n",
    "            bh_time_table = pd.read_csv(spike_data_path / bh_data_csv, sep='\\t')\n",
    "        else:\n",
    "            print(\"The datatype is csv, bh_data_csv should have at least three columns naming \\\n",
    "Behavior, Start, Stop\")\n",
    "    \n",
    "    elif ('intan' in datatype) and (bh_data_intan is not None):\n",
    "        bh_path = os.path.join(spike_data_path, bh_data_intan)\n",
    "\n",
    "        if bh_data_intan.endswith('csv'):\n",
    "            excel = pd.read_csv(bh_path, usecols=['Start', 'Stop', 'Behavior', 'Success'])\n",
    "        else:\n",
    "            excel = pd.read_excel(bh_path, usecols=['Start', 'Stop', 'Behavior', 'Success'])\n",
    "\n",
    "        excel = excel.dropna()\n",
    "        camera = np.squeeze(np.load(os.path.join(spike_data_path, 'camera.npy')))\n",
    "        time = np.squeeze(np.load(os.path.join(spike_data_path, 'time.npy')))\n",
    "        camera_exp_start = np.diff(camera, prepend=0) > 0\n",
    "        camera_timestamps = time[camera_exp_start]\n",
    "\n",
    "        start = []\n",
    "        for i in excel['Start']:\n",
    "            i=int(i)\n",
    "            start.append(camera_timestamps[i])\n",
    "        stop = []\n",
    "        for i in excel['Stop']:\n",
    "            i=int(i)\n",
    "            stop.append(camera_timestamps[i])\n",
    "\n",
    "        bh_time_table = pd.DataFrame()\n",
    "        bh_time_table['Start'] = start\n",
    "        bh_time_table['Stop'] = stop\n",
    "        bh_time_table['Behavior'] = excel['Behavior']\n",
    "        bh_time_table['Success'] = excel['Success']\n",
    "            \n",
    "    elif ('NPX' in datatype) and (bh_data_NPX is not None):\n",
    "        bh_path = os.path.join(spike_data_path, bh_data_NPX)\n",
    "\n",
    "        if bh_data_NPX.endswith('csv'):\n",
    "            excel = pd.read_csv(bh_path, usecols=['Start', 'Stop', 'Behavior', 'Success'])\n",
    "        else:\n",
    "            excel = pd.read_excel(bh_path, usecols=['Start', 'Stop', 'Behavior', 'Success'])\n",
    "\n",
    "        excel = excel.dropna()\n",
    "        camera_timestamps = pd.read_csv(glob.glob(str(spike_data_path) + r'/*camera_timestamps.csv')[0], header=None).values.squeeze()\n",
    "\n",
    "        start = []\n",
    "        for i in excel['Start']:\n",
    "            i=int(i)\n",
    "            start.append(camera_timestamps[i])\n",
    "        stop = []\n",
    "        for i in excel['Stop']:\n",
    "            i=int(i)\n",
    "            stop.append(camera_timestamps[i])\n",
    "\n",
    "        bh_time_table = pd.DataFrame()\n",
    "        bh_time_table['Start'] = start\n",
    "        bh_time_table['Stop'] = stop\n",
    "        bh_time_table['Behavior'] = excel['Behavior']\n",
    "        bh_time_table['Success'] = excel['Success']\n",
    "        \n",
    "    return bh_time_table\n",
    "\n",
    "def get_animal_path(batch, probe, opsin, n, session_date, bh_format, path):\n",
    "    #path is r'W:\\scratch\\garber\\yangwuzh\\001_ChR_probe'\n",
    "    \n",
    "    dict_animal = {\n",
    "        'batch': batch, \n",
    "        'probe': probe,\n",
    "        'opsin': opsin,\n",
    "        'n': n,\n",
    "        'session_date': session_date,\n",
    "        'bh_format': bh_format,\n",
    "    }\n",
    "\n",
    "    df_animal = pd.DataFrame(data=dict_animal)\n",
    "    df_animal['CTX'] = ['CTX' for i in range(len(df_animal['n']))]\n",
    "    df_animal['bh_table'] = ['bh' for i in range(len(df_animal['n']))]\n",
    "    df_animal['data_path'] = ['data_path' for i in range(len(df_animal['n']))]\n",
    "    df_animal['MI_file'] = ['MI_file' for i in range(len(df_animal['n']))]\n",
    "    df_animal['Reach_file'] = ['Reach_file' for i in range(len(df_animal['n']))]\n",
    "    df_animal['Handle_file'] = ['Handle_file' for i in range(len(df_animal['n']))]\n",
    "    df_animal['Retract_file'] = ['Retract_file' for i in range(len(df_animal['n']))]\n",
    "\n",
    "    for idx, n in enumerate(df_animal['n']):\n",
    "        if int(n/10):\n",
    "            if n%10:\n",
    "                df_animal['CTX'].iloc[idx] = 'Ins'\n",
    "            else:\n",
    "                df_animal['CTX'].iloc[idx] = 'LAC'\n",
    "        else:\n",
    "            df_animal['CTX'].iloc[idx] = 'MAC'\n",
    "\n",
    "        batch = df_animal['batch'].iloc[idx]\n",
    "        CTX = df_animal['CTX'].iloc[idx]\n",
    "        n = df_animal['n'].iloc[idx]\n",
    "        session_date = df_animal['session_date'].iloc[idx]\n",
    "        bh_format = df_animal['bh_format'].iloc[idx]\n",
    "        probe = df_animal['probe'].iloc[idx]\n",
    "        opsin = df_animal['opsin'].iloc[idx]\n",
    "\n",
    "        df_animal['bh_table'].iloc[idx] = f'Annot_{batch}_{CTX}{n}_{session_date}.{bh_format}'\n",
    "        df_animal['MI_file'].iloc[idx] = f'MI_{batch}_{session_date}_{n}.csv'\n",
    "        df_animal['Reach_file'].iloc[idx] = f'Reach_table_{batch}_{session_date}_{n}.csv'\n",
    "        df_animal['Handle_file'].iloc[idx] = f'Handle_table_{batch}_{session_date}_{n}.csv'\n",
    "        df_animal['Retract_file'].iloc[idx] = f'Retract_table_{batch}_{session_date}_{n}.csv'\n",
    "\n",
    "        if probe == 'probe':\n",
    "            regex_path = os.path.join(path, f'{batch}_{opsin}_{probe}', f'{batch}_{opsin}_{CTX}_{probe}_latRM_{n}_{session_date}*')\n",
    "\n",
    "        elif probe == 'NPX':\n",
    "            regex_path = os.path.join(path, f'{batch}_{opsin}_{probe}', f'{batch}_{opsin}_{CTX}_{probe}_latRM_{n}_{session_date}*')\n",
    "            \n",
    "        print(regex_path)\n",
    "\n",
    "        df_animal['data_path'].iloc[idx] = glob.glob(regex_path)[0]\n",
    "        \n",
    "    return df_animal\n",
    "\n",
    "class Behavior_Timestamp():\n",
    "  #this class takes a DataFrame with at least three columns naming Behavior, Start, Stop\n",
    "    def __init__(self, bh_data):\n",
    "        self.bh_data = bh_data\n",
    "    \n",
    "    def get_bh_time(self, behavior, time_range):\n",
    "        #get specific behavior timestamp in a time_range\n",
    "        #time_range has to be a list of start and end timepoint, in second\n",
    "        #otherwise all timepoints will be taken\n",
    "        bh_dict = {}\n",
    "        \n",
    "        if (behavior.endswith('_stop')) or (behavior.endswith('_Stop')):\n",
    "            bh_time = self.bh_data[(self.bh_data.Behavior==f'{behavior}'[0:-5])]['Stop'].values\n",
    "        else:\n",
    "            bh_time = self.bh_data[(self.bh_data.Behavior==f'{behavior}')]['Start'].values\n",
    "        if len(bh_time) == 0:\n",
    "            print(f'{behavior} not found')\n",
    "            \n",
    "        if type(time_range) == list:\n",
    "            bh_dict[behavior] = [[x] for x in bh_time if time_range[0]<=x<=time_range[1]]\n",
    "        else:\n",
    "            bh_dict[behavior] = [[x] for x in bh_time]\n",
    "            \n",
    "        return bh_dict\n",
    "    \n",
    "    def get_more_bh_times(self, bh_win, time_range, *behavior):\n",
    "        #input behaviors as wish and output a dictionary of values of a list of lists,\n",
    "        #where each sub-list is a set of timepoint (tp)s corresponding to the dictionary key.\n",
    "        #the first tp in the sub-list is the the first behavior[0] as the base\n",
    "        #following behavior tps in the sub-list are all in bh_win wrt behavior0\n",
    "        more_bh_time = self.get_bh_time(behavior[0], time_range)\n",
    "        \n",
    "        for k in range(len(behavior)-1):\n",
    "            bh_time_0_key = list(more_bh_time)[k]\n",
    "            bh_time_0 = more_bh_time[bh_time_0_key]\n",
    "            \n",
    "            add_bh_dict = self.get_bh_time(behavior[k+1], time_range)\n",
    "            bh_time_add_key = behavior[k+1]\n",
    "            bh_time_add = [item[0] for item in add_bh_dict[bh_time_add_key]]\n",
    "            more_bh_time[f'{bh_time_0_key}_{bh_time_add_key}']=[]\n",
    "            for i in range(len(bh_time_0)):\n",
    "                for j in range(len(bh_time_add)):\n",
    "                    if np.absolute(bh_time_add[j]-bh_time_0[i][0]) < bh_win:\n",
    "                        bh_time_lst = list(bh_time_0[i])\n",
    "                        bh_time_lst.append(bh_time_add[j])                        \n",
    "                        more_bh_time[f'{bh_time_0_key}_{bh_time_add_key}'].append(bh_time_lst)\n",
    "        return more_bh_time        \n",
    "    \n",
    "    def get_lst_bh_times(self, bh_win, only_bh, time_range, *behavior):\n",
    "        #get the timepoints of the corresponding behaviors in bh_win behavior window.\n",
    "        #the number of behavior and the value of only_bh are very critical.\n",
    "        #if only_bh is False, this funciton is the same as get_more_bh_times.\n",
    "        #if only_bh is 1, every behavior after behavior0 should not be in the bh_win\n",
    "        #if only_bh is 2, only the first two bhs are in the bh_win while all the rest not.\n",
    "        #if only_bh is 3, all of the three bhs are in the bh_win.\n",
    "        #currently max is 3, if need more, add more conditons\n",
    "        mul_bh_times = self.get_more_bh_times(bh_win, time_range, *behavior)\n",
    "        bh_num = len(behavior)\n",
    "        \n",
    "        if bh_num >=4:\n",
    "            print('the number of considered behaviors is 3 max, add more conditions if needed')\n",
    "        \n",
    "        if only_bh == 1:\n",
    "            key0 = behavior[0]\n",
    "            bh_time_0 = mul_bh_times[key0]\n",
    "            if bh_num == 1:\n",
    "                return {key0: bh_time_0}\n",
    "            elif bh_num >= 2:\n",
    "                key01 = list(mul_bh_times)[1]                \n",
    "                bh_time_01 = mul_bh_times[key01]\n",
    "                bh_time_01_sub = [[item[0]] for item in bh_time_01]\n",
    "                only_bh_time_0_wo1 = [item for item in bh_time_0 if item not in bh_time_01_sub]\n",
    "                key1 = key01[len(key0):]\n",
    "                key0_wo1 = f'{key0}_wo{key1}'\n",
    "                bh_time_0_wo1 = {key0_wo1: only_bh_time_0_wo1}\n",
    "                if bh_num >=3:\n",
    "                    bh_time_02_dict = self.get_more_bh_times(bh_win, time_range, behavior[0], behavior[2])\n",
    "                    key02 = list(bh_time_02_dict)[1]\n",
    "                    bh_time_02 = bh_time_02_dict[key02]\n",
    "                    bh_time_02_sub = [[item[0]] for item in bh_time_02]\n",
    "                    \n",
    "                    only_bh_time_0_wo12 = [item for item in only_bh_time_0_wo1 if item not in bh_time_02_sub]\n",
    "                    key2 = key02[len(key0):]\n",
    "                    key0_wo12 = f'{key0_wo1}{key2}'\n",
    "                    bh_time_0_wo12 = {key0_wo12: only_bh_time_0_wo12}\n",
    "                    return bh_time_0_wo12\n",
    "                return bh_time_0_wo1\n",
    "                    \n",
    "        if only_bh == 2:\n",
    "            if bh_num == 1:\n",
    "                print('1 behavior input but only_bh is 2, please put more than one behavior')\n",
    "                return {behavior[0]: mul_bh_times[behavior[0]]}\n",
    "            elif bh_num == 2:\n",
    "                return {f'{behavior[0]}_{behavior[1]}': mul_bh_times[f'{behavior[0]}_{behavior[1]}']}\n",
    "            elif bh_num >=3:\n",
    "                key01 = list(mul_bh_times)[1]\n",
    "                key012 = list(mul_bh_times)[2]\n",
    "                bh_time_01 = mul_bh_times[key01]\n",
    "                bh_time_012 = mul_bh_times[key012]\n",
    "                bh_time_012_sub = [item[0:2] for item in bh_time_012]\n",
    "                only_bh_time_01_wo2 = [item for item in bh_time_01 if item not in bh_time_012_sub]\n",
    "                key2 = key012[len(key01):]\n",
    "                key01_wo2 = f'{key01}_wo{key2}'\n",
    "                return {key01_wo2: only_bh_time_01_wo2}\n",
    "            \n",
    "        if only_bh == 3:\n",
    "            if bh_num == 1:\n",
    "                print('1 behavior input but only_bh is 3, please put more than two behaviors')\n",
    "                return {behavior[0]: mul_bh_times[behavior[0]]}\n",
    "            elif bh_num == 2:\n",
    "                print('2 behaviors input but only_bh is 3, please put more than two behaviors')\n",
    "                return {f'{behavior[0]}_{behavior[1]}': mul_bh_times[f'{behavior[0]}_{behavior[1]}']}\n",
    "            elif bh_num == 3:\n",
    "                return {f'{behavior[0]}_{behavior[1]}_{behavior[2]}': mul_bh_times[f'{behavior[0]}_{behavior[1]}_{behavior[2]}']}\n",
    "            \n",
    "        return mul_bh_times\n",
    "    \n",
    "    def diff_bh_times(self, bh_win, only_bh, time_range, *behavior):\n",
    "        #align behavior1 to behavior0 and get the time interval\n",
    "        diff_bh = self.get_lst_bh_times(bh_win, only_bh, time_range, *behavior)\n",
    "        \n",
    "        diff_bh_key = list(diff_bh)[0]\n",
    "        diff_bh_time = diff_bh[diff_bh_key]\n",
    "        if only_bh:\n",
    "            bh_lst = [behavior[i] for i in range(np.minimum(only_bh, len(behavior)))]\n",
    "        else:\n",
    "            bh_lst = [behavior[0]]\n",
    "        diff_bh_time_df = pd.DataFrame(diff_bh_time, columns=bh_lst)\n",
    "        \n",
    "        diff_bh_lst = [f'diff_{behavior[0]}_{bh_lst[i]}' for i in range(len(bh_lst))]\n",
    "        for i in range(len(diff_bh_lst)):\n",
    "            diff_bh_time_df[diff_bh_lst[i]] = diff_bh_time_df[f'{bh_lst[i]}'] - diff_bh_time_df[f'{behavior[0]}']\n",
    "        \n",
    "        diff_pd = pd.DataFrame([np.zeros(len(diff_bh_lst)), np.zeros(len(diff_bh_lst))], \n",
    "         columns = diff_bh_lst)\n",
    "        diff_pd.iloc[0] = diff_bh_time_df[diff_bh_lst].mean(axis=0)\n",
    "        diff_pd.iloc[1] = diff_bh_time_df[diff_bh_lst].sem(axis=0)\n",
    "        return diff_pd, diff_bh_time_df\n",
    "        \n",
    "class Spike_Behavior():\n",
    "    def __init__(self, spike_data, bh_data):\n",
    "        #spike_data is a dictionary of the sorted cluster, where the value is a list of spike times\n",
    "        #bh_data is a dictionary of the specific behavior, where the value is a list of lists of timepoints\n",
    "        self.spike_data = spike_data\n",
    "        self.bh_data = bh_data\n",
    "    \n",
    "    #def get_time_frame(self, timepoint, time_before, time_after):\n",
    "    #    time_frame = [x*self.sampling_rate for x in [timepoint-time_before, timepoint+time_after]]\n",
    "    #    return time_frame        \n",
    "    \n",
    "    def get_spikes_in_tw_wrt_bh(self, time_window):\n",
    "        #time_window should be a list [time_before,time_after] (both are in second and positive).\n",
    "        sp = self.spike_data\n",
    "        bh = self.bh_data\n",
    "        \n",
    "        spike_data_key = list(sp)[0]\n",
    "        spike_data = sp[spike_data_key]\n",
    "        \n",
    "        if bh is None:\n",
    "            bh_data_key = 'No_behavior'\n",
    "            bh_data_values = [0]\n",
    "            print('behavior not specified')\n",
    "        else:    \n",
    "            bh_data_key = list(bh)[0]\n",
    "            bh_data_values = [bh[0] for bh in bh[bh_data_key]]\n",
    "            \n",
    "        sp_bh_dict = {}\n",
    "        for i in range(len(bh_data_values)):\n",
    "            time_start = bh_data_values[i] - time_window[0]\n",
    "            time_stop = bh_data_values[i] + time_window[1]\n",
    "            sp_wrt_bh = [tp for tp in spike_data if time_start < tp < time_stop]\n",
    "            key_id = f'{spike_data_key}_{bh_data_key}_{i}'\n",
    "            sp_bh_dict[key_id] = [x-bh_data_values[i] for x in sp_wrt_bh]\n",
    "        return sp_bh_dict\n",
    "    \n",
    "    def bin_spike(self, time_window, downsample_rate):\n",
    "        #downsample_rate is number per second\n",
    "        sp_bh_dict = self.get_spikes_in_tw_wrt_bh(time_window)\n",
    "        bin_len = 1/downsample_rate\n",
    "        time_pt = np.arange(-time_window[0] , time_window[1]+bin_len, bin_len)\n",
    "        binned_sp_df = pd.DataFrame({'timepoint':time_pt[:-1]})\n",
    "        for key, value in sp_bh_dict.items():                \n",
    "            binned_sp_df[key] = np.histogram(value, bins=time_pt)[0]\n",
    "        return binned_sp_df\n",
    "    \n",
    "    def rolling_spike(self, time_window, downsample_rate, rolling_window):\n",
    "        binned_sp_df = self.bin_spike(time_window, downsample_rate)\n",
    "        rolling_sp_df = binned_sp_df.rolling(rolling_window, center=True).sum()\n",
    "        return rolling_sp_df\n",
    "    \n",
    "    def freq_spike(self, time_window, downsample_rate, rolling_window, rolling):\n",
    "        if rolling:\n",
    "            df = self.rolling_spike(time_window, downsample_rate, rolling_window)\n",
    "            lst_column = list(df.columns)\n",
    "            lst_column.remove('timepoint')\n",
    "            df['timepoint'] /= rolling_window\n",
    "            df[lst_column] *= downsample_rate/rolling_window\n",
    "        else:\n",
    "            df = self.bin_spike(time_window, downsample_rate)\n",
    "            lst_column = list(df.columns)\n",
    "            lst_column.remove('timepoint')\n",
    "            df[lst_column] *= downsample_rate\n",
    "        return df\n",
    "\n",
    "class Laser_NPX():\n",
    "    def __init__(self, datapath):\n",
    "        self.datapath = datapath\n",
    "        \n",
    "    def read_laser(self, laser_files, single, train):\n",
    "        #single should be a list of the length of single pulse stimulation\n",
    "        #train should be a list of the length of a train of stimulation\n",
    "        laser_dict = {}\n",
    "        if single:\n",
    "            for item in single:\n",
    "                if item.endswith('ms'):\n",
    "                    key = f'Laser_{float(item[:-2])/1000}'\n",
    "                    filename = f'{laser_files}_{item}.csv'\n",
    "                    file_path = os.path.join(self.datapath, filename)\n",
    "                    laser_data = pd.read_csv(file_path)\n",
    "                    laser_dict[key] = laser_data\n",
    "            \n",
    "        if train:\n",
    "            for item in train:\n",
    "                if item.endswith('ms'):\n",
    "                    key = f'Laser_Hz_{float(item[:-2])/1000}'\n",
    "                    filename = f'{laser_files}_{item}.csv'\n",
    "                    file_path = os.path.join(self.datapath, filename)\n",
    "                    laser_data = pd.read_csv(file_path)\n",
    "                    laser_dict[key] = laser_data\n",
    "            \n",
    "        return laser_dict\n",
    "    \n",
    "    def get_laser_diff(self, laser_files, single, train):\n",
    "        #laser_ts is the timepoint when the laser is on\n",
    "        #laser_diff is the time difference between the timepoints\n",
    "        #laser_inter is the \n",
    "        laser_dict = self.read_laser(laser_files, single, train)\n",
    "        laser_ts = {}\n",
    "        laser_diff = {}\n",
    "        laser_inter = {}\n",
    "        \n",
    "        for key in laser_dict.keys():\n",
    "            pulse = float(key.split('_')[-1])\n",
    "            col = str(round(pulse*1000))\n",
    "            laser_on_ts = np.array(laser_dict[key][col])\n",
    "            laser_on_ts_diff = np.round(np.diff(laser_on_ts), 3)\n",
    "            laser_on_diff = np.unique(laser_on_ts_diff, return_counts=True)\n",
    "            \n",
    "            laser_ts[key] = laser_on_ts\n",
    "            laser_diff[key] = laser_on_ts_diff\n",
    "            laser_inter[key] = laser_on_diff\n",
    "                \n",
    "        return laser_ts, laser_diff, laser_inter\n",
    "    \n",
    "    def get_stim_table(self, laser_files, single=None, train=None):\n",
    "        laser_ts, laser_diff, laser_inter = self.get_laser_diff(laser_files, single, train)\n",
    "        stim_table = pd.DataFrame(columns=['Behavior', 'Start', 'Stop'])\n",
    "        stim_single_table = pd.DataFrame(columns=['Behavior', 'Start', 'Stop'])\n",
    "        stim_train_table = pd.DataFrame(columns=['Behavior', 'Start', 'Stop'])\n",
    "\n",
    "        for key in laser_ts.keys():\n",
    "            pulse = float(key.split('_')[-1])\n",
    "            laser_on_ts = laser_ts[key]\n",
    "            laser_on_ts_diff = laser_diff[key]\n",
    "            laser_on_diff = laser_inter[key]\n",
    "            \n",
    "            if \"Hz\" not in key:\n",
    "                laser_interval = [laser_on_diff[0][ii] for ii in range(len(laser_on_diff[1])) if laser_on_diff[1][ii]>1]\n",
    "                bh_lst = []\n",
    "                start_lst = []\n",
    "                stop_lst = []\n",
    "                for ii, ts in enumerate(laser_on_ts_diff):\n",
    "                    if ts in laser_interval:\n",
    "                        bh_lst.append(f'Laser_{round(ts)}_{pulse}')\n",
    "                        start_lst.append(laser_on_ts[ii])\n",
    "                        stop_lst.append(laser_on_ts[ii]+(pulse))\n",
    "                \n",
    "                stim_single_table = pd.DataFrame({\n",
    "                    'Behavior': bh_lst,\n",
    "                    'Start': start_lst,\n",
    "                    'Stop' : stop_lst\n",
    "                })\n",
    "                \n",
    "            if \"Hz\" in key:\n",
    "                if np.min(laser_on_diff[0]) == laser_on_diff[0][np.argmax(laser_on_diff[1])]:\n",
    "                    period = np.min(laser_on_diff[0])\n",
    "                else:\n",
    "                    print('Please double check the laser pattern, period is not the most frequent')\n",
    "                    period = None\n",
    "                freq = round(1/period)\n",
    "                key_ele = key.split('_')\n",
    "                pattern = f'{key_ele[0]}_{freq}{key_ele[1]}'\n",
    "                \n",
    "                start_idx = np.where(laser_on_ts_diff != period)\n",
    "                stim_start = np.concatenate([laser_on_ts[0], laser_on_ts[np.array(start_idx)+1]], axis=None)\n",
    "                stim_stop = np.concatenate([laser_on_ts[start_idx], laser_on_ts[-1]+period], axis=None)\n",
    "                \n",
    "                stim_train_table = pd.DataFrame({\n",
    "                    'Behavior': [f'{pattern}_{round(stim_stop[ii]-ss, 1)}' for ii, ss in enumerate(stim_start)],\n",
    "                    'Start': stim_start,\n",
    "                    'Stop' : stim_stop\n",
    "                })\n",
    "                \n",
    "        stim_table = pd.concat([stim_single_table, stim_train_table])\n",
    "        \n",
    "        return stim_table\n",
    "    \n",
    "class Laser_Intan():\n",
    "    def __init__(self, datapath, laser_ch=1):\n",
    "        self.analog = np.load(os.path.join(datapath, 'analog.npy'))\n",
    "        self.time = np.squeeze(np.load(os.path.join(datapath, 'time.npy')))\n",
    "        self.laser = self.analog[laser_ch,:]\n",
    "  \n",
    "    def laser_filter(self, exp=None):\n",
    "        laser = self.laser\n",
    "        time = self.time\n",
    "        laser_filt  = ndimage.median_filter(laser, 5)\n",
    "        if exp:\n",
    "            plt.figure(figsize = (50,15))\n",
    "            plt.plot(time, laser_filt)\n",
    "        return laser_filt\n",
    "  \n",
    "    def get_laser_on_off(self):\n",
    "        laser_filt = self.laser_filter()\n",
    "        time = self.time\n",
    "        laser_on = laser_filt > .2\n",
    "        laser_on_diff = np.diff(laser_on, prepend=0) > 0\n",
    "        laser_off_diff = np.diff(laser_on, prepend=0) < 0\n",
    "        laser_on_ts = time[laser_on_diff]\n",
    "        laser_off_ts = time[laser_off_diff]\n",
    "        return laser_on_ts, laser_off_ts\n",
    "  \n",
    "    def get_laser_pulse_width(self):\n",
    "        laser_on_ts, laser_off_ts = self.get_laser_on_off()\n",
    "        return np.round(laser_off_ts - laser_on_ts, 3)\n",
    "  \n",
    "    def get_laser_diff(self):\n",
    "        laser_on_ts, laser_off_ts = self.get_laser_on_off()\n",
    "        time = self.time\n",
    "        laser_on_ts_diff = np.round(np.diff(laser_on_ts, append = time[-1]), 3)\n",
    "        laser_off_ts_diff = np.round(np.diff(laser_off_ts, prepend = time[-1]), 3)\n",
    "        return laser_on_ts_diff, laser_off_ts_diff\n",
    "  \n",
    "    def get_stim_on_off(self, periods, stim_pattern=None):\n",
    "        #group the laser timestamps according to stimulation period\n",
    "        #to know all of the stimulation patterns, set stim_pattern as True\n",
    "        laser_on_ts, laser_off_ts = self.get_laser_on_off()\n",
    "        time = self.time\n",
    "        laser_on_ts_diff = np.round(np.diff(laser_on_ts, append = time[-1]), 3)\n",
    "        pulse_width = np.round(laser_off_ts - laser_on_ts, 3)\n",
    "    \n",
    "        stim_on = {}\n",
    "        stim_off = {}\n",
    "        stim_pw = {}\n",
    "        stim_len = {}\n",
    "        stim_concat = []\n",
    "    \n",
    "        for period in periods:\n",
    "            stim_mask = np.zeros(len(laser_on_ts))\n",
    "            stim_mask[laser_on_ts_diff == period] = 1\n",
    "            stim_on_mask = np.diff(stim_mask, prepend = 0) > 0\n",
    "            stim_off_mask = np.diff(stim_mask, prepend = 0) < 0\n",
    "            if np.sum(stim_on_mask) == 0:\n",
    "                print(f'No stimulation trials of period {period} found, check histogram below of different stimulation periods possible')\n",
    "                plt.hist(laser_on_ts_diff, 10, range = (0,1))\n",
    "            else:\n",
    "                stim_on[period] = laser_on_ts[stim_on_mask]\n",
    "                stim_off[period] = laser_off_ts[stim_off_mask]\n",
    "                stim_pw[period] = pulse_width[stim_on_mask]\n",
    "                stim_len[period] = np.round((stim_off[period] - stim_on[period] - stim_pw[period] + period), 2)\n",
    "                stim_concat = np.concatenate((stim_concat, \\\n",
    "                laser_on_ts[stim_mask.astype(bool)], \\\n",
    "                laser_on_ts[stim_off_mask.astype(bool)]), axis = 0)\n",
    "    \n",
    "            if stim_pattern:\n",
    "                print(f'{len(stim_on[period])} stimulation trials with period {period}s found, pulse width = {np.unique(stim_pw[period])}, each stimulation length = {np.unique(stim_len[period])}')\n",
    "              \n",
    "        stim_on['other'] = np.setdiff1d(laser_on_ts, stim_concat)\n",
    "        if len(stim_on['other']) > 0:\n",
    "            if stim_pattern:\n",
    "                stim_other = np.round(np.diff(stim_on['other']), 3)\n",
    "                stim_other_period = np.unique(stim_other, return_counts=True)\n",
    "                stim_other_events = stim_other_period[1][stim_other_period[1]>1]\n",
    "                stim_pw['other'] = []\n",
    "                for other_stim in stim_on['other']:\n",
    "                    stim_pw['other'] = np.concatenate((stim_pw['other'], \\\n",
    "                    pulse_width[np.where(laser_on_ts == other_stim)]))\n",
    "                print(\"!some other stimulations found!\")\n",
    "                print(f\"{len(stim_on['other'])} laser-on events with other periods, pulse width = {np.unique(stim_pw['other'])}\")\n",
    "                print(f'putative periods could be {stim_other_period[0]} with {stim_other_events} events')\n",
    "        \n",
    "        return stim_on, stim_off, stim_pw, stim_len, laser_on_ts, laser_off_ts\n",
    "      \n",
    "    def table_stim_single(self, periods):\n",
    "        #this method finds the on-off pair in each period and return a table of start and end of each pulse\n",
    "        stim_on, stim_off, stim_pw, stim_len, laser_on_ts, laser_off_ts = self.get_stim_on_off(periods)\n",
    "        \n",
    "        stim_table = pd.DataFrame(columns=['Behavior', 'Start', 'Stop'])\n",
    "        for period in periods:\n",
    "            for i in range(len(stim_on[period])):\n",
    "                num_stim = int(np.round(stim_len[period][i] / period))\n",
    "                pw = stim_pw[period][i]\n",
    "                tp_on = stim_on[period][i]\n",
    "                tp_idx = np.where(laser_on_ts == tp_on)[0][0]\n",
    "                tp_end = tp_idx+num_stim\n",
    "                stim_tp_on = laser_on_ts[tp_idx : tp_end]\n",
    "                stim_tp_off = laser_off_ts[tp_idx : tp_end]\n",
    "                stim_table_period = pd.DataFrame({\"Behavior\": [f\"Laser_{period}_{pw}\" for i in range(num_stim)], \n",
    "                \"Start\": stim_tp_on, \"Stop\":stim_tp_off})\n",
    "                stim_table = pd.concat([stim_table, stim_table_period])\n",
    "        stim_table = stim_table.reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "        return stim_table\n",
    "  \n",
    "    def table_stim_train(self, periods):\n",
    "        #this method convert a stimulation with multiple on-off pairs into a table\n",
    "        #consisting of the start and end of the stimulation\n",
    "        stim_on, stim_off, stim_pw, stim_len, laser_on_ts, laser_off_ts = self.get_stim_on_off(periods)\n",
    "    \n",
    "        stim_table = pd.DataFrame(columns=['Behavior', 'Start', 'Stop'])\n",
    "        for period in periods:\n",
    "            num_stim = len(stim_on[period])\n",
    "            freq = 1 / period\n",
    "            tp_on = stim_on[period]\n",
    "            tp_off = stim_off[period]\n",
    "            stimu_len = np.round(tp_off - tp_on, 1)\n",
    "            stim_table_period = pd.DataFrame({\n",
    "             \"Behavior\": [f\"Laser_{freq}Hz_{stimu_len[i]}\" for i in range(num_stim)], \n",
    "             \"Start\": tp_on, \"Stop\":tp_off\n",
    "            })\n",
    "            stim_table = pd.concat([stim_table, stim_table_period])\n",
    "        stim_table = stim_table.reset_index().drop(['index'], axis=1)\n",
    "    \n",
    "        return stim_table\n",
    "\n",
    "    def get_stim_table(self, stim_single=None, stim_train=None):\n",
    "        #input should be a list of periods\n",
    "        #stim_single is for each on-off pair\n",
    "        #stim_train is for multiple on-off pairs in a train\n",
    "        #in the output table, the Behavior column shows the stimulation patterns\n",
    "        #for stim_single, the Behavior shows the period and the stimulation pulse width, \n",
    "        #the Stop is the timestamp when a pulse is off\n",
    "        #for stim_train, the Beahavior shows the frequency of the stimulation and the pulse width of each\n",
    "        #laser on event, the Stop is the timestamp when the stimulation train stops (the last pulse off)\n",
    "        stim_table = pd.DataFrame(columns=['Behavior', 'Start', 'Stop'])\n",
    "        if stim_single:\n",
    "            stim_single_table = self.table_stim_single(stim_single)\n",
    "            stim_table = pd.concat([stim_single_table, stim_table]).reset_index().drop(['index'], axis=1)\n",
    "        if stim_train:\n",
    "            stim_train_table = self.table_stim_train(stim_train)\n",
    "            stim_table = pd.concat([stim_table, stim_train_table]).reset_index().drop(['index'], axis=1)\n",
    "        return stim_table\n",
    "\n",
    "class Plot_Sp_Bh():\n",
    "    def __init__(self, datatype, spike_data_path, sampling_rate=None, stim_single=None, stim_train=None, laser_files=None, \n",
    "                 bh_time_dataframe=None, bh_data_plexon=None, bh_data_csv=None, bh_data_intan=None, \n",
    "                 bh_data_NPX=None, include_mua=True, exp=None, wodis=None, channel='latrm'):\n",
    "        #sampling_rate plexon 40000, intan 30000\n",
    "        self.spike_data = Spike_Read(spike_data_path, datatype)\n",
    "        self.get_spike, self.get_all_spike, self.get_spike_info, self.get_ch_info = self.spike_data.extract_spike(include_mua=include_mua, exp=exp, wodis=wodis, channel=channel)\n",
    "\n",
    "        self.bh_data = read_bh_data(datatype, spike_data_path, stim_single=stim_single, \n",
    "                                    stim_train=stim_train, laser_files=laser_files, \n",
    "                                    bh_time_dataframe=bh_time_dataframe, \n",
    "                                    bh_data_plexon=bh_data_plexon, bh_data_csv=bh_data_csv, \n",
    "                                    bh_data_intan=bh_data_intan, bh_data_NPX=bh_data_NPX)\n",
    "        \n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.datatype = datatype\n",
    "        self.spike_data_path = spike_data_path\n",
    "        self.wodis = wodis\n",
    "        self.channel = channel\n",
    "        \n",
    "        if 'intan' in self.datatype:\n",
    "            self.time = np.squeeze(np.load(os.path.join(self.spike_data_path, 'time.npy')))\n",
    "        elif 'NPX' in self.datatype:\n",
    "            maxtime = self.get_all_spike.iloc[-1,0]\n",
    "            self.time = np.arange(0, maxtime, 1/30000)\n",
    "    \n",
    "    def get_spike_intan(self, cluster_id):\n",
    "        sp_frame = self.get_spike\n",
    "        sp_tp = []\n",
    "        sp_frame_lst = np.hstack(sp_frame[cluster_id].values)\n",
    "        for frame in sp_frame_lst:\n",
    "            sp_tp.append(self.time[frame])\n",
    "        return sp_tp\n",
    "    \n",
    "    def get_sp_id(self, cluster_id):\n",
    "        if 'intan' in self.datatype:\n",
    "            sp = self.get_spike_intan(cluster_id)\n",
    "            sp_id = {cluster_id: np.array(sp)}\n",
    "        elif 'NPX' in self.datatype:\n",
    "            sp = self.get_spike\n",
    "            sp_id = {cluster_id: np.hstack(sp[cluster_id].values)}\n",
    "        else:\n",
    "            sp = self.get_spike\n",
    "            sp_id = {cluster_id: np.hstack(sp[cluster_id].values)/self.sampling_rate}\n",
    "        return sp_id\n",
    "\n",
    "    def link_sp_bh(self, cluster_id, bh_win, only_bh, time_range, success, *behavior):\n",
    "        sp_id = self.get_sp_id(cluster_id)\n",
    "        \n",
    "        if behavior is None:\n",
    "            bh = None\n",
    "        else:\n",
    "            bh = self.get_bh_data(bh_win, only_bh, time_range, success, *behavior)\n",
    "            \n",
    "        sp_bh = Spike_Behavior(sp_id, bh)\n",
    "        return sp_bh\n",
    "    \n",
    "    def wrap_bh_data(self, success):\n",
    "        bh_data = self.bh_data\n",
    "        if success==1:\n",
    "            return Behavior_Timestamp(bh_data.loc[(bh_data.Success==1)])\n",
    "        elif success==0:\n",
    "            return Behavior_Timestamp(bh_data.loc[(bh_data.Success==0)])\n",
    "        else:\n",
    "            return Behavior_Timestamp(bh_data)\n",
    "    \n",
    "    def get_bh_data(self, bh_win, only_bh, time_range, success, *behavior):\n",
    "        return self.wrap_bh_data(success).get_lst_bh_times(bh_win, only_bh, time_range, *behavior)\n",
    "    \n",
    "    def get_bh_diff(self, bh_win, only_bh, time_range, success, *behavior):\n",
    "        return self.wrap_bh_data(success).diff_bh_times(bh_win, only_bh, time_range, *behavior)\n",
    "    \n",
    "    def get_bh(self, success):\n",
    "        return self.wrap_bh_data(success).Behavior.unique()\n",
    "    \n",
    "    def get_sp_in_bh_tw(self, cluster_id, time_window, behavior, success, time_range='All', bh_win=1.5, \n",
    "    only_bh=None):\n",
    "        sp_bh = self.link_sp_bh(cluster_id, bh_win, only_bh, time_range, success, *behavior)\n",
    "        return sp_bh.get_spikes_in_bh_tw(time_window)\n",
    "        \n",
    "    def get_freq_in_bh_tw(self, cluster_id, time_window, behavior, success, time_range='All', \n",
    "                          downsample_rate=20, bh_win=1.5, only_bh=None, rolling=True, rolling_window=5):\n",
    "        sp_bh = self.link_sp_bh(cluster_id, bh_win, only_bh, time_range, success, *behavior)\n",
    "        return sp_bh.get_freq_in_bh_tw(time_window, downsample_rate, rolling_window, rolling)\n",
    "    \n",
    "    def group_cluster_freq_to_bh(self, time_window, behavior, cluster_type, time_range, exclude_note, \n",
    "                            delimiter, downsample_rate, bh_win, only_bh, rolling, rolling_window, success):\n",
    "        if cluster_type == 'good':\n",
    "            sp = self.get_spike(include_mua=False, exclude_note=exclude_note, delimiter=delimiter)\n",
    "        else:\n",
    "            sp = self.get_spike()\n",
    "        bh = self.get_bh_data(bh_win, only_bh, time_range, success, *behavior)\n",
    "        bh_data_key = list(bh)[0]\n",
    "        bh_data_values = [bh[0] for bh in bh[bh_data_key]]\n",
    "        sp_bh_freq = pd.DataFrame(bh_data_values, columns=[bh_data_key])\n",
    "\n",
    "        for cluster_id in sp:\n",
    "            sp_id = {cluster_id: np.hstack(sp[cluster_id].values)}\n",
    "            sp_bh = Spike_Behavior(sp_id, bh, self.sampling_rate)\n",
    "            sp_freq = sp_bh.get_freq_in_bh_tw(time_window, downsample_rate, rolling_window, rolling)[0]\n",
    "            sp_bh_name = sp_freq.columns[1]\n",
    "            sp_bh_series = sp_freq[sp_bh_name]\n",
    "            sp_bh_sum = sum(np.sum(sp_bh_series))\n",
    "            if sp_bh_sum < 20*sp_bh_series.count():\n",
    "                print(f'{sp_bh_name} dropped')\n",
    "            else:\n",
    "                sp_bh_freq[sp_bh_name] = sp_bh_series\n",
    "\n",
    "        return sp_bh_freq\n",
    "    \n",
    "    def scatter_plot(self, cluster_id, time_window, behavior, success=None, time_range='All', only_bh=None, \n",
    "     bh_line=True, fig_exp=None, bh_win=0.9, plot=True, raw_exp=False, exp_drop_col=None, marker='|', msize=1, \n",
    "     figsize=(10,10), bh_line_error=None, bh_line_color='g', min_sp=0, plot_all=True, n_trial=None, trial_lst=None):\n",
    "        #cluster_id should be good_, mua_ or unit_\n",
    "        #KSlabels can be found with get_spike_info attribute\n",
    "        #the raw result from this method is a dictionary, keys are behaivor trials of the unit, values are timepoint in time_window\n",
    "        sp_bh = self.link_sp_bh(cluster_id, bh_win, only_bh, time_range, success, *behavior)\n",
    "        sp_bh_dict = sp_bh.get_spikes_in_tw_wrt_bh(time_window)\n",
    "\n",
    "        lst_drop = []\n",
    "        if min_sp>0:\n",
    "            for key in sp_bh_dict.keys():\n",
    "                if len(sp_bh_dict[key]) < min_sp:\n",
    "                    lst_drop.append(key)\n",
    "\n",
    "        if plot_all is False:\n",
    "            for item in lst_drop:\n",
    "                sp_bh_dict.pop(item)\n",
    "\n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "            sp_bh_keys = list(sp_bh_dict)\n",
    "            y = np.linspace(0.2, 0.8, len(sp_bh_keys)+1)\n",
    "            ybin = 0.6/len(sp_bh_keys)*0.9\n",
    "            count = 0\n",
    "            for idx, key in enumerate(sp_bh_keys):\n",
    "                x = sp_bh_dict[key]\n",
    "                if n_trial:\n",
    "                    if idx<n_trial:\n",
    "                        for ii in x:\n",
    "                            plt.axvline(ii, ymin = y[idx], ymax = y[idx]+ybin)\n",
    "                    else:\n",
    "                        break\n",
    "                elif trial_lst:\n",
    "                    y = np.linspace(0.2, 0.8, len(trial_lst)+1)\n",
    "                    ybin = 0.6/len(trial_lst)*0.9\n",
    "                    if idx in trial_lst:\n",
    "                        for ii in x:\n",
    "                            plt.axvline(ii, ymin = y[count], ymax = y[count]+ybin)\n",
    "                        count +=1\n",
    "                        \n",
    "                else:      \n",
    "                    for ii in x:\n",
    "                        plt.axvline(ii, ymin = y[idx], ymax = y[idx]+ybin)\n",
    "                #plt.scatter(x, y, s=msize, c='k', marker=marker)                    \n",
    "\n",
    "            if bh_line:\n",
    "                bh_line_df = self.get_bh_diff(bh_win, only_bh, time_range, success, *behavior)[0]\n",
    "                i = len(sp_bh_keys)\n",
    "                ax.vlines(bh_line_df.iloc[0], 0, i, color=bh_line_color)\n",
    "                if bh_line_error:\n",
    "                    for diff in bh_line_df.columns:\n",
    "                        xerr_min = bh_line_df[diff][0] - bh_line_df[diff][1]\n",
    "                        xerr_max = bh_line_df[diff][0] + bh_line_df[diff][1]\n",
    "                        ax.axvspan(xerr_min, xerr_max, alpha=0.2, color=bh_line_color)\n",
    "            if fig_exp:\n",
    "                animal = self.spike_data_path.split(\"\\\\\")[-1]\n",
    "                batch = animal.split('_')[0]\n",
    "                n = animal.split('_')[-3]\n",
    "                plt.savefig(f'{self.spike_data_path}\\\\scatter_{batch}_{n}_{cluster_id}_{behavior[0]}_{idx}_trials.pdf')\n",
    "        \n",
    "        if exp_drop_col:\n",
    "            print(f'Dropped: {lst_drop}')\n",
    "            return lst_drop    \n",
    "        if raw_exp:\n",
    "            return sp_bh_dict\n",
    "            \n",
    "    def freq_line_plot(self, cluster_id, time_window, behavior, success=None, time_range='All', only_bh=None, \n",
    "     downsample_rate=20, rolling_window=5, rolling=False, bh_line=True, bh_win=0.9, raw_exp=None, \n",
    "     plot=True, figsize=(10,10), avg=True, fig_exp=None, line_color='r', exp_drop_col=None, \n",
    "     ymin=None, ymax=None, bh_line_error=None, bh_line_color='g', min_sp=0, avg_exp=None):\n",
    "        #the raw result from this method is a dataframe of the firing rate in time_window, each column is a bh trial\n",
    "        #downsample_rate set how many bins in one second, to determine how big the df is and how smooth the line can be\n",
    "        sp_bh = self.link_sp_bh(cluster_id, bh_win, only_bh, time_range, success, *behavior)\n",
    "        df = sp_bh.freq_spike(time_window, downsample_rate, rolling_window, rolling)\n",
    "        df_new = df.drop('timepoint', axis=1)\n",
    "        \n",
    "        if min_sp>0:\n",
    "            lst_drop = [col for col in df_new.columns if df_new[col].sum()<min_sp]\n",
    "        else:\n",
    "            lst_drop = []\n",
    "        \n",
    "        if plot:\n",
    "            fig, ax = plt.subplots(figsize=figsize)\n",
    "            plt.title(f'{df.columns[1][:-2]}')\n",
    "            plt.ylabel('Spike Freq. Hz')\n",
    "            plt.xlabel('Time s')\n",
    "            if avg is not True:\n",
    "                df.plot(x='timepoint', figsize=figsize)\n",
    "            else:\n",
    "                df_new = df_new.drop(lst_drop, axis=1)\n",
    "                df['avg'] = df_new.mean(axis=1, numeric_only=True)\n",
    "                df['error'] = df_new.sem(axis=1, numeric_only=True) \n",
    "\n",
    "                ax.plot(df.timepoint, df.avg, c=line_color)\n",
    "                ax.fill_between(df.timepoint, df.avg+df.error, df.avg-df.error, color=line_color, alpha=0.2)\n",
    "                if ymin:\n",
    "                    ax.set_ylim(bottom = ymin)\n",
    "                if ymax:\n",
    "                    ax.set_ylim(top = ymax)\n",
    "\n",
    "            if bh_line:\n",
    "                bh_line_df = self.get_bh_diff(bh_win, only_bh, time_range, success, *behavior)[0]\n",
    "                ymin_bh, ymax_bh = ax.get_ylim()\n",
    "                ax.vlines(bh_line_df.iloc[0], ymin_bh, ymax_bh, color=bh_line_color)\n",
    "                if bh_line_error:\n",
    "                    for diff in bh_line_df.columns:\n",
    "                        xerr_min = bh_line_df[diff][0] - bh_line_df[diff][1]\n",
    "                        xerr_max = bh_line_df[diff][0] + bh_line_df[diff][1]\n",
    "                        ax.axvspan(xerr_min, xerr_max, alpha=0.2, color=bh_line_color)\n",
    "\n",
    "            if fig_exp:\n",
    "                animal = self.spike_data_path.split(\"\\\\\")[-1]\n",
    "                batch = animal.split('_')[0]\n",
    "                n = animal.split('_')[-3]\n",
    "                plt.savefig(f'{self.spike_data_path}\\\\freqline_{batch}_{n}_{cluster_id}_{behavior[0]}.pdf')\n",
    "                \n",
    "        if exp_drop_col:\n",
    "            print(f'Dropped: {lst_drop}')\n",
    "            return lst_drop\n",
    "        if raw_exp:\n",
    "            return df\n",
    "        if avg_exp:\n",
    "            df['avg'] = df_new.mean(axis=1, numeric_only=True)\n",
    "            df['error'] = df_new.sem(axis=1, numeric_only=True)\n",
    "            return df\n",
    "    \n",
    "    def modulation_index(self, time_mod, behavior, downsample_rate=200, baseline_time=[-0.75, -0.25], \n",
    "                         time_window=[4,4], rand_no=1000, rand_seed=555, success=1):\n",
    "        #time_mod is the time during which the firing frequency will be compared with the baseline to find the modulation index.\n",
    "        #behavior is the behavior to be tested.\n",
    "        #If behavior contains Laser, the time_mod is used.\n",
    "        #If behavior contains pre_, the behaivior is after pre_, timepoint is the end.\n",
    "        #Otherwise the time_mod is set to the average duration of the input behavior.\n",
    "        print(f'working on {time_mod} {behavior}, this method runs random_mod_idx first and takes a long time to run!')\n",
    "        MI_rand, bh_duration = self.random_mod_idx(time_mod, behavior, downsample_rate, baseline_time, time_window, rand_no, rand_seed)\n",
    "        neurons = self.get_spike.keys()\n",
    "        MI = pd.DataFrame(columns= ['neurons'] )\n",
    "        MI['neurons']= neurons\n",
    "            \n",
    "        for nn, neuron in enumerate(neurons):\n",
    "            if 'pre_' in behavior:\n",
    "                r_behavior = behavior[4:]\n",
    "                df = self.freq_line_plot(neuron, time_window, behavior=[r_behavior], \n",
    "                                         downsample_rate=downsample_rate, plot=False, \n",
    "                                         raw_exp=True, success=success)\n",
    "            else:\n",
    "                df = self.freq_line_plot(neuron, time_window, behavior=[behavior], \n",
    "                                         downsample_rate=downsample_rate, plot=False, \n",
    "                                         raw_exp=True, success=success)\n",
    "            trials = df.columns[1:]\n",
    "            MI_neuron = np.zeros(len(trials))\n",
    "            print(f'Testing neuron {nn} for a total of {len(neurons)}')\n",
    "            \n",
    "            for ii, trial in enumerate(trials):\n",
    "                if 'pre_' in behavior:\n",
    "                    bh_end = int(time_window[0] * downsample_rate)\n",
    "                    bh_start = int(np.around(bh_end - time_mod * downsample_rate))\n",
    "                elif 'Laser' in behavior:\n",
    "                    bh_start = int(time_window[0] * downsample_rate)\n",
    "                    bh_end = int(np.around(bh_start + time_mod * downsample_rate))\n",
    "                elif 'Handle' in behavior:\n",
    "                    if time_mod:\n",
    "                        bh_start = int(time_window[0] * downsample_rate)\n",
    "                        bh_end = int(np.around(bh_start + time_mod * downsample_rate))\n",
    "                    else:\n",
    "                        bh_start = int(time_window[0] * downsample_rate)\n",
    "                        bh_end = int(bh_start + bh_duration.iloc[ii] * downsample_rate)\n",
    "                else:\n",
    "                    bh_start = int(time_window[0] * downsample_rate)\n",
    "                    bh_end = int(bh_start + bh_duration.iloc[ii] * downsample_rate)\n",
    "                \n",
    "                baseline_frame = [int(baseline_time[0]*downsample_rate+bh_start), int(baseline_time[1]*downsample_rate+bh_start)]\n",
    "                trial_freq = df[trial]\n",
    "                base_freq = np.mean(trial_freq.iloc[baseline_frame[0]:baseline_frame[1]])\n",
    "                avg_bh_freq= np.mean(trial_freq.iloc[bh_start:bh_end])\n",
    "                MI_neuron[ii] = (avg_bh_freq - base_freq)\n",
    "            \n",
    "            stat_beh, p_beh = mannwhitneyu(MI_rand[:, nn], MI_neuron)\n",
    "            if time_mod:\n",
    "                MI.loc[MI['neurons']==neuron, f'MI_{behavior}_{time_mod}'] =np.mean(MI_neuron)\n",
    "                MI.loc[MI['neurons']==neuron, f'p_{behavior}_{time_mod}'] = p_beh\n",
    "            else:\n",
    "                MI.loc[MI['neurons']==neuron, f'MI_{behavior}'] =np.mean(MI_neuron)\n",
    "                MI.loc[MI['neurons']==neuron, f'p_{behavior}'] = p_beh\n",
    "        \n",
    "        return MI\n",
    "    \n",
    "    def random_mod_idx(self, time_mod, behavior, downsample_rate, baseline_time, time_window, rand_no, rand_seed):\n",
    "        random.seed(rand_seed)\n",
    "        neurons = self.get_spike.keys()\n",
    "        time = self.time\n",
    "        bin_size = 1000/downsample_rate #in ms\n",
    "        bh_start = int((time_window[0] * 1000)/bin_size)\n",
    "        \n",
    "        random_timepoints = np.asarray(random.choices(time, k = rand_no))\n",
    "        rand_df = pd.DataFrame(columns = ['Behavior', 'Start', 'Stop'])\n",
    "        rand_df['Behavior'] = ['Rand' for x in range(rand_no)]\n",
    "\n",
    "        if \"pre_\" in behavior:\n",
    "            print('input behavior contains pre_, start and stop flipped.')\n",
    "            rand_df['Start'] = random_timepoints - time_mod\n",
    "            rand_df['Stop'] = random_timepoints\n",
    "        elif \"Laser\" in behavior:\n",
    "            print('input behavior contains Laser, time_mod is used.')\n",
    "            rand_df['Start'] = random_timepoints\n",
    "            rand_df['Stop'] = random_timepoints + time_mod\n",
    "        else:\n",
    "            print('time_mod is set to the average duration of the input behavior.')\n",
    "            bh_target_data = self.bh_data.loc[self.bh_data['Behavior']==behavior]\n",
    "            bh_target_duration = bh_target_data['Stop'] - bh_target_data['Start']\n",
    "            bh_target_duration_avg = np.mean(bh_target_duration)\n",
    "            rand_df['Start'] = random_timepoints\n",
    "            rand_df['Stop'] = random_timepoints + bh_target_duration_avg\n",
    "        bh_duration = rand_df['Stop']- rand_df['Start']\n",
    "        bh_duration_avg = np.mean(bh_duration)\n",
    "        \n",
    "        if 'NPX' in self.datatype:\n",
    "            R_class = Plot_Sp_Bh('dataframe_NPX', self.spike_data_path, sampling_rate=self.sampling_rate, \n",
    "                                 bh_time_dataframe=rand_df, wodis=self.wodis, channel=self.channel)\n",
    "        elif 'intan' in self.datatype:\n",
    "            R_class = Plot_Sp_Bh('dataframe_intan', self.spike_data_path, sampling_rate=self.sampling_rate, \n",
    "                                 bh_time_dataframe=rand_df, wodis=self.wodis, channel=self.channel)\n",
    "        else:\n",
    "            R_class = Plot_Sp_Bh('dataframe', self.spike_data_path, sampling_rate=self.sampling_rate, \n",
    "                                 bh_time_dataframe=rand_df, wodis=self.wodis, channel=self.channel)\n",
    "            \n",
    "        MI_rand = np.zeros((len(bh_duration), len(neurons)))\n",
    "        for nn, neuron in enumerate(neurons):\n",
    "            df = R_class.freq_line_plot(neuron, time_window, ['Rand'], \n",
    "                                        downsample_rate=downsample_rate, plot=False, raw_exp=True)\n",
    "            trials = df.columns[1:]\n",
    "            print(f'Random MI on neuron {nn} for a total of {len(neurons)}')\n",
    "            for ii, trial in enumerate(trials):\n",
    "                baseline_frame = [int(baseline_time[0]*1000/bin_size+bh_start), int(baseline_time[1]*1000/bin_size+bh_start)] \n",
    "                trial_freq = df[trial]\n",
    "                base_freq = np.mean(trial_freq.iloc[baseline_frame[0]:baseline_frame[1]])  \n",
    "                bh_end = int(np.around(bh_duration_avg*1000/bin_size + bh_start))\n",
    "                avg_bh_freq= np.mean(trial_freq.iloc[bh_start:bh_end])\n",
    "                MI_rand[ii, nn] = (avg_bh_freq - base_freq)\n",
    "        \n",
    "        return MI_rand, bh_duration\n",
    "    \n",
    "    def get_baseline_freq(self, bh=None, baseline_time=[-2, -1], all_time=None, time_window=[4,4], downsample_rate=200):\n",
    "        #this method gives a dataframe listing the baseline firing rate of each unit in the recording animal.\n",
    "        #if all_time is Ture, the baseline firing rate if of the whole recording session.\n",
    "        #if bh is provided, the baseline firing rate is calculated according to the baseline_time of each bh trial.\n",
    "        neurons = self.get_spike.keys()\n",
    "        time = self.time\n",
    "        Baseline = pd.DataFrame()\n",
    "        Baseline['neurons'] = neurons\n",
    "        \n",
    "        if all_time:\n",
    "            for neuron in neurons:\n",
    "                sp = self.get_sp_id(neuron)\n",
    "                base_freq = len(sp[neuron])/time.max()\n",
    "                Baseline.loc[Baseline['neurons']==neuron, 'all_time'] = base_freq\n",
    "                \n",
    "        if bh:\n",
    "            bh_start = int(time_window[0] * downsample_rate)\n",
    "            baseline_frame = [int(baseline_time[0]*downsample_rate+bh_start), int(baseline_time[1]*downsample_rate+bh_start)]\n",
    "\n",
    "            for neuron in neurons:\n",
    "                df = self.freq_line_plot(neuron, time_window, behavior=[bh], \n",
    "                                         downsample_rate=downsample_rate, plot=False, raw_exp=True)\n",
    "                trials = df.drop(columns='timepoint')\n",
    "                bl_trials = trials.iloc[baseline_frame[0]:baseline_frame[1]]\n",
    "                base_freq = bl_trials.mean().mean()\n",
    "                Baseline.loc[Baseline['neurons']==neuron, bh] = base_freq\n",
    "                \n",
    "        return Baseline\n",
    "    \n",
    "def get_MI_dict(time_len=None, bh_lst=None, independ=None, time_mod=None, behavior=None, downsample_rate=200, \n",
    "                baseline_time=[-0.75, -0.5], time_window=[4,4], rand_no=1000, rand_seed=555, success=1):\n",
    "    #this function get the modulation index for laser or behavior in all of the animals\n",
    "    #to use this one, a laser_dict or behavior_dict is needed\n",
    "    #bh_lst is a list of behavior\n",
    "    #if both time_len and bh_lst should be considered\n",
    "    #only input one time_len or bh at a time, the other parameter can be more\n",
    "    #if bh_lst is None, bh is set to Laser\n",
    "    \n",
    "    MI_dict = {}\n",
    "\n",
    "    for item in laser_dict.keys():\n",
    "        MI = pd.DataFrame(columns=['neurons'])\n",
    "        MI['neurons'] = laser_dict[item].get_spike.keys()\n",
    "        print(f'working on {item}')\n",
    "\n",
    "        if independ:\n",
    "            print('independ is True, time_len and bh_lst will be treated independently')\n",
    "            if time_len:\n",
    "                for ii in time_len:\n",
    "                    MI_Laser = laser_dict[item].modulation_index(ii, 'Laser',\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                    MI = MI.merge(MI_Laser, on=['neurons'], how='left')\n",
    "            if bh_lst:\n",
    "                for bh in bh_lst:\n",
    "                    MI_bh = behavior_dict[item].modulation_index(None, bh,\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                    MI = MI.merge(MI_bh, on=['neurons'], how='left')\n",
    "        else:\n",
    "            if time_len:            \n",
    "                if bh_lst is None:\n",
    "                    bh = 'Laser'\n",
    "                    for ii in time_len:\n",
    "                        MI_Laser = laser_dict[item].modulation_index(ii, bh,\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                        MI = MI.merge(MI_Laser, on=['neurons'], how='left')\n",
    "                elif (len(time_len)>1) and (len(bh_lst)>1):\n",
    "                    print('multipe items in time_len and bh_lst detected, they will be treated independently')\n",
    "                    for ii in time_len:\n",
    "                        MI_Laser = laser_dict[item].modulation_index(ii, 'Laser',\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                        MI = MI.merge(MI_Laser, on=['neurons'], how='left')\n",
    "                    for bh in bh_lst:\n",
    "                        MI_bh = behavior_dict[item].modulation_index(None, bh,\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                        MI = MI.merge(MI_bh, on=['neurons'], how='left')\n",
    "                else:\n",
    "                    for bh in bh_lst:\n",
    "                        if 'Laser' in bh:\n",
    "                            for ii in time_len:\n",
    "                                MI_Laser = laser_dict[item].modulation_index(ii, bh,\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                                MI = MI.merge(MI_Laser, on=['neurons'], how='left')\n",
    "                        else:\n",
    "                            for ii in time_len:\n",
    "                                MI_bh = behavior_dict[item].modulation_index(ii, bh,\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                                MI = MI.merge(MI_bh, on=['neurons'], how='left')\n",
    "            elif bh_lst:\n",
    "                for bh in bh_lst:\n",
    "                    MI_bh = behavior_dict[item].modulation_index(None, bh,\n",
    "                                                                 downsample_rate=downsample_rate, baseline_time=baseline_time, \n",
    "                                                                 time_window=time_window, rand_no=rand_no, rand_seed=rand_seed, success=success)\n",
    "                    MI = MI.merge(MI_bh, on=['neurons'], how='left')\n",
    "\n",
    "        MI_dict[item] = MI\n",
    "    \n",
    "    return MI_dict\n",
    "\n",
    "def get_bh_table_dict(bh_lst, time_window=[4,4], downsample_rate = 50):\n",
    "    #this function get the average firing rate of all neurons in all animals wrt the bh in bh_lst\n",
    "    #to use this one, a behavior_dict is needed\n",
    "    \n",
    "    Behavior_table_dict = {}\n",
    "\n",
    "    for item in bh_lst:\n",
    "        for animal in behavior_dict.keys():\n",
    "            bh_table = pd.DataFrame()\n",
    "            print(f'working on {animal} {item}')\n",
    "            for neuron in behavior_dict[animal].get_spike.keys():\n",
    "                neu_no = neuron.split('_')[1]\n",
    "                bh_align = behavior_dict[animal].freq_line_plot(neuron, time_window, behavior=[item], \n",
    "                                                   downsample_rate = downsample_rate, plot=False, avg_exp=True)\n",
    "                bh_table[f'{animal}_{neu_no}'] = bh_align['avg']\n",
    "\n",
    "            bh_table['timepoint'] = bh_align['timepoint']\n",
    "            key = f'{item}_{animal}'\n",
    "            Behavior_table_dict[key] = bh_table\n",
    "            \n",
    "    return Behavior_table_dict\n",
    "\n",
    "def get_baseline_table_dict(bh_lst=None, baseline_time=[-2, -1], all_time=None, time_window=[4,4], \n",
    "                       downsample_rate=200):\n",
    "    Baseline_table_dict = {}\n",
    "    animals = laser_dict.keys()\n",
    "    for animal in animals:\n",
    "        neurons = laser_dict[animal].get_spike.keys()\n",
    "        Baseline_table_dict[animal] = pd.DataFrame({'neurons':list(neurons)})\n",
    "        \n",
    "    if all_time:\n",
    "        for animal in animals:\n",
    "            print(f'working on all_time {animal}')\n",
    "            baseline_table = laser_dict[animal].get_baseline_freq(all_time=all_time)\n",
    "            Baseline_table_dict[animal] = Baseline_table_dict[animal].merge(baseline_table, on=['neurons'], how='left')\n",
    "        \n",
    "    if bh_lst:\n",
    "        for bh in bh_lst:\n",
    "            for animal in animals:\n",
    "                print(f'working on {bh} {animal}')\n",
    "                if \"Laser\" in bh:\n",
    "                    baseline_table = laser_dict[animal].get_baseline_freq(bh=bh, baseline_time=baseline_time, \n",
    "                                                                          time_window=time_window, \n",
    "                                                                          downsample_rate=downsample_rate)\n",
    "                else:\n",
    "                    baseline_table = behavior_dict[animal].get_baseline_freq(bh=bh, baseline_time=baseline_time, \n",
    "                                                                             time_window=time_window, \n",
    "                                                                             downsample_rate=downsample_rate)\n",
    "                Baseline_table_dict[animal] = Baseline_table_dict[animal].merge(baseline_table, on=['neurons'], how='left')\n",
    "    \n",
    "    return Baseline_table_dict\n",
    "\n",
    "def merge_dict_of_df(dict1, dict2, on_column=['neurons']):\n",
    "    \n",
    "    key1 = dict1.keys()\n",
    "    key2 = dict2.keys()\n",
    "    new_dict = {}\n",
    "    \n",
    "    if set(key1) == set(key2):\n",
    "        for key in key1:\n",
    "            new_df = dict1[key].merge(dict2[key], on=on_column, how='left')\n",
    "            new_dict[key] = new_df\n",
    "    else:\n",
    "        new_dict = {**dict1, **dict2}\n",
    "        \n",
    "    return new_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ce495be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "datapath = root+r'\\Fig 6 and S5'\n",
    "BL_MI_table = pd.read_csv(datapath+'\\BL_MI_table.csv', index_col=0)\n",
    "Laser_pos_dict = np.load(datapath+'\\Laser_pos_BH_dict.npy', allow_pickle='TRUE').reshape(1)[0]\n",
    "Laser_pos_dict_5ms = np.load(datapath+'\\Laser_pos_BH_dict_5ms.npy', allow_pickle='TRUE').reshape(1)[0]\n",
    "\n",
    "# find the neurons modulated by behavior\n",
    "BL_MI_table['Reach_final'] = BL_MI_table['Reach']- BL_MI_table['Reach_neg']\n",
    "BL_MI_table['Handle_final'] = BL_MI_table['Handle']- BL_MI_table['Handle_neg']\n",
    "BL_MI_table['Retract_final'] = BL_MI_table['Retract']- BL_MI_table['Retract_neg']\n",
    "BL_MI_table['preReach_final'] = BL_MI_table['preReach']- BL_MI_table['preReach_neg']\n",
    "\n",
    "pos_n = len(BL_MI_table[BL_MI_table['id']>0])\n",
    "print(f'{pos_n} neurons are positively modulated')\n",
    "counts = 0\n",
    "mask = np.zeros(BL_MI_table.shape[0])\n",
    "for ii in range(len(BL_MI_table)):\n",
    "    row = BL_MI_table.loc[:,['preReach_final', 'Reach_final', 'Retract_final', 'Handle_final']].iloc[ii, :].values\n",
    "    if all(row<=0) and any(row<0):\n",
    "        counts += 1\n",
    "        mask[ii] =1\n",
    "print(f'{counts} neurons are negatively modulated only')\n",
    "BL_MI_table['id_final'] = BL_MI_table['preReach_final']*1000 + BL_MI_table['Reach_final']*100 + BL_MI_table['Retract_final']*10 + BL_MI_table['Handle_final']\n",
    "n = np.sum(BL_MI_table['id_final'] == 0)\n",
    "print(f'{n} neurons are not modulated at all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87ed5ff5",
   "metadata": {},
   "source": [
    "## plot neuronal activity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81af860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap_bh(ctx, behavior):\n",
    "    before = 4\n",
    "    after = 4\n",
    "    bin_size = 50\n",
    "    cmap = 'viridis'\n",
    "    bh_start = int((before * 1000)/bin_size)\n",
    "\n",
    "    Laser_df = Laser_pos_dict[f'{ctx}_{behavior}']\n",
    "    trace_pos = Laser_df.add(-Laser_df.min(axis = 0))\n",
    "    trace_norm = trace_pos.div(trace_pos.max(axis= 0))\n",
    "    time_x = np.around(np.arange(-before, after, bin_size/1000), decimals = 2)\n",
    "    trace_norm = trace_norm.set_index(time_x)\n",
    "    trace_norm_val = trace_norm.values\n",
    "    max_ind = np.argmax(trace_norm_val, axis = 0)\n",
    "    sort_ind =np.argsort(max_ind)\n",
    "    trace_norm_sorted = trace_norm.iloc[:, sort_ind]\n",
    "    plt.figure(figsize = (15,10))\n",
    "    plt.title(f'{ctx} modulated neurons aligned to {behavior}, n = {len(Laser_df.columns)}', fontsize = 20)\n",
    "    sns.set(font_scale=2)\n",
    "    sns.heatmap(trace_norm_sorted.T, cmap = 'viridis', yticklabels = False, xticklabels= 200)\n",
    "\n",
    "    plt.xlabel('Time (s)')\n",
    "    plt.axvline(bh_start, ls = '--', color = 'r', linewidth = 2, alpha = 1)\n",
    "\n",
    "    plt.figure(figsize = (15,10))\n",
    "\n",
    "    tuning= []\n",
    "    for neuron_id in Laser_df.columns[sort_ind]:\n",
    "        tuning.extend(BL_MI_table[BL_MI_table.neurons == neuron_id][['preReach_final', 'Reach_final', 'Retract_final', 'Handle_final']].values)\n",
    "    sns.heatmap(tuning, cmap = cmap, vmin = -1, vmax= 1, yticklabels = False, linewidths=1,  cbar_kws=dict(ticks=[-1, 0, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c984ae4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heatmap for MAC modulated neurons in reaching\n",
    "heatmap_bh('MAC', 'Reach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18eff83b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heatmap for LAC modulated neurons in handling\n",
    "heatmap_bh('LAC', 'Handle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e46862b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heatmap for MAC modulated neurons in handling\n",
    "heatmap_bh('MAC', 'Handle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b783e94",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# heatmap for LAC modulated neurons in reaching\n",
    "heatmap_bh('LAC', 'Reach')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b0246bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_plot(ctx, behavior, color='black', bl_subtracted=True, plot=True, exp=None):\n",
    "    before = 4\n",
    "    after = 4\n",
    "    bin_size = 5\n",
    "    baseline_start = -4\n",
    "    baseline_end = -3\n",
    "    bh_start = int((before * 1000)/bin_size)\n",
    "    bl_frames = [bh_start + int(baseline_start * 1000/bin_size), bh_start + int(baseline_end * 1000/bin_size)]\n",
    "\n",
    "    Laser_df = Laser_pos_dict_5ms[f'{ctx}_{behavior}']\n",
    "    trace_pos = Laser_df.add(-Laser_df.min(axis = 0))\n",
    "    trace_norm = trace_pos.div(trace_pos.max(axis= 0))\n",
    "    if bl_subtracted:\n",
    "        bl = trace_norm.iloc[bl_frames[0]: bl_frames[1]].mean(axis = 0)\n",
    "        trace_norm = trace_norm.add(-bl)\n",
    "    time_x = np.linspace(-before,after-(bin_size/1000),int((before+after)*1000/bin_size))\n",
    "    MI_Behavior_avg = pd.DataFrame()\n",
    "    Laser_df_filtered = savgol_filter(trace_norm, 39,2, axis = 0)\n",
    "\n",
    "    Laser_df_new = pd.DataFrame(Laser_df_filtered, columns=Laser_df.columns)\n",
    "\n",
    "    MI_Behavior_avg['timepoint'] = time_x\n",
    "    MI_Behavior_avg['avg'] = Laser_df_new.mean(axis=1, numeric_only=True)\n",
    "    MI_Behavior_avg['error'] = Laser_df_new.std(axis=1, numeric_only=True)/np.sqrt(Laser_df_new.shape[1])\n",
    "    LAC_reach_aligned = MI_Behavior_avg.avg\n",
    "    \n",
    "    if plot==True:\n",
    "        plt.figure(figsize = (15,10))\n",
    "\n",
    "        plt.plot(MI_Behavior_avg.timepoint, MI_Behavior_avg.avg, c=color, label = 'LAC')\n",
    "        plt.title(f'{ctx} neurons aligned to onset of {behavior}')\n",
    "        plt.fill_between(MI_Behavior_avg.timepoint, MI_Behavior_avg.avg+MI_Behavior_avg.error, MI_Behavior_avg.avg-MI_Behavior_avg.error, color=color, alpha=0.2)\n",
    "        plt.axvline(0, ls = '--', color = 'r', linewidth = 2)\n",
    "    \n",
    "    if exp==True:\n",
    "        return Laser_df_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fdbc3a2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average activity of MAC and LAC modulated neurons aligned to onset of Reach\n",
    "avg_plot('MAC', 'Reach', 'darkmagenta')\n",
    "avg_plot('LAC', 'Reach', 'darkturquoise')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22a579b3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# average activity of MAC and LAC modulated neurons aligned to onset of Handle\n",
    "avg_plot('MAC', 'Handle', 'darkmagenta')\n",
    "avg_plot('LAC', 'Handle', 'darkturquoise')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979763c2",
   "metadata": {},
   "source": [
    "# Figure 7 and S8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb3c881",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d079768a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the packages for Figure 2 first before running the code in this session\n",
    "datapath = root+r'\\Fig 7 and S8'\n",
    "\n",
    "x_y_dict = {\n",
    "    'stn': (0.5, 2.5, 0.5, 2.5),\n",
    "    'suc': (0, 2.5, 3, 6),\n",
    "    'str': (1, 4, 0, 5),\n",
    "    'snr': (1, 2.5, 1.5, 3)\n",
    "}\n",
    "\n",
    "rois = ['rstr', 'istr', 'cstr', 'rstn', 'istn', 'cstn', 'rsuc', 'csuc', 'rsnr', 'csnr', 'sstr', 'sstn', 'ssuc', 'ssnr']\n",
    "\n",
    "#plot injection sites\n",
    "inj_site = Injection_Sites(datapath, \"injection_site.csv\")\n",
    "inj_site_table = inj_site.read_data()\n",
    "inj_site.cross_plot(name=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80ff45bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {}\n",
    "data_dict_sn = {}\n",
    "roi_dict = {'1':'rstr', '2':'istr', '3':'cstr', '4':'rstn', '5':'istn', '7':'xxx', '6':'cstn', '8':'rsuc', '9':'csuc', \n",
    "            '10':'5n', '11':'pcrt', '12':'lpgi'}\n",
    "roi_dict_sn = {'7': 'rsnr', '8': 'csnr'}\n",
    "regions = ['sstr', 'sstn', 'ssuc', 'ssnr']\n",
    "syn_dict = {}\n",
    "\n",
    "for ii, item in enumerate(inj_site_table.brain):\n",
    "    \n",
    "#data_dict has a key for each injection site, all data points are in one dataframe    \n",
    "    df = pd.read_csv(datapath+f'\\\\{item}'+'_woSN.csv')\n",
    "    df_sn = pd.read_csv(datapath+f'\\\\{item}'+'_SN.csv')\n",
    "    df_new = df.rename(columns={'AP_location':'z', 'DV_location':'y', 'ML_location':'x'})\n",
    "    df_new_sn = df_sn.rename(columns={'AP_location':'z', 'DV_location':'y', 'ML_location':'x'})\n",
    "    df_new['y'] = 7.2 - df_new['y']\n",
    "    df_new_sn['y'] = 7.2 - df_new_sn['y']\n",
    "    \n",
    "#add one column roi to the dataframe to define the region of interest of each section\n",
    "    roi_lst = []\n",
    "    for file in df_new['roiFIle']:\n",
    "        ii = file.split('-')[1].split('_')[0]\n",
    "        roi = roi_dict.get(ii)\n",
    "        if roi:\n",
    "            roi_lst.append(roi)\n",
    "    df_new['roi'] = roi_lst\n",
    "    data_dict[item] = df_new\n",
    "    \n",
    "    roi_lst_sn = []\n",
    "    for file in df_new_sn['roiFIle']:\n",
    "        ii = file.split('-')[1].split('_')[0]\n",
    "        roi_sn = roi_dict_sn[ii]\n",
    "        roi_lst_sn.append(roi_sn)\n",
    "    df_new_sn['roi'] = roi_lst_sn\n",
    "    data_dict_sn[item] = df_new_sn\n",
    "    \n",
    "#syn_dict has a key for each brain in each roi, value is the Syn_Position instantiation\n",
    "    for roi in roi_dict.values():\n",
    "        data = data_dict[item].loc[data_dict[item]['roi']==roi]\n",
    "        if data.shape[0] > 0:\n",
    "            key = item+'_'+roi\n",
    "            syn_dict[key] = Syn_Position(datadf = data)\n",
    "            \n",
    "    for roi in roi_dict_sn.values():\n",
    "        data = data_dict_sn[item].loc[data_dict_sn[item]['roi']==roi]\n",
    "        if data.shape[0] > 0:\n",
    "            key = item+'_'+roi\n",
    "            syn_dict[key] = Syn_Position(datadf = data)\n",
    "            \n",
    "    for region in regions:\n",
    "        if region == 'ssnr':\n",
    "            data = data_dict_sn[item].loc[data_dict_sn[item]['roi'].str.contains(region[1:])]\n",
    "        else:\n",
    "            data = data_dict[item].loc[data_dict[item]['roi'].str.contains(region[1:])]\n",
    "        if data.shape[0] > 0:\n",
    "            key = item+'_'+region\n",
    "            syn_dict[key] = Syn_Position(datadf = data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0681830c",
   "metadata": {},
   "source": [
    "## plot synaptic distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7dbf21",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_brain(target, exp=None, line_num=None):\n",
    "    #only works when the syn_dict is defined\n",
    "    #easily plot the scatter and contour plot for all the regions\n",
    "\n",
    "    for roi in rois:\n",
    "        xmin, xmax, ymin, ymax = x_y_dict[roi[-3:]]\n",
    "        key = f'{target}_{roi}'\n",
    "\n",
    "        if syn_dict.get(key):\n",
    "            print(key)\n",
    "            syn_dict[f'{target}_{roi}'].scatter_plot(nordf=True, nor=False, one_side=False, exp=exp, section=key)\n",
    "            syn_dict[f'{target}_{roi}'].contour_plot(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, nordf=True, nor=False, one_side=False, line_num=line_num, exp=exp, section=key)\n",
    "\n",
    "def contour_plot_group(ctx, roi):\n",
    "    xmin, xmax, ymin, ymax = x_y_dict[f'{roi[-3:]}']\n",
    "    ctx_roi = np.reshape(np.zeros(100**2), xx.shape)\n",
    "    for key in syn_dict.keys():\n",
    "        if (ctx in key) and (roi[-3:] in key):\n",
    "            est_2d = syn_dict[key].contour_plot(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, nordf=True, nor=False, one_side=False, est_exp=True, plot=False)\n",
    "            ctx_roi += est_2d\n",
    "    ctx_roi /= ctx_roi.max()\n",
    "    return ctx_roi\n",
    "\n",
    "def contour_plot_3ctx(roi):\n",
    "    xmin, xmax, ymin, ymax = x_y_dict[f'{roi[-3:]}']\n",
    "    xx, yy = np.mgrid[xmin:xmax:100j, ymin:ymax:100j]\n",
    "    levels = [0.3, 0.45, 0.6, 0.75, 0.9]\n",
    "    \n",
    "    fig = plt.figure(figsize=(20,20))\n",
    "    ax = fig.gca()\n",
    "    ax.set_xlim(xmin, xmax)\n",
    "    ax.set_ylim(ymin, ymax)\n",
    "    \n",
    "    ctx_color = [('mac', 'darkmagenta'), ('lac', 'darkcyan'), ('ins', 'limegreen')]\n",
    "    for item in ctx_color:\n",
    "        ctx_roi = contour_plot_group(item[0], roi)\n",
    "        cset_mac_roi = ax.contour(xx, yy, ctx_roi, levels=levels, colors=item[1])\n",
    "    \n",
    "    plt.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51e70070",
   "metadata": {},
   "outputs": [],
   "source": [
    "levels = [0.3, 0.45, 0.6, 0.75, 0.9]\n",
    "plot_brain('mac_0', line_num=levels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43cb0e8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot_3ctx('sstr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f619de",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot_3ctx('ssuc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4935d5c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot_3ctx('sstn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15b7660",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_plot_3ctx('ssnr')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1260a72",
   "metadata": {},
   "source": [
    "## cosine similarity and  clustering analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2670864",
   "metadata": {},
   "outputs": [],
   "source": [
    "est2d = {}\n",
    "for key, value in syn_dict.items():\n",
    "    roi = f'{key[-3:]}'\n",
    "    x_y = x_y_dict.get(roi)\n",
    "    if x_y:\n",
    "        xmin, xmax, ymin, ymax = x_y\n",
    "        est2d[key] = value.contour_plot(xmin=xmin, xmax=xmax, ymin=ymin, ymax=ymax, nordf=True, nor=False, one_side=False, est_exp=True, plot=False)\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "est1d_dict = kde2d_to_1d(est2d)\n",
    "\n",
    "for brain in inj_site_table['brain']:\n",
    "    for roi in rois:\n",
    "        key_roi = f'{brain}_{roi}'\n",
    "        if est1d_dict.get(key_roi) is None:\n",
    "            est1d_dict[key_roi] = np.zeros((1,10000))\n",
    "\n",
    "est3d_brain = {}\n",
    "for brain in inj_site_table['brain']:\n",
    "    brain_est1d = [est1d_dict[f'{brain}_{roi}'] for roi in rois]\n",
    "    est3d_brain[brain] = np.concatenate(brain_est1d, axis=1)\n",
    "\n",
    "est3d_roi = {}\n",
    "roi_region = ['str', 'stn', 'suc', 'snr']\n",
    "for roi in roi_region:\n",
    "    for brain in inj_site_table['brain']:\n",
    "        roi_est1d = [est1d_dict[f'{brain}_{xroi}']for xroi in rois if roi in xroi]\n",
    "        est3d_roi[f'{brain}_{roi}'] = np.concatenate(roi_est1d, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105f6127",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_matrix(est3d_brain, section=f'retro_antero_all_levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa804173",
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distance_dendrogram(est3d_brain, section=f'retro_antero_all_levels')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0e33f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "for roi in roi_region:\n",
    "    roi_ana_brain = [key for key in est3d_roi if roi in key]\n",
    "    est3d_roi_ana = {}\n",
    "    for roi_ana in roi_ana_brain:\n",
    "        est3d_roi_ana[roi_ana] = est3d_roi[roi_ana]\n",
    "    cosine_distance_dendrogram(est3d_roi_ana, section=f'retro_antero_{roi}')\n",
    "    print(roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0da63250",
   "metadata": {},
   "source": [
    "# Figure S4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2ec02c1",
   "metadata": {},
   "source": [
    "## load packages and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48bee2bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#load the packages for Figure 2 first before running the code in this session\n",
    "\n",
    "datapath = root+r'\\Fig S4'\n",
    "\n",
    "Brains = {}\n",
    "datalst = os.listdir(datapath)\n",
    "for file in datalst:\n",
    "    key = file[:-4]\n",
    "    value = np.load(glob.glob(os.path.join(datapath, file))[0])\n",
    "    Brains[key] = value\n",
    "    \n",
    "ap = [-4.5, -7]\n",
    "dv = [5, 7.3]\n",
    "ml = [0, -3]\n",
    "\n",
    "kde3d_matrix = {}\n",
    "kde3d_dict = {}\n",
    "\n",
    "n_ap = 50j\n",
    "n_dv = 100j\n",
    "n_ml = 100j\n",
    "\n",
    "xi, yi, zi = np.mgrid[ap[0]:ap[1]:n_ap, dv[0]:dv[1]:n_dv, ml[0]:ml[1]:n_ml]\n",
    "grid = np.vstack([item.ravel() for item in [xi, yi, zi]])\n",
    "\n",
    "for key, value in Brains.items():\n",
    "    data = value.T\n",
    "    kde = st.gaussian_kde(data)\n",
    "    density = kde(grid).reshape(xi.shape)\n",
    "    kde3d_dict[key] = density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514c6d64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate KDE in 3 dimensions AP, DV, ML\n",
    "mac_dv_ml_avg = np.zeros((int(abs(n_ml)),int(abs(n_dv))))\n",
    "lac_dv_ml_avg = np.zeros((int(abs(n_ml)),int(abs(n_dv))))\n",
    "ins_dv_ml_avg = np.zeros((int(abs(n_ml)),int(abs(n_dv))))\n",
    "mac_ap_ml_avg = np.zeros((int(abs(n_ml)),int(abs(n_ap))))\n",
    "lac_ap_ml_avg = np.zeros((int(abs(n_ml)),int(abs(n_ap))))\n",
    "ins_ap_ml_avg = np.zeros((int(abs(n_ml)),int(abs(n_ap))))\n",
    "mac_ap_dv_avg = np.zeros((int(abs(n_ap)),int(abs(n_dv))))\n",
    "lac_ap_dv_avg = np.zeros((int(abs(n_ap)),int(abs(n_dv))))\n",
    "ins_ap_dv_avg = np.zeros((int(abs(n_ap)),int(abs(n_dv))))\n",
    "ins_count = 0\n",
    "mac_count = 0\n",
    "lac_count = 0\n",
    "for key, value in kde3d_dict.items():\n",
    "    \n",
    "    if 'ins' in key:\n",
    "        dv_ml = np.mean(value, axis=0).T\n",
    "        ap_ml = np.mean(value, axis=1).T\n",
    "        ap_dv = np.mean(value, axis=2)\n",
    "        \n",
    "        ins_dv_ml_avg += dv_ml\n",
    "        ins_ap_ml_avg += ap_ml\n",
    "        ins_ap_dv_avg += ap_dv\n",
    "        \n",
    "        ins_count += 1\n",
    "        \n",
    "    if 'lac' in key:\n",
    "        dv_ml = np.mean(value, axis=0).T\n",
    "        ap_ml = np.mean(value, axis=1).T\n",
    "        ap_dv = np.mean(value, axis=2)\n",
    "        \n",
    "        lac_dv_ml_avg += dv_ml\n",
    "        lac_ap_ml_avg += ap_ml\n",
    "        lac_ap_dv_avg += ap_dv\n",
    "        \n",
    "        lac_count += 1\n",
    "        \n",
    "    if 'mac' in key:\n",
    "        dv_ml = np.mean(value, axis=0).T\n",
    "        ap_ml = np.mean(value, axis=1).T\n",
    "        ap_dv = np.mean(value, axis=2)\n",
    "        \n",
    "        mac_dv_ml_avg += dv_ml\n",
    "        mac_ap_ml_avg += ap_ml\n",
    "        mac_ap_dv_avg += ap_dv\n",
    "        \n",
    "        mac_count += 1\n",
    "            \n",
    "ins_dv_ml_avg /= ins_dv_ml_avg.max()\n",
    "ins_ap_ml_avg /= ins_ap_ml_avg.max()\n",
    "ins_ap_dv_avg /= ins_ap_dv_avg.max()\n",
    "lac_dv_ml_avg /= lac_dv_ml_avg.max()\n",
    "lac_ap_ml_avg /= lac_ap_ml_avg.max()\n",
    "lac_ap_dv_avg /= lac_ap_dv_avg.max()\n",
    "mac_dv_ml_avg /= mac_dv_ml_avg.max()\n",
    "mac_ap_ml_avg /= mac_ap_ml_avg.max()\n",
    "mac_ap_dv_avg /= mac_ap_dv_avg.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde823e7",
   "metadata": {},
   "source": [
    "## plot density curves for neuronal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1710f90d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#coronal view\n",
    "\n",
    "xx, yy = np.mgrid[ml[0]: ml[1]:n_ml, 7.3-dv[0]:7.3-dv[1]:n_dv]\n",
    "fig = plt.figure(figsize=(20,20))\n",
    "ax = fig.gca()\n",
    "levels = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "\n",
    "cset_ins = ax.contour(xx, yy, ins_dv_ml_avg, levels= [0.5, 0.6, 0.7, 0.8, 0.9], colors='g')\n",
    "cset_mac = ax.contour(xx, yy, mac_dv_ml_avg, levels= [0.5, 0.6, 0.7, 0.8, 0.9], colors='r')\n",
    "cset_lac = ax.contour(xx, yy, lac_dv_ml_avg, levels= [0.5, 0.6, 0.7, 0.8, 0.9], colors='b')\n",
    "\n",
    "#ax.clabel(cset_ins, fontsize=10)\n",
    "#ax.clabel(cset_mac, inline=1, fontsize=10)\n",
    "#ax.clabel(cset_lac, inline=1, fontsize=10)\n",
    "ax.set_xlim(0, 3)\n",
    "ax.set_ylim(0, 2.5)\n",
    "ax.axis('equal')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d2baa6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here y is dv\n",
    "def kernel_1d_y(data):\n",
    "    y_d = np.linspace(dv[0], dv[1], 100)\n",
    "\n",
    "    y = data[:, 1]\n",
    "    kernel = st.gaussian_kde(y)\n",
    "    p= kernel(y_d)\n",
    "    return p\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "y_d = np.linspace(7.3-dv[0], 7.3-dv[1], 100)\n",
    "lac_0 = kernel_1d_y(Brains['lac_2'])\n",
    "lac_1 = kernel_1d_y(Brains['lac_11'])\n",
    "lac_2 = kernel_1d_y(Brains['lac_30'])\n",
    "lac_3 = kernel_1d_y(Brains['lac_33'])\n",
    "\n",
    "lac_all = np.array([lac_0, lac_1, lac_2, lac_3])\n",
    "lac_all_mean = np.average(lac_all, axis=0)\n",
    "lac_all_sem = np.std(lac_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "mac_0 = kernel_1d_y(Brains['mac_1'])\n",
    "mac_1 = kernel_1d_y(Brains['mac_2'])\n",
    "mac_2 = kernel_1d_y(Brains['mac_30'])\n",
    "\n",
    "mac_all = np.array([mac_0, mac_1, mac_2])\n",
    "mac_all_mean = np.average(mac_all, axis=0)\n",
    "mac_all_sem = np.std(mac_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "ins_0 = kernel_1d_y(Brains['ins_11'])\n",
    "ins_1 = kernel_1d_y(Brains['ins_22'])\n",
    "ins_2 = kernel_1d_y(Brains['ins_33'])\n",
    "ins_all = np.array([ins_0, ins_1, ins_2])\n",
    "ins_all_mean = np.average(ins_all, axis=0)\n",
    "ins_all_sem = np.std(ins_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "plt.title('KDE of anterograde labeled cell in dorso-ventral axis')\n",
    "plt.xlabel(\"DV axis (mm)\")\n",
    "plt.ylabel(\"Density\")\n",
    "plt.plot(y_d, ins_all_mean, color='yellowgreen')\n",
    "plt.fill_between(y_d, ins_all_mean+ins_all_sem, ins_all_mean-ins_all_sem, alpha = 0.2, color='yellowgreen')\n",
    "\n",
    "plt.plot(y_d, mac_all_mean, color='darkmagenta')\n",
    "plt.fill_between(y_d, mac_all_mean+mac_all_sem, mac_all_mean-mac_all_sem, alpha = 0.2, color='darkmagenta')\n",
    "\n",
    "\n",
    "plt.plot(y_d, lac_all_mean, color='darkcyan')\n",
    "plt.fill_between(y_d, lac_all_mean+lac_all_sem, lac_all_mean-lac_all_sem, alpha = 0.2, color='darkcyan')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "932c5b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "#here the y is ml \n",
    "def kernel_1d_y(data):\n",
    "    y_d = np.linspace(ml[0], ml[1], 100)\n",
    "\n",
    "    y = data[:, 2]\n",
    "    kernel = st.gaussian_kde(y)\n",
    "    p= kernel(y_d)\n",
    "    return p\n",
    "fig = plt.figure(figsize=(10,10))\n",
    "\n",
    "y_d = np.linspace(ml[0], ml[1], 100)\n",
    "lac_0 = kernel_1d_y(Brains['lac_2'])\n",
    "lac_1 = kernel_1d_y(Brains['lac_11'])\n",
    "lac_2 = kernel_1d_y(Brains['lac_30'])\n",
    "lac_3 = kernel_1d_y(Brains['lac_33'])\n",
    "\n",
    "lac_all = np.array([lac_0, lac_1, lac_2, lac_3])\n",
    "lac_all_mean = np.average(lac_all, axis=0)\n",
    "lac_all_sem = np.std(lac_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "mac_0 = kernel_1d_y(Brains['mac_1'])\n",
    "mac_1 = kernel_1d_y(Brains['mac_2'])\n",
    "mac_2 = kernel_1d_y(Brains['mac_30'])\n",
    "\n",
    "mac_all = np.array([mac_0, mac_1, mac_2])\n",
    "mac_all_mean = np.average(mac_all, axis=0)\n",
    "mac_all_sem = np.std(mac_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "ins_0 = kernel_1d_y(Brains['ins_11'])\n",
    "ins_1 = kernel_1d_y(Brains['ins_22'])\n",
    "ins_2 = kernel_1d_y(Brains['ins_33'])\n",
    "ins_all = np.array([ins_0, ins_1, ins_2])\n",
    "ins_all_mean = np.average(ins_all, axis=0)\n",
    "ins_all_sem = np.std(ins_all, axis=0)/np.sqrt(3)\n",
    "\n",
    "plt.title('KDE of anterograde labeled cell in medio-lateral axis')\n",
    "plt.xlabel(\"ML axis (mm)\")\n",
    "plt.ylabel(\"Density\")\n",
    "\n",
    "plt.plot(y_d, ins_all_mean, color='yellowgreen')\n",
    "plt.fill_between(y_d, ins_all_mean+ins_all_sem, ins_all_mean-ins_all_sem, alpha = 0.2, color='yellowgreen')\n",
    "\n",
    "plt.plot(y_d, mac_all_mean, color='darkmagenta')\n",
    "plt.fill_between(y_d, mac_all_mean+mac_all_sem, mac_all_mean-mac_all_sem, alpha = 0.2, color='darkmagenta')\n",
    "\n",
    "\n",
    "plt.plot(y_d, lac_all_mean, color='darkcyan')\n",
    "plt.fill_between(y_d, lac_all_mean+lac_all_sem, lac_all_mean-lac_all_sem, alpha = 0.2, color='darkcyan')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
